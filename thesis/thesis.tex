\documentclass[]{article}
\usepackage{amsmath}\usepackage{amsfonts}
\usepackage[english]{babel}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\usepackage[margin=1in,footskip=0.25in]{geometry}
\usepackage{mathtools}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}
\usepackage[final]{graphicx}
\usepackage{listings}
\usepackage{courier}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\newcommand{\indep}{\perp \!\!\! \perp}
% \usepackage{wrapfig}
\graphicspath{{.}}
% \usepackage{fancyvrb}

%%
%% Julia definition (c) 2014 Jubobs
%%
\usepackage[T1]{fontenc}
\usepackage{beramono}
\usepackage[usenames,dvipsnames]{xcolor}
\lstdefinelanguage{Julia}%
  {morekeywords={abstract,break,case,catch,const,continue,do,else,elseif,%
      end,export,false,for,function,immutable,import,importall,if,in,%
      macro,module,otherwise,quote,return,switch,true,try,type,typealias,%
      using,while},%
   sensitive=true,%
   alsoother={$},%
   morecomment=[l]\#,%
   morecomment=[n]{\#=}{=\#},%
   morestring=[s]{"}{"},%
   morestring=[m]{'}{'},%
}[keywords,comments,strings]%

\lstset{%
    language         = Julia,
    basicstyle       = \ttfamily,
    keywordstyle     = \bfseries\color{blue},
    stringstyle      = \color{magenta},
    commentstyle     = \color{ForestGreen},
    showstringspaces = false,
}

\linespread{1}
\usepackage[fontsize=12pt]{fontsize}

\begin{document}
\numberwithin{equation}{subsection}
\tableofcontents
\newpage
\section*{Notations}
\begin{enumerate}
    \item $\text{ran}(A):=\{Ax :\forall x \; \in \mathbb R^n\}, A \in \mathbb R^{m\times n}$, The range of a matrix. 
\end{enumerate}
\section*{Introduction}
\newpage
\section{Foundations}
    This sections focuses on important mathematical entities that are important for formulating, analyzing the Conjugate Gradient and the Lanczos Algorithm. Major parts of this sections cited from... 

    \subsection{Projectors}
        A projector is a type of idempotent matrix. Its properties has theoretical importance under the context of subspace projection method. In this section, we go through 2 types of projectors, the orthgonal projector and the oblique projector. The oblique projector is made useful for the derivation of the classic CG algorithm, which is referred to as RACG in the context of this paper. The orthogonal projector is made useful for the interpretations of Arnoldi Iterations. 
        \begin{definition}
            A matrix $P$ is a projector when: 
            \begin{align}
                P^2 = P
            \end{align}    
        \end{definition}
        
        This property is sometimes referred as \textit{idempotent}. As a consequence, $\text{ran}(I - P) = \text{null}(P)$ and here is the proof: 
        \begin{proof}
            \begin{align}
                \forall x \in \mathbb{C}^n: P(I - P)x &= \mathbf{0} \implies \text{ran}(I - P)\subseteq \text{null}(P)
                \\
                \forall x \in \text{null}(P): Px &= \mathbf{0} \implies (I - P)x = x \implies x \in \text{ran}(I - P)
                \\
                \implies \text{ran}(I - P) &= \text{null}(P)
                \label{a:1.1.4}
            \end{align}
        \end{proof}
        \noindent
        This consequence states the fact that any vector $x$ can be represented in the form of: $x = Px + (I - P)x$, and every projector will be defined via the range of $I - P$ and $P$. 
        
        \subsubsection{Orthogonal Projector}
            An orthogonal projector is a projector such that: 
            \begin{definition}
                \begin{align}
                    \text{null}(P) \perp \text{ran}(P)
                \end{align}    
            \end{definition}
            
            This property is in fact, very special. A good example of an orthogonal projector would be the Householder Reflector Matrix. Or just any $\hat{u}\hat{u}^H$ where $\hat{u}$ is being an unitary vector. For convenience of proving, assume subspace $M = \text{ran}(P)$. Consider the following lemma: 
            \begin{lemma}
                \begin{align}
                    \text{null}(P^H) = \text{ran}(P)^{\perp}
                    \\
                    \text{null}(P) = \text{ran}(P^H)^{\perp}
                \end{align}    
            \end{lemma}
            \noindent
            Using \hyperref[a:1.1.4]{(1.1.4)} and consider the proof: 
            \begin{proof}
                \begin{align}
                    \langle P^Hx, y\rangle &= \langle x, Py\rangle 
                    \\
                    \forall  x &\in \text{null}(P^H), y\in \mathbb{C}^n
                    \\
                    \implies \langle P^Hx ,y\rangle &= 0 = \langle x, Py\rangle
                    \\
                    \implies \text{null}(P^H) \perp& \text{ran}(P)
                    \\
                    \forall y \in& \text{null}(P), x \in \mathbb{C}^n: 
                    \\
                    \langle x, Py\rangle &= 0 = \langle P^Hx, y\rangle
                    \\
                    \implies \text{ran}(P^H) \perp& \text{null}(P)
                \end{align}
            \end{proof}
            
            \begin{prop}
                A projector is orthogonal iff it's Hermitian. 
            \end{prop}
            \begin{proof}
                $\impliedby$ Assuming the matrix is Hermitian and it's a projector, then we wish to prove that it's an orthogonal projector. Let's recall: 
                \begin{align}
                    \text{null}(P^H) = \text{ran}(P)^{\perp}
                    \\
                    \text{null}(P) = \text{ran}(P^H)^{\perp}
                \end{align}
                Substituting $P^H = P$, we have $\text{null}(P) = \text{ran}(P)^{\perp}$, Which is the definition of Orthogonal Projector. Therefore, $P$ is an orthogonal projector by the definition of the projector. 
                \par
                For the $\implies$ direction, we assume that $P$ is an Orthogonal Projector, then we wish to show that it's also Hermitian. Observe that $P^H$ is also a projector because $(P^H)^2 = (P^2)^H$. Then, using the definition of orthogonal projector: 
                \begin{align}
                    \text{null}(P) &\perp\text{ran}(P) 
                    \\
                    \text{null}(P^H) &\perp \text{ran}(P^H)
                \end{align}
                Notice that using above statement together with Lemma 1 means $\text{null}(P) = \text{ran}(P)^\perp = \text{null}(P^H)$, and then $\text{ran}(P)=\text{null}(P)^\perp = \text{ran}(P^H)$. Therefore, $P^H$ is an projector such that: $\text{ran}(P) = \text{ran}(P^H) \wedge \text{null}(P) = \text{null}(P^H)$. The range and null space of $P^H$ and $P$ is the same therefore $P$ has to be Hermitian. 
            \end{proof}
            \subsubsection{Oblique Projector}
                An oblique projector is not orthogonal, and vice versa. It's a projector that satisfies the following conditions: 
                \begin{definition}
                    \begin{align}
                        Px \in M \quad (I - P)x \perp L \quad \text{where: } M \neq L
                    \end{align}    
                \end{definition}
                An orthgonal projector is the case when the subspace $M = L$. 
                \par
                A famous example of an orthogonal projector is $QQ^H$ where $Q$ is an Unitary Matrix. This is a Hermitian Matrix and it's idempotent, making it an orthogonal projector. 
            \subsubsection{Projector Geometric Intuitions}
                A projector describes a given vector using some elements from another basis. The oblique projector creates a light sources in the form of the subspace $L$ and it shoots parrell light ray orthogonal to $L$, crossing vectors and projecting their shadow onto subspace $M$. 
        \subsubsection{Projector as a 2-Norm Minimizer}
            An orthogonal projector always reduce the 2 norm of a vector. Given any subspace $M$, we can create a basis of vectors packing into the some matrix, say $A$, then $P_M$ as a projector onto the basis $M$ one example can be: $A(AA^T)^{-1}A^T$. Let's consider the claim: 
            \begin{align}
                \Vert P_Mx\Vert^2 \le \Vert x\Vert^2
            \end{align}
            Proof: 
            \begin{align}
                x &= Px + (I - P)x 
                \\
                \Vert x\Vert^2 &= \Vert Px\Vert^2 + \Vert (I - P)x\Vert^2
                \\
                \Vert x\Vert^2 &\ge \Vert Px\Vert^2
            \end{align}
            Using this property of the Orthogonal Projector, we consider the following minimizations problem: 
            \begin{align}
                \min_{x\in M} \Vert y - x\Vert_2^2 = \Vert y - P_M(y)\Vert_2^2
            \end{align}
            Proof:
            \begin{align}
                \Vert y - x\Vert_2^2 &= 
                \Vert y - P_My + P_My - x\Vert_2^2
                \\
                \Vert y - x\Vert_2^2 &= 
                \Vert y - P_My\Vert_2^2 + \Vert P_My - x\Vert_2^2
                \\
                \implies 
                \Vert y - P_My\Vert_2^2 &\le \Vert y - x\Vert_2^2
            \end{align}
            That concludes the proof. Observe that, $y - P_My\perp M$ and $P_My - x \in M$ because $P_My, x \in M$, which allows us to split the norm of $y - x$ into 2 components. In addition using the fact that the projector is orthogonal. That concludes the proof. 
    \subsection{Subspace Projection Methods}
        Let $\mathcal K, \mathcal L$ be subspaces where candidates soltuions are chosen and residuals are orthogonalized against. Under the idea case the 2 subspaces spans all dimensions, and it's able to approximate all solutions and forcing the residual vector ($b - Ax$) to be zero. This is a description of this framework: 
        \begin{align}
            \tilde{x} \in x_0 + \mathcal{K} \text{ s.t: } b - A\tilde{x} \perp \mathcal{L}
        \end{align}
        it looks for an $x$ in the affine linear subspace $\mathcal{K}$ such that it's perpendicular to the subspace $\mathcal{L}$, or, equivalently, minimizing the projection onto the subspace $\mathcal{L}$. One interpretation of it is an projection of residual onto the basis that is orthogonal to $\mathcal L$. 
        \par
        Sometimes, for convenience and the exposition and exposing hidden connections between ideas, the above conditions can be expressed using matrix. 
        \begin{align}
            \text{Let } V \in \mathbb{C}^{n\times m} \text{ be a aasis for: }\mathcal{K}
            \\
            \text{Let } W \in \mathbb{C}^{n\times m} \text{ be a basis for: } \mathcal{L}
        \end{align}
        We can then make us of (1.3.1) and express it in the form of: 
        \begin{align}
            \tilde{x} &= x^{(0)} + Vy
            \\
            b - A\tilde{x}  &\perp \text{ran}(W)
            \\
            W^T(b - A\tilde{x} - AVy) &= \mathbf{0}
            \\
            W^Tr^{(0)} - W^TAVy&= \mathbf{0}
            \\
            W^TAVy &= W^Tr^{(0)}
        \end{align}
        \subsubsection{Prototype Algorithm}
            And from here, we can define a simple prototype algorithm using this framworks. 
            \begin{align}
            \begin{aligned}
                & \text{While not converging}: 
                \\&\hspace{1.1em}
                        \begin{aligned}
                        &\text{Increase Span for: } \mathcal{K, L}
                        \\
                        &\text{Choose: } V, W \text{ for } \mathcal{K}, \mathcal{L}
                        \\
                        & r := b - Ax
                        \\
                        & y := (W^TAV)^{-1}W^Tr
                        \\
                        & x := x + Vy
                    \end{aligned}
            \end{aligned}\tag{6}
            \end{align}
            Each time, we increase the span of the subspace $\mathcal K, \mathcal L$, which gives us more space to choose the solution $x$, and more space to reduce the residual vector $r$. This idea is incredibly flexible, and we will see in later part where it reduces to a more concrete algorithm. Finally, when $\mathcal K = \mathcal L$, this is referred to as a Petrov Galerkin's Conditions. 
        
        \subsubsection{Norm Minimizations using Subspace}\label{sec:1.4}
            Other times, iterative method will choose to build up a subspace for each step with a subspace generator, and build up the solution on this expanding subspace, but with the additional objective of minimizing the residual under certain norm. Assuming that the vector $x\in x_0 + \mathcal{K}$, and we want to minimize the residual under a norm induced by positive definite operator $B$. Let it be the case that the columns of matrix $K$ span subspace $\mathcal{K}$ with $\dim(\mathcal K) = k$. 
            \begin{align}
                &\hspace{0.6em} \min_{x\in x_0 + \mathcal{K}} \Vert b - Ax\Vert_B^2 
                \\
                &= \min_{w\in \mathbb{R}^{k}} 
                \Vert b - A(x_0 + Kw)\Vert_B^2 & 
                \\
                &= \min_{w\in \mathbb{R}^{k}} 
                \Vert 
                    r_0 - AKw
                \Vert_B^2
            \end{align}
            We take the derivative of it and set the derivative to zero, this translate the problem to a projection problem under the $A$ norm. 
            \begin{align}
                \nabla_w \left[
                    \Vert r_0 - AKx\Vert_B^2
                \right] &= \mathbf{0}
                \\
                (AK)^TB(r_0 - AKx) &= \mathbf{0}
                \\
                (AK)^TBr_0 - (AK)^TBAKx &= \mathbf{0}
                \\
                (AK)^TBr_0 &= (AK)^TBAKx
            \end{align}
            The above formulation is tremendously powerful. I used gradient instead of projector for the simplicity of the argument. One can derive the same using projector but the math is bit more hedious. 
    \subsection{Krylov Subspace}
        A Krylov Subspace is a sequence basis paramaterized by $A$, an linear operator, $v$ an initial vector, and $k$, which basis in the sequece of basis that we are looking at. 
        \begin{definition}[Krylov subspace]
            \begin{align}
                \mathcal{K}_k(A|b) = \text{span}( b, Ab, A^2b, \cdots A^{k - 1}b)
            \end{align}
        \end{definition}
        \noindent
        Please immediately observe that from the definition we have: 
        \begin{align}
            \forall v: \mathcal{K}_1(A|v)  \subseteq  \mathcal{K}_2(A|v)  \subseteq \mathcal{K}_3(A|v)  \cdots 
        \end{align}
        Please also observe that, every element inside of krylov subspace generated by matrix $A$, and an initial veoctr $v$ can be represented as a polynomial of matrix $A$ multiplied by the vector $v$ and vice versa.
        \begin{align}
            & \forall x \in \mathcal K_k(A|v) \;\exists\; w: p_k(A|w)v = x
        \end{align}
        We use $p_k(A|w)$ to denotes a matrix polynomial with coefficients $w\in \mathcal K_j$, where $w$ is a vector. No proof this is trivial. Take note that, we can change the field of where the scalar $w_i$ is coming from, but for discussion below, $\mathbb R, \mathbb C$  doesn't matter and won't change the results so we stick to $\mathbb R$ and we let $v, A$ be real vectors and matrices so it's consistent. 
        \begin{align}
            p_k(A|w)v = \sum_{j = 0}^{k - 1}w_jA^jv
        \end{align}
        \subsubsection{The Grade of a Krylov Subspace}
            The most important porperty of the subspace is the idea of grade denoted as: $\text{grade}(A|v)$, indicating when the Krylov Subspace of $A$ wrt to $v$ stops expanding after a certain size. To show this idea, we consider 3 propositions about Krylov Subspace. And then we show that ir results in the existence of a grade of the Krylov Subspace, and then proceed to prove these 3 propositions we made. 
            \begin{prop}
                \begin{align}
                    \exists 1 \le k \le m + 1: \mathcal K_k(A|v) = \mathcal K_{k + 1}(A|v)
                \end{align}
                There exists an natural number between $1$ and $m+ 1$ such that, the successive krylov subspace span the same space asthe previous one. 
            \end{prop} 
            \begin{prop}
                \begin{align}
                    \exists \min k \text{ s.t: }\mathcal K_k(A|v) = \mathcal K_{k + 1}(A|v) \implies 
                    \mathcal K_k(A|v) \text{ is Lin Ind} \wedge \mathcal K_{k + 1}(A|v) \text{ is Lin Dep}. 
                \end{align}
                There eixsts a minimum such $k$ where the immediate next krylov subspace is linear dependent. 
            \end{prop}
            \begin{prop}
                \begin{align}
                    \mathcal K_k(A|v) \text{ Lin Dep} \implies \mathcal K_{k + 1}(A|v) = \mathcal K_k(A|v)
                \end{align}
                if the $k$ krylov subspace is linear dependent, then it stops expanding and the successive krylov subspace spans the same space. 
            \end{prop}
            \begin{theorem}[Existence of Grade for a Krylov Subspace]
                Let $k$ be the minumum number when the krylov subspace stops expanding, then all successive krylov subspace spand the same space. $\mathcal K_k(A|v) = \mathcal K_{k + j}(A|v) \;\forall j \ge 0$. The number $k$ is regarded as the grade of krylov subspace wrt to v denoted using $\text{grade}(A|v)$. 
            \end{theorem}
            Proposition 2, 3 ensures that there exists a term in the sequence of krylov subspace stops expanding, and when that happens all subsequent Krylov Subspace will span the same subspace, this is by Proposition 4. 
            \par
            Next, let's consider the proof of theorem 1 by proving all proposition 2, 3, and 4. 
            \begin{proof}[Proposition 2]
                For notational simplicity, $\mathcal K_k$ now denotes $\mathcal K_k(A|v)$. Let's start the considerations from the definition of the Krylov Subspace: 
                \begin{align}
                    \forall\; k: \mathcal K_k \subseteq \mathcal K_{k + 1}\implies \text{dim}(\mathcal K_{k})\le \text{dim}(\mathcal K_{k + 1})
                    \\
                    \mathcal K_{k + 1}\setminus \mathcal K_k = \text{span}(A^{k}v) 
                    \\
                    \implies \dim(\mathcal K_{k + 1}) - \dim(\mathcal K_k) \le 1
                \end{align}
                Therefore, the dimension of the successive krylov subspace forms a sequence of positive integer that is monotonically increasing. By the Cayley's Hamilton's theorem (will be stated later), the sequence is bounded by $m$, since there are $m + 1$ terms, it must be the case that at least 2 of the krylov subspace has the same dimension (And the earliest such occurance will exist), implying the the fact that the new added vector from $k$ to $k + 1$ is in the span of the previous subspace. 
            \end{proof}
            \begin{proof}[Proposition 4]
                \begin{align}
                    & \mathcal K_k \text{Lin Dep}
                    \\
                    \implies & \exists w_k \neq \mathbf 0 : p_k(A|w^+_k)v = \mathbf 0
                    \\
                    \implies & Ap_k(A|w_k^+)v = \mathbf 0
                    \\
                    & p_{k + 1}(A| [0 \; (w_k^+)^T]) = \mathbf 0
                    \\
                    & \mathcal K_{k + 1} \text{ is Lin Dep}
                \end{align}
                The recurrence of multplying by $A$ allows the krylov subspace to grow and the new bigger subspace will contain the previous one. Therefore inheriting the linear dependence, we use the idea of matrix polynomial for the proof. 
            \end{proof}
            \begin{proof}[Proposition 3]
                Assuming that prop 2, 4 are true. prop 3 implies the existence of the smallest such $k$. For contradiction, we only have one case to assume, that is $\mathcal K_k$ and $\mathcal K_{k + 1}$ are linear dependence. Then $\mathcal K_k$ is either Linear Dependence, or Independence. 
                \par
                If $\mathcal K_{k - 1}$ is linear dependence, then by (3) $\mathcal K_{k - 1} = \mathcal K_k$, hence $k$ is not the minimum. Else assume $\mathcal K_{k - 1}$ is linear independence, however $\mathcal K_k$ is linear dependence, and $\mathcal K_k \setminus \mathcal K_{k - 1} = \text{span}(A^{k - 1}v)$; therefore, $A^{k -1}v$ is in the span of $\mathcal K_{k -1}$, hence $\mathcal K_{k -1} = \mathcal K_k$, contradicing again that $k$ is the minimum such $k$. 
            \end{proof}
    \subsection{Minimal Polynomial of a Matrix}
        \begin{definition}
            A minimal polynomial is a polynomial $p_k(x)$ with degree k that is monic such that $p_k(A) = \mathbf{0}$. 
        \end{definition}
    \subsection{Useful Theorems}
        \subsubsection{Cauchy Interlace Theorem}

        \subsubsection{Caley Hamilton's Theorem}
            \begin{theorem}
                A matrix satisfies it's own characteristic equation, let $p(x)$ be the characteristic polynomial for the matrix $A$, then $p(A) = \mathbf 0$. 
            \end{theorem}
            The Calye's Hamilton's Theorem is important in the sense that, a direct consequence is the termination conditions for all krylov Subspace Methods (regardless of initial guess vectors). It's saying that all Krylov Subspace methods will terminates at step $n + 1$ at most, if $n$ is the size of the operator. However, the more important fact is that, when it terminates, the solution $x$ is a weighted sum of the Krylov Subspace vectors and the weights are related to the characterstic polynomial of the matrix. 

    \subsection{Deriving Conjugate Gradient from First Principles}
        \subsubsection{CG Objective and Framework}
            We introduce the algorithm as an attempt to minimize the energy norm of the error for a linear equation $Ax = b$, here we make the assumptions: 
            \begin{itemize}
                \item [1)] The matrix $A$ is symmetric semi-positive definite.  
                \item [2)] Further assume another matrix $P_k = [p_0 \;p_1\;\cdots p_{k-1}]$ as a matrix whose columns is a basis.
            \end{itemize}
            \begin{align}
                \min_{w \in \mathbb{R}^k}\Vert 
                    A^{-1}b - (x_0 + P_kw)
                \Vert_A^2 \iff P^T_kr_0 = P_k^TAP_kw
            \end{align}
            Refer back to \hyperref[sec:1.4]{(1.4)} for how to deal with the above minimization objective. Using the matrix form for the Petrov Galerkin Conditions where $W, V$ are both $P_k$, we reformulate the Norm Minimizations conditions under the framework of Petrov Galarkin conditions: 
            \begin{align}
                \text{choose: }x \in x_0 + \text{ran}(P_k) \text{ s.t: } b - Ax \perp \text{ran}(P_k)    
            \end{align}
            Take note that the link between a norm minimzation and an equivalent subspace Orthogonality conditions don't garantee to happen for other subspace projector methods, for example the FOM and Bi-Lanczos Methods are orthogonalizations method that doesn't directly link to a norm minimization objective. 
            \par
            To solve for $w$, we wish to make $P_k^TAP_k$ to be an easy-to-solve matrix. Let the easy-to-solve matrix to be a diagonal matrx and hence we let $P_k$ to be a \textit{matrix whose columns are A-Orthogonal vectors}.
            \begin{align}
                P^T_kAP_k &= D_k \text{ where: } (D_k)_{i,i} = \langle p_{i - 1}, Ap_{i - 1}\rangle
                \\
                P_kr_0 &= P^T_kAP_kw = D_kw
                \\
                w &= D^{-1}_kP_k^Tr_0
            \end{align}
            The idea here is: Accumulating vectors $p_j$ into the matrix $P_k$ and then iterative improve the solution $x_k$, by reducing the error denote as $e_k$ which is defined as $A^{-1}b - x_k$. Then, we can derive the following expression for the solution at step k $x_k$ and the residual at step $r_k = b - Ax_k$ for the algorithm: 
            \begin{align}
                \begin{cases}
                    x_k = x_0 + P_kD^{-1}_kP^T_kr_0
                    \\
                    r_k = r_0 - AP_kD^{-1}_kP^T_k r_0
                    \\
                    P^T_kAP_k = D_k
                \end{cases}
            \end{align}
            Let this algorithm be the prototype. 
        \subsubsection{Using the Projector}
            Here, we consider the simple algorithm from ????(hyperref needed). Please observe that $AP_kD_k^{-1}P_k$ is a projector, and so is $P_kD^{-1}_kP_k^TA$. 
            \begin{proof}
                \begin{align}
                    AP_kD^{-1}_kP_k^T(AP_kD^{-1}_kP_k^T) &= AP_kD^{-1}_kP_k^TAP_kD^{-1}_kP_k^T 
                    \\
                    &= AP_kD_k^{-1}D_kD_k^{-1}P_k^{T}
                    \\
                    &= AP_kD_k^{-1}P_k^T 
                    \\[1.1em]
                    P_kD^{-1}_kP_k^{T}A(P_kD^{-1}_kP_k^{T}A) &= P_kD^{-1}_kD_kD_{k}^{-1}P^T_kA
                    \\
                    &= 
                    P_kD^{-1}_kP^T_kA
                \end{align}
            \end{proof}
            \noindent
            Both matrices are indeed projectors. Please take note that they are not Hermitian, which would mean that they are not orthogonal projector, hence, oblique projectors. For notational convenience, we denote $\overline{P}_k = P_kD_k^{-1}P_k^{T}$; then these 2 projectros are: 
            \begin{align}
                AP_kD^{-1}_kP_k^T &= A\overline{P}_k 
                \\
                P_kD^{-1}_kP_k^TA &= \overline{P}_kA
            \end{align}
            One immediate consequence is: 
            \begin{align}
                & \text{ran}(I - A\overline{P}_k )\perp \text{ran}(P_k)
                \\
                & \text{ran}(I - \overline{P}_kA) \perp \text{ran}(AP_k)
            \end{align}
            \begin{proof}
                \begin{align}
                    P_k^T(I - A\overline{P}_k) &= P_k^T - P_k^{T}A\overline{P}_k
                    \\
                    &= P_k^{T} - D_kD_k^{-1}P^T_k
                    \\
                    &= \mathbf{0}
                    \\
                    (AP_k)^T(I - \overline{P}_kA) &=P_k^TA - P_k^TA\overline{P}_kA
                    \\
                    &= P_k^TA - P_k^TAP_kD_k^{-1}P_k^TA
                    \\
                    &= P_k^TA - P^T_kA 
                    \\
                    &= \mathbf{0}
                \end{align}
            \end{proof}
            Using the properties of the oblique projector, we can proof 2 facts about this simple norm minimization method we developed: 
            \begin{prop}[Residuals are Orthogonal to $P_k$]
                \begin{align}
                    r_k &= r_0 - A\overline{P}_kr_0 = (I - A\overline{P}_k)r_0
                    \\
                    \implies 
                    r_k &\perp \text{ran}(P_k)
                \end{align}
            \end{prop}
            \begin{prop}[Generating $A$ Orthogonal Vectors]
                Given any set of basis vector, for example $\{u_k\}_{i = 0}^{n - 1}$, one can generate a set of A-Orthogonal vectors from it. More specifically: 
                \begin{align}
                    p_k &= (I - \overline{P}_kA)u_k
                    \\
                    \text{span}(p_k) &\perp \text{ran}(AP_k)
                \end{align}
            \end{prop}
            For above propositions, we used the immediate consequence of the range of these oblique projectors. 
        \subsubsection{Assisted Conjugate Gradient}
            So far, we have this particular scheme of solving the optimization problem, coupled with the way to computing the solution $x_k$ at each step, and the residual at each step, while also getting the residual vector at each step too. However, it would be great if we can accumulate on the same subspace $P_k$ and look for a chance to reuse the computational results from the previous iterations of the algorithm: 
            \begin{align}
                \begin{cases}
                    x_k = x_0 + \overline{P}_k r_0
                    \\
                    r_k = (I - A\overline{P}_k) r_0
                    \\
                    P^T_kAP_k = D_k
                    \\
                    \overline{P}_k = P_kD^{-1}_kP_k^T
                    \\
                    p_k = (I - \overline{P}_kA)u_k & \{u_i\}_{i = 0}^{n - 1} \text{ is a Basis}
                \end{cases}
            \end{align}
            With the assitance of a set of basis vector that span the whole space, this algorithm is possible to achieve the objective. Take note that we can accumulate the solution for $x_k$ accumulatively, instead of computing the whole projector process, we have the choice to update it recursively as the newest $p_k$ vector is introduced at that step. Let's Call this formulation of the algorithm: \textit{Assisted Conjugate Gradient}. 
        \subsubsection{Properties of Assisted Conjugate Gradient}
            Here we setup several useful lemma and propositions that can derive the short recurrences of A-Orthogonal vectors 
            \begin{prop}
                \begin{align}
                    p_{k + j}^Tr_k &= p_{k + j}^Tr_0 \quad \forall \; 0 \le j \le n - k
                    \\
                    p_{k + j}^Tr_k &= p_k^T(I - A\overline{P}_k)r_0
                    \\
                    &= (p^T_{k + j} - p^T_{k + j}A\overline{P}_k)r_0
                    \\
                    &= p_{k + j}^Tr_0    
                \end{align}
                This made the recurrence between successive residual from the ACG possible. 
            \end{prop}
            Next, we wish to use this property to find out a recurrences for the residuals of ACG, and here is how we do it: 
            \begin{align}
                    r_k - r_{k - 1} &= r_0 - A\overline{P}_kr_0 - (r_0 - A\overline{P}_{k - 1}r_0)
                    \\
                    &= A\overline{P}_kr_0 - A\overline{P}_{k - 1}r_0
                    \\
                    &= - Ap_{k - 1}\frac{\langle p_{k - 1}, r_0\rangle}{\langle p_{k - 1}, Ap_{k - 1}\rangle}
                    \\
                    \implies 
                    x_{k} - x_{k - 1} &= 
                    p_{k - 1}\frac{\langle p_{k - 1}, r_0\rangle}{\langle p_{k - 1}, Ap_{k - 1}\rangle}
                    \\
                    \text{def: } a_{k - 1} &:= \frac{\langle p_{k - 1}, r_0\rangle}{
                        \langle p_{k - 1}, Ap_{k - 1}\rangle
                    } = 
                    \frac{\langle p_{k - 1}, r_{k - 1}\rangle}{
                        \langle p_{k - 1}, Ap_{k - 1}\rangle
                    }
            \end{align}
            We define the value of $a_{k - 1}$, and in above, we have 2 equivalent representation. Please take note that, Proposition still remains true for the ACG algorithm we just developed here. 
        \subsubsection{Residual Assisted Conjugate Gradient}
            Now, consider the case where, the set of basis vector: $\{u\}_{i = 0}^{n - 1}$ to be the residual vector generated from the ACG itself. Then there are a series of new added lemmas that are true. However, this is where things started to get exciting, because a short recurrence for $p_k$ during each iteration arised and residuals are all orthgonal. We wish to proceed to prove that part. 
            \begin{lemma}
                \begin{align}
                    \langle p_{k + j}, Ap_k\rangle
                    &=\langle r_k, Ap_{k + j}\rangle
                    = \langle p_{k + j}, Ar_k\rangle \quad \forall\; 0 \le j \le n - k
                \end{align}
            \end{lemma}
            \begin{proof}
                \begin{align}
                    p_{k + j} Ap_k &= p_{k + j}^TAr_k - p_{k + j}^TA\overline{P}_{k}Ar_k \quad 
                    \forall\; 0 \le j \le n - k
                    \\
                    &= p_{k + j}^TAr_k
                    \\
                    \langle p_{k + j}, Ap_k\rangle
                    &= \langle r_k, Ap_{k + j}\rangle
                    = \langle p_{k + j}, Ar_k\rangle
                \end{align}
            \end{proof}
            \begin{lemma}
                \begin{align}
                    \langle r_k, p_k\rangle &= \langle r_k, r_k\rangle
                \end{align}
            \end{lemma}
            \begin{proof}
                \begin{align}
                    \langle r_k, p_k\rangle &= 
                    \langle r_k, p_k\rangle
                    \\
                    &= \langle r_k, r_k\rangle - \langle r_k, \overline{P}_kAr_k\rangle
                    \\
                    &= \langle r_k, r_k\rangle
                \end{align}
                From the first line to the second line, we make use of the definition proposed. 
            \end{proof}
            \begin{prop}[Residual Assisted CG Generates Orthogonal Residuals]
                \begin{align}
                    \langle r_k , r_j \rangle = 0 \quad \forall\; 0 \le j \le k - 1 
                \end{align}
            \end{prop}
            \noindent
            Let this above claim be inductively true then consider the following proof: 
            \begin{proof}
                \begin{align}
                    r_{k + 1} &= r_k - a_kAp_k
                    \\
                    \implies 
                    \langle r_{k + 1}, r_k\rangle &= \langle r_k, r_k\rangle - 
                    a_k \langle r_k, Ap_k\rangle
                    \\
                    &= \langle r_k, r_k\rangle - 
                    \frac{\langle r_k, r_k\rangle}{\langle p_k, Ap_k\rangle}
                    \langle r_k, Ap_k\rangle
                    \\
                    &= 
                    0
                \end{align}
                The first line is from the reccurrence of ACG residuals, and then next we make use of the updated definition for $a_k$. Next we consider: 
                \begin{align}
                    p_j &= (I - \overline{P}_jA)r_j \quad \forall\; 0\le j \le k - 1
                    \\
                    r_j &= p_j + \overline{P}_jAr_j
                    \\
                    r_k &= (I - A\overline{P}_k)P_0
                    \\
                    r_k\perp \text{ran}(P_k) & \implies 
                    \langle r_k, r_j\rangle =\langle r_k, p_j + \overline{P}_jAr_j\rangle = 0
                \end{align}
                Here we again make use of the projector $I - A \overline{P}_k$. The base case of the argument is simple, because $p_0 = r_0$, and by the property of the projector, $\langle r_1, r_0\rangle = 0$. The theorem is now proven. 
            \end{proof}
            \begin{prop}[RACG Recurrences]
                \begin{align}
                    p_k &= r_k + b_{k - 1}p_{k - 1} \quad b_{k - 1} = \frac{\Vert r_k\Vert_2^2}
                    {\Vert r_{k - 1}\Vert_2^2}
                \end{align}
            \end{prop}
            The proof is direct and we start with the definition of ACG, which is given as: 
            \begin{proof}
                \begin{align}
                    p_k &= (I - \overline{P}_kA)r_k
                    \\
                    r_k - \overline{P}_kAr_k &= 
                    r_k - P_kD^{-1}_kP^T_kAr_k
                    \\
                    &= r_k - P_kD^{-1}_k(AP_k)^Tr_k
                \end{align}
                Observe that the term $(AP_k)^{T}$ can be expanded and we can make use of the Symmetric Property of the operator $A_k$. 
                \begin{align}
                    (AP_k)^Tr_k &= 
                    \begin{bmatrix}
                        \langle p_0, Ar_k\rangle
                        \\
                        \langle p_1, Ar_k\rangle
                        \\
                        \vdots
                        \\
                        \langle p_{k - 1}, Ar_k\rangle
                    \end{bmatrix}
                \end{align}
                Next, we can make use of Lemma 2 to get rid of $Ar_k$. Please consider: 
                \begin{align}
                    (AP_k)^Tr_k &= 
                    \begin{bmatrix}
                        \langle p_0, Ar_k\rangle
                        \\
                        \langle p_1, Ar_k\rangle
                        \\
                        \vdots
                        \\
                        \langle p_{k - 1}, Ar_k\rangle
                    \end{bmatrix}
                \end{align}
                The second line is using the property that the matrix $A$ is symmetric, the third line is using the recurrence of the residual of ACG, and the last line is true for all $0 \le j \le k - 2$ by the orhogonality of the residual proved in Claim 1. Therefore we have: 
                \begin{align}
                    (AP_k)^Tr_k &= 
                    \begin{bmatrix}
                        \langle p_0, Ar_k\rangle
                        \\
                        \langle p_1, Ar_k\rangle
                        \\
                        \vdots
                        \\
                        \langle p_{k - 1}, Ar_k\rangle
                    \end{bmatrix}
                    = 
                    a_{k - 1}^{-1}\langle r_k, (r_{k - 1} - r_{k})\rangle \xi_k
                \end{align}
                Take note that the vector $\xi_k$ is the k th standard basis vector in $\mathbb{R}^k$. And using this we can simplify the expression for $p_k$ into: 
                \begin{align}
                    p_k &= r_k - P_kD^{-1}_k(AP_k)^Tr_k
                    \\
                    &= r_k - P_kD_k^{-1}a_{k - 1}^{-1}(\langle r_k, (r_{k - 1} - r_{k})\rangle) \xi_k
                    \\
                    &= 
                    r_k - \frac{a_{k -1}^{-1}\langle -r_k, r_k\rangle}
                    {\langle p_{k - 1}, Ap_{k - 1}\rangle}p_k
                    \\
                    &= r_k + \frac{a_{k -1}^{-1}\langle r_k, r_k\rangle}
                    {\langle p_{k - 1}, Ap_{k - 1}\rangle}p_k
                    \\
                    &= r_k + 
                    \left(
                        \frac{\langle r_{k - 1}, r_{k - 1}\rangle}{\langle p_{k - 1}, Ap_{k - 1}\rangle}
                    \right)^{-1}
                    \frac{\langle r_k, r_k\rangle}{\langle p_{k - 1}, Ap_{k - 1}\rangle}p_k
                    \\
                    &= 
                    r_k + \frac{\langle r_k, r_k\rangle}{\langle r_{k - 1}, r_{k - 1}\rangle}p_k
                \end{align}
                We make use of the definition for $a_{k-1}$ in figured out for the $ACG$ algorithm. At this point, we have proven the RACG recurrences. 
            \end{proof}
            Up until this point we have proven the usual version of conjugate gradient, we started with the minimizations objective and the properties of $P_k$, then we define a recurrences for the residual (Simultaneously the solution $x_k$), and the A-Orthogonal vectors using a basis as assistance for the generations process. Next, we make the key changes of the assistance basis, making it equalto the set of residuals vector generated from the algorithm itself; after some proof, we uncovered the exact same parameters found in most of the definitions of the CG algorithm, which we refers to as Residual Assisted Conjugate Gradient. Here we proposed the RACG: 
            \begin{definition}[RACG]
                \begin{align}
                    & p^{(0)} = b - Ax^{(0)} 
                    \\&
                    \text{For } i = 0,1, \cdots
                    \\&\hspace{1.1em}
                    \begin{aligned}
                        & a_{i} = \frac{\Vert r^{(i)}\Vert^2}{\Vert p^{(i)}\Vert^2_A}
                        \\
                        & x^{(i + 1)} = x^{(i)} + a_i p^{(i)}
                        \\
                        & r^{(i + 1)} = r^{(i)} - a_iAp^{(i)}
                        \\
                        & b_{i} = \frac{\Vert r^{(j + 1)}\Vert_2^2}{\Vert r^{(i)}\Vert_2^2}
                        \\
                        & p^{(i + 1)} = r^{(i + 1)} + b_{i}p^{(i)}
                    \end{aligned}
                \end{align}
            \end{definition}
            That is the algorithm, stated with all the iteration number listed as a super script inside of a parenthesis. Which is equivalent to what we have proven for the Residual Assisted Conjugate Gradient. 
        \subsubsection{RACG and Krylov Subspace}
            The conjugate Gradient Algorithm is actually a residual assisted conjugate gradient, a special case of the algorithm we derived at the start of the excerp. The full algorithm can be seem by the short recurrence for the residual and the conjugation vector. This part is trivial. Next, we want to show the relations to the Krylov Subspace, which only occurs for the Residual Assisted Conjugate Gradient algorithm. 
            \begin{prop}
                \begin{align}
                    p_k \in \mathcal K_{k + 1}(A|r_0)
                    \\
                    r_k \in \mathcal K_{k + 1}(A|r_0)
                \end{align}    
            \end{prop}
            \begin{proof}
                The base case is tivial and it's directly true from the definition of Residual Assisted Conjugate Gradient: $r_0 \in \mathcal K_1(A|r_0), p_0 = r_0 \in \mathcal K_1(A|r_0)$. Next, we inductively assume that $r_k \in \mathcal K_{k + 1}(A|r_0), p_k \in \mathcal K_{k + 1}(A|r_0)$, then we consider: 
                \begin{align}
                    r_{k + 1} &= r_k - a_kAp_k
                    \\
                    &\in r_k + A\mathcal K_{k + 1}(A|r_0)
                    \\
                    &\in r_k + \mathcal K_{k + 2}(A|r_0)
                    \\
                    r_k 
                    &\in 
                    \mathcal K_{k + 1}(A|r_0) \subseteq \mathcal K_{k + 2}(A|r_0)
                    \\
                    \implies r_{k + 1}
                    &\in 
                    \mathcal K_{k + 2}(A|r_0)
                \end{align}
                At the same time the update of $p_k$ would asserts the property that: 
                \begin{align}
                    p_{k + 1} &= r_{k + 1} + b_kp_k
                    \\
                    &\in 
                    r_{k + 1} + \mathcal K_{k + 1}(A|r_0)
                    \\
                    &\in \mathcal K_{k + 2}(A|r_0)
                \end{align}
                This is true because $r_{k + 1}$ is already a member of the expanded subspace $\mathcal K_{k + 2}(A|r_0)$. And from this formulation of the algorithm, we can update the Petrov Galerkin's Conditions to be: 
                \begin{align}
                    \text{choose: } x_k\in x_0 + \mathcal K_{k}(A|r_0) \text{ s.t: } r_k \perp \mathcal K_{k}(A|r_0)
                \end{align}
                Take note that, $\text{ran}(P_k) = \mathcal K_k(A|r_0)$ because the index starts with zero. The above formulations gives theoretical importance for the Conjugate Gradient Algorithm. 
            \end{proof}
    \subsection{Anoldi Iterations and Lanczos}
        We first define the Arnoldi Algorithm, and then we proceed to derive it using the idea of orthgonal projector. Next, we discuss a special case of the Arnoldi Iteration: the Lanczos Algorithm, which is just Arnoldi applied to a symmetric matrix. And such algorithm will inherit the properties of the Arnoldi Iterations. 
        \par
        Before stating the algorithm, I would like to point out the interpreations of the algorithm and its relations to Krylov Subspace. Consider a matrix of Hessenberg Form:
        \begin{align}
            \tilde{H}_k = 
            \begin{bmatrix}
                h_{1, 1} & h_{1, 2} & \cdots & h_{1, k} 
                \\
                h_{1, 2} & h_{2, 2} & \cdots & h_{2, k}
                \\
                \\
                & \ddots & &\vdots
                \\
                & & h_{k, k - 1}& h_{k, k}
                \\
                & & & h_{k + 1, k}
            \end{bmatrix}
        \end{align}
        We initialize the orhtogonal projector with the vector $q_1$, which is $q_1q_1^H$, next, we apply the linear operator $A$ on the current range of the projector: $Aq_1$, then, we orthogonalize it against $q$. Let the projection of $Aq_1$ onto $I - q_1q_1^H$ be $h_{1, 2}q_2$, and let the projection onto $q_1q_1^H$ be $h_{1,1}$. This completes the first column of $H_k$, we do this recursively. Please allow me to demonstrate: 
        \begin{align}
            (\tilde{H}_k)_{2, 1}q_2 &= (I - q_1q_1^H)Aq_1
            \\
            (\tilde{H}_k)_{1, 1}q_1 &= q_1q_1^HAq_1
            \\
            Q_2 &:= \begin{bmatrix}
                q_1 & q_2
            \end{bmatrix}
            \\
            (\tilde{H}_k)_{3, 2}q_3 &= (I - Q_2Q_2^H)Aq_2
            \\
            (\tilde{H}_k)_{1:2, 2} &= Q_2Q_2^HAq_2
            \\
            Q_3 &:= \begin{bmatrix}
                q_1 & q_2 & q_3
            \end{bmatrix}
            \\
            & \vdots 
            \\
            Q_j &:= \begin{bmatrix}
                q_1 & q_2 & \cdots & q_j
            \end{bmatrix}
            \\
            (\tilde{H}_k)_{j + 1, j}q_{j + 1}&= (I - Q_jQ_j^H)Aq_j
            \\
            (\tilde{H}_k)_{1:j, j} &= Q_jQ_j^HAq_j
            \\
            &\vdots 
            \\
            Q_k &:= \begin{bmatrix}
                q_1 & q_2 & \cdots & q_k
            \end{bmatrix}
            \\
            (\tilde{H}_k)_{k + 1, k}q_{k + 1}&= (I - Q_kQ_k^H)Aq_k
            \\
            (\tilde{H}_k)_{1:k, k} &= Q_kQ_k^HAq_k
        \end{align}
        Reader please observe that $Q_k$ is going to be orthogonal because how at the start, $q_1q_1^H$ and $I - q_1q_1^H$ is giving us an orthogonal subspace. As a consequence, we can express the recurrences of the subspace vector in matrix form: 
        \begin{align}
            AQ_{k} &= Q_{k + 1}\tilde{H}_k
            \\
            Q_{k}^HAQ_{k} &= H_k
        \end{align}
        And here, we explicitly define $H_k$ to be the principal submatrix of $\tilde{H}_k$. Reader please immediately observe that, if $A$ is symmetric, then it has to be the case that $Q^H_kAQ_k$ is also symmetric, which will make $H_k$ to be symmetric as well, which implies that $H_k$ will be a Symmetric Tridiagonal Matrix. And under that assumption, we can develop the Lanczos Algorithm. Instead of orthogoalizing against all previous vectors, we have the option to simply orthogonalize against the previous $q_k, q_{k - 1}$ vector. And we can reuse the sub-diagonal elements for $q_{k - 1}$; giving us the Lanczos Algorithm in the following form: 

        \begin{definition}[Lanczos Iterations]
            \begin{align}
                & \text{Given arbitrary: } q_1 \text{ s.t: } \Vert q_1\Vert = 1
                \\
                & \text{set: }\beta_0 = 0
                \\
                & \text{For } j = 1, 2, \cdots 
                \\
                &\hspace{1.1em}\begin{aligned}
                    & \tilde{q}_{j + 1} := Aq_j - \beta_{j - 1}q_{j - 1}
                    \\
                    & \alpha_j := \langle q_j,\tilde{q}_{j + 1}\rangle
                    \\
                    & \tilde{q}_{j + 1} \leftarrow \tilde{q}_{j + 1} - \alpha_j q_j
                    \\
                    & \beta_j = \Vert \tilde{q}_{j + 1}\Vert
                    \\
                    & q_{j + 1} := \tilde{q}_{j + 1}/\beta_j
                \end{aligned}
            \end{align}
        \end{definition}
        Often time, we refers the $k\times k$ symmetric tridiagonal matrix generated from Iterative Lanczos as $T_k$. 

        
\section{Analysis of Conjugate Gradient and Lanczos Iterations}
    \subsection{Terminations Conditions of RACG}
    \subsection{Terminations Conditions of The Lanczos Iterations}
    \subsection{Convergence Rate of RACG under Exact Arithematic}
    \subsection{The Equivalence between the Lanczos and RACG}

\section{Effects of Floating Point Arithematic}
\section{Appendix}
\section{Biliography}
            
            

\end{document}
