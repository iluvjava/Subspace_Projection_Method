\documentclass[]{article}
\usepackage{amsmath}\usepackage{amsfonts}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{courier}

\usepackage{fancyvrb}
\usepackage{beramono}
\usepackage{mathtools}
\usepackage{hyperref}

\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[T1]{fontenc}
\usepackage[margin=1in,footskip=0.25in]{geometry}  % margin, foot skip
\usepackage[fontsize=12pt]{fontsize}
\usepackage[font=tiny]{caption}
\usepackage{wrapfig}
\usepackage{float}
\usepackage[final]{graphicx}


\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\newcommand{\indep}{\perp \!\!\! \perp}
\graphicspath{{.}}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}            % Theorem counter global 
\newtheorem{prop}{Proposition}[section]  % proposition counter is section
\newtheorem{lemma}{Lemma}[subsection]    % lemma counter is subsection
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}[subsection]
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

%%
%% Julia definition (c) 2014 Jubobs
%%

\lstdefinelanguage{Julia}%
  {morekeywords={abstract,break,case,catch,const,continue,do,else,elseif,%
      end,export,false,for,function,immutable,import,importall,if,in,%
      macro,module,otherwise,quote,return,switch,true,try,type,typealias,%
      using,while},%
   sensitive=true,%
   alsoother={$},%
   morecomment=[l]\#,%
   morecomment=[n]{\#=}{=\#},%
   morestring=[s]{"}{"},%
   morestring=[m]{'}{'},%
}[keywords,comments,strings]%

\lstset{%
    language         = Julia,
    basicstyle       = \ttfamily,
    keywordstyle     = \bfseries\color{blue},
    stringstyle      = \color{magenta},
    commentstyle     = \color{ForestGreen},
    showstringspaces = false,
}

\linespread{1}  % double spaced or single spaced


\begin{document}
\numberwithin{equation}{subsection}
%\counterwithin*{equation}{subsubsection}
\begin{center}
    \vspace*{1cm}
    \textbf{Thesis Title}
        Add later I forgot what title I submitted for my thesis along with my graduation request
    \vspace{1.5cm}

    \textbf{Author Name}: Hongda Li

    \vfill
         
    A thesis presented for the degree of: ???? 
         
    \vspace{0.8cm}
         
    Department Name: Department of Applied Mathematics\\

    University Name: University of Washington\\

    

\end{center}

    \begin{center}
        Abstract
    \end{center}

    This thesis is expository in nature and we intend to discuss the Conjugate Gradient without pre-conditioning toether with the Lanczos iteration in thorough details. We derive the Conjugate Gradient algorithm using the Conjugate Direction method and projector. We derive Lanczos iterations as the symmetric case of Arnoldi iterations. We analyze the convergence rate of Conjugate Gradient in exact arithmetic and prove the equivalence between the Conjugate Gradient algorithm and the Lanczos iterations. In the end, we analyze the behaviors of Lanczos iterations and Conjugate Gradient under floating point arithematic, illustrate some of their behaviors under floating point arithematic using numerical experiments. 
         

\newpage
\tableofcontents

\newpage 


\section{Notations}
\begin{enumerate}
    \item $\text{ran}(A):=\{Ax :\forall x \; \in \mathbb R^n\}, A \in \mathbb R^{m\times n}$, The range of a matrix. 
    \item $(A)_{i, j}$: The element in i th row and jth column of the matrix $A$.
    \item $(A)_{i:i', j:j'}$: The submatrix whose top left corner is the $(i, j)$ element in matrix $A$, and whose' right bottom corner is the $(i', j ')$ element in the matrix $A$. The notation is similar to matlab's rules for indexing. 
    \item $\forall\; 0\le j \le k$: under certain context it indicates the range for an index: $j = 0, 1, \cdots, k-1, k$
    \item Boldface $\mathbf 0$ denotes the zero vector or matrix, depending on the context it can be either a zero row/column vector, or a zero matrix.
    \item The $\hat{}$ decorator is reserved for denoting the unit vector of some non zero vector. For example $\hat{x}:= x/\Vert x\Vert, x \neq \mathbf 0$. 
    \item $p_k(A|w)$ denotes the matrix polynomial $\sum_{j = 0}^k w_jA^j$. 
\end{enumerate}

\section{Introduction}
    The Conjugate Gradient method is an iterative method used for solving linear systems which date back to the period when computers were programmed using punched cards. It didn't receive much attention at the start but was revised and reappeared as a method for solving large sparse linear systems decades later, becoming the best option for positive definite linear systems that are sparse and large and, by extension, for optimizing strongly convex functions as well. In this thesis, we discuss Conjugate Gradient without pre-conditioning by deriving it and analyzing it along with Lanczos Iterations, a tightly related algorithm for symmetric eigenproblems. Finally, we use their connections to analyze its behaviour under floating-point arithmetic. The thesis will require some background in numerical linear algebra for the best understanding. 
    \par
    In the first section, we introduce the projectors and Krylov subspace as important mathematical objects for the subspace projections method, the frameworks for subspace projection methods, and the Lanczos Iteration as a symmetric case of the Arnoldi Iterations. At the end of the section, we proceed and derive the conjugate gradient method using only these ideas and concepts. In the second section, we analyze the behaviors of Conjugate gradient and Lanczos Iterations, including the terminations conditions for both algorithm, their convergence rate, and the equivalence between them. The goal is to show how they can be the same and have similar properties and how the connections between them can spark other methods for solving a symmetric indefinite linear system. And in the final parts of the second section, we derive the convergence bound for the conjugate gradient algorithm. 
    \par
    In the third section, we demonstrate the behaviors of Lanczos Iterations and Conjugate Gradient numerically and then show that the algorithm is backward stable. We show how floating points affect the Lanczos algorithm more rigor and how it may be fixed and mitigated, consequently improving the conjugate gradient algorithm. 

    \newpage
\begin{center}\large
    \textbf{Acknowledgement}
    Thanks to Professor Greenbaum taking the time reviewing the first draft of this thesis and her help throughout my research throughout the year. 
\end{center}


\newpage
\section{Foundations}
    In this section, we go over the foundations of Conjugate Gradient and the Lanczos Algorithm. We introduce the important ideas at the beginning and then we proceed to prove the conjugate gradient algorithm via the method conjugate directions, and then we state the Lanczos Iterations as a symmetric case of the Arnoldi Iterations. 
    \subsection{The Basics}
        In this subsection we go over some basics concepts and mathematical entities that are important to the Subspace Projection methods in general. 
        \subsubsection{Krylov Subspace}
            \begin{definition}[Krylov Subspace]
                $$
                \mathcal{K}_k(A|b) = \text{span}( b, Ab, A^2b, \cdots A^{k - 1}b)
                $$
                Observe that, for every element in the subspace, it's a matrix polynomial, we write it as $p_{k-1}(A|w)$, a $k-1$ degree matrix polynomial where $A$ is the matrix and $w$ is a vector denoting the coefficients. 
            \end{definition}
            \begin{definition}[The Grade of Krylov Subspace]
                The grade of the krylov subspace is the the smallest $\mathcal K_{k}(A|v)$ such that the vectors in the subspace are linear dependent, denoted as $\text{grade}(A|v)$. 
            \end{definition}
            The word grade of a krylov Subspace is used in Y.Saad's work (\textbf{CITATION NEEDED}) in reference to Krylov Subspace. And once the grade is reached, the Krylov Subspace becomes an invariant subspace to the matrix $A$. For a proof, refers to \hyperref[prop:Krylov_Subspace_Grade_Invariant_Theorem]{Krylov Subspace Grade Invariant Theorem} in the appendix. This concept is useful for determining the termination conditions for Lanczos iterations.  
            \begin{prop}[When the Grade is Reached]\label{prop:When_the_Grade_is_Reached}
                Assuming that matrix $A$ is diagonalizable, $A = V\Lambda V^{-1}$, then $\text{grade}(A|u)$ is the number of unique $\lambda_i$ such that $(V^{-1}u)_i$ is non-zero. 
            \end{prop}
            \begin{proof}
                Let $k$ be the grade, then it's the minimum number such that the matrix polynomial of $A$ multiplied by $u$ equals to the zero vector, and it's nontrivial. 
                \begin{align}
                    & \mathbf 0 = \sum_{j = 0}^{k}
                    w_jA^{j}u
                    \\
                    & \mathbf 0 = V\sum_{j = 0}^{k} w_j\Lambda^jV^{-1}u
                    \\
                    & \forall i \quad 0 = \sum_{j = 0}^{k} w_j\lambda_i^{j}(V^{-1}u)_i
                \end{align}
                When non trivial solution exists for some $w_j \neq 0$, it will be the case that whenever $(V^{-1}u)_i$ is not zero, then the a polynomial will have to interpolate $\lambda_i$. Therefore, minimum $k$ is the number of unique $\lambda_i$ such that $(V^{-1}u)_i\neq 0$
            \end{proof}
            
        \subsubsection{Projectors}
            \begin{definition}
                A matrix $P$ is a projector when $P^2 = P$, we call this property idempotent. 
            \end{definition}
            There are 2 types of projectors, oblique and orthogonal projectors. A projector is orthogonal projector when it's Hermitian, it's oblique when it's not Harmitian. 
            \begin{prop}[Projector Complementary]
                The projector $I - P$ projects onto the null space of $P$ and vice versa. 
                \begin{align}
                    & \text{ran}(P) = \text{null}(I - P)
                    \\
                    & \text{ran}(I - P) = \text{null}(P)
                \end{align}
            \end{prop}
            The proof is immediate from the definition. For more coverage of facts, refer to Trefethen's Book on Numerical Linear Algebra(\textbf{CITATION HERE}).

    \subsection{Subspace Projection Methods}\label{sec:Subspace_Projection_Methods}
        Let $\mathcal K, \mathcal L$ be 2 subspaces. $\mathcal K$ can be viewed as a subspace where we choose our solutions for the $x$ in the linear system $Ax = b$, and $\mathcal L$ is a subspace where we test our residual $r = b - Ax$. This is a description of this framework: 
        \begin{align}
            \text{choose }\tilde{x} \in x_0 + \mathcal{K} \text{ s.t: } b - A\tilde{x} \perp \mathcal{L}
        \end{align}
        We look for a $x$ in the affine linear subspace $x_0 + \mathcal{K}$ such that it's perpendicular to the subspace $\mathcal{L}$, or, equivalently, minimizing the projection onto the subspace $\mathcal{L}$.  
        \par
        The above conditions can be expressed using matrix. 
        \begin{align}
            \text{Let } V \in \mathbb{R}^{n\times m} \text{ be a aasis for: }\mathcal{K}
            \\
            \text{Let } W \in \mathbb{R}^{n\times m} \text{ be a basis for: } \mathcal{L}
        \end{align}
        Substituting the matrices to the above set of conditions: 
        \begin{align}
            \tilde{x} &= x_0 + Vy
            \\
            \text{choose } x \text{ s.t: } b - A\tilde{x}  &\perp \text{ran}(W)
            \\
            \implies W^T(b - Ax_0 - AVy) &= \mathbf{0}
            \\
            W^Tr_0 - W^TAVy&= \mathbf{0}
            \\
            W^TAVy &= W^Tr_0
        \end{align}
        \subsubsection{Prototype Algorithm}
            And from here, we can define a simple prototype algorithm using this framworks. 
            \begin{align}
                \begin{aligned}
                    & \text{While not converging}: 
                    \\&\hspace{1.1em}
                            \begin{aligned}
                            &\text{Increase Span for: } \mathcal{K, L}
                            \\
                            &\text{Choose: } V, W \text{ for } \mathcal{K}, \mathcal{L}
                            \\
                            & y:= (W^TAV)^{-1}W^Tr_0
                            \\
                            & x:= x+ Vy
                            \\
                            & r:= r_0 - AVy
                        \end{aligned}
                \end{aligned}
            \end{align}
            Each time, we increase the span of the subspace $\mathcal K, \mathcal L$, which gives us more space to choose the solution $x$, and more space to reduce the residual vector $r$. This idea is incredibly flexible, and we will see in later part where it reduces to a more concrete algorithm. Finally, when $\mathcal K = \mathcal L$, this is referred to as a Petrov Galerkin's Conditions (\textbf{CHECK THIS}). 
            \\
            \begin{remark}[Projector in the Prototype Algorithm]
                One may proceed to find a projector in the above prototype algorithm. 
                \begin{align}
                    & y = (W^TAv)^{-1}W^Tr_0
                    \\
                    & AVy = AV(W^TAV)^{-1}W^{T}r_0
                    \\
                    & x = x_0 + AVy
                    \\
                    & b - Ax = b - Ax_0 - AVy
                    \\
                    & r = r_0 - AV(W^TAV)^{-1}W^Tr_0
                \end{align}
                In here, $AV(W^TAV)^{-1}W^T$ is a projector, assuming the invertibility of $W^TAV$. 
            \end{remark}
        
        \subsubsection{Energy Norm Minimization using Gradient}\label{sec:Energy_Norm_Minimization_using_Gradient}
            Other times, iterative method will choose to build up a subspace for each step with a subspace generator, and build up the solution on this expanding subspace, but with the additional objective of minimizing the residual under some norm. Assuming that the vector $x\in x_0 + \mathcal{K}$, and we want to minimize the residual under a norm induced by positive definite operator $B$. Let it be the case that the columns of matrix $K$ span subspace $\mathcal{K}$ with $\dim(\mathcal K) = k$, then one may consider using gradient as a more direct approach instead of projector. 
            \begin{align}
                &\hspace{0.6em} \min_{x\in x_0 + \mathcal{K}} \Vert b - Ax\Vert_B^2 
                \\
                &= \min_{w\in \mathbb{R}^{k}} 
                \Vert b - A(x_0 + Kw)\Vert_B^2 & 
                \\
                &= \min_{w\in \mathbb{R}^{k}} 
                \Vert 
                    r_0 - AKw
                \Vert_B^2
            \end{align}
            We take the derivative of it and set the derivative to zero, skipping the proof that the derivative of $\nabla_x[\frac{1}{2}\Vert x\Vert_A^2] = Ax$ and just apply this formula for computing the gradient $\nabla_x[f(Ax)] = A^T\nabla [f(x)]$ where $f(x)$ is a mapping from $\mathbb R^n$ to $\mathbb R$. 
            \begin{align}
                \nabla_w \left[
                    \Vert r_0 - AKx\Vert_B^2
                \right] &= \mathbf{0}
                \\
                (AK)^TB(r_0 - AKx) &= \mathbf{0}
                \\
                (AK)^TBr_0 - (AK)^TBAKx &= \mathbf{0}
                \\
                (AK)^TBr_0 &= (AK)^TBAKx
            \end{align}
            The above formulation is powerful. I used gradient instead of projector for the simplicity of the argument. One can derive the same using orhtogonal projector to minimizes the 2 norm, but the math is bit more tedious. However, this minimization objective is minimizing the residual, which is fine for deriving subspace methods such as the GMRes, or the Minres and Orthomin, however, for the sake of the conjugate gradient, we have to consider the altenative. Let's this be a proposition that we proceed to prove. 
            \begin{prop}[Conditions for Minimum Error Under Energy Norm]
                Here, we let matrix $B$ be positive definite so it can induce a norm, we let $K$ be a matrix whose columns forms a basis for $\mathcal K$, we let $e_k$ denotes the error, given by: $A^{-1}b - x_k$, and we let $r_k$ denotes the residual given as $b - Ax_k$. 
                \begin{align}
                    \min_{x_k\in x_0 + \mathcal K}\Vert A^{-1}b - x\Vert_B^2
                    \iff
                    K^TBe_0 - K^TBKw &= \mathbf 0
                \end{align} 
            \end{prop}
            Next, we proceed to prove it and explain its interpretations and importance: 
            \begin{proof}
                \begin{align}
                    \min_{x \in x_0 + \mathcal K}
                    \Vert A^{-1}b - x\Vert_B^2
                    &= 
                    \min_{x\in \mathbb R} 
                    \Vert A^{-1}b - x_0 - Kw\Vert_B^2
                    \\
                    &= \min_{x\in \mathbb R^k}
                    \Vert e_0 - Kw\Vert_B^2
                \end{align}
                To attain the minimum of the norm, we take the derivative and set it to be zero, giving us: 
                \begin{align}
                    \mathbf 0 &= \nabla_w[\Vert e_0 - Kw\Vert_B^2]
                    \\
                    &= \nabla_w[e_0 - Kw]^TB(e_k - Kw)
                    \\
                    &= 2K^TB(e_0 - Kw)
                    \\
                    \implies 
                    K^TBe_0 - K^TBKw &= \mathbf 0
                \end{align}
            \end{proof}
            This conditions is implicity describing the objective of a Preconditioned Conjugate Gradient algorithm, where $B$ is the $M^{-1}$ matrix (\textbf{VERIFICATION NEEDED}), however this discussion right now it's a digression. Instead let's set $B$ to $A$, so that it's equivalent to the Energy Norm minimization of Conjugate Gradient, giving us this conditions: 
            \begin{align}
                K^TAA^{-1}r_0 - K^TAKw &= \mathbf 0
                \\
                K^Tr_0 - K^TAKw &= \mathbf 0
            \end{align}
            Here, we jsut made the substitution of $e_k = A^{-1}r_k$, and $B = A$. Later, we will see how this condition is linked to the idea of an Oblique Projector, similar to how an Orthogonal Projector is able to minimize the 2-Norm of the residual. 

     

    \subsection{Deriving Conjugate Gradient from Conjugate Directions}
        By the time this is being written, it's been 70 years since the first time Conjugate Gradient algorithm is proposed by Hestenes and Stiefel back in 1952(\textbf{CITATION NEEDED}). Upon their first discussion of the algorithm, numerious perspectives were explored. Three of the most important ideas are using Conjugate Directions, minimizing the energy norm of the error of the linear system and coming up with an update of the conjugate vectors using the residual vector at the current iterations. Here, we use the exact same idea but we diverge from Hestenes and Stiefel's approach in favor of using the oblique projector and the subspace orthogonality conditions to derive it. The ideas are rehashed and in the end we point out its relations to Krylov Subspace, which ultimately, leads to other important new ideas that are not yet present back in 1952. In most time under classroom settings or textbook, the relations of Conjugate Gradient, Lanczos Iterations and Krylov Subspaces are discussed together to explain some of the more important properties of the algorithm so we can move on and talk about other things. In this section of the paper, we take a different approach that similar in spirit to a coursenotes by Shewchuk (\textbf{CITATION NEEDED}) where we focuses more on that inspirations behind the algorithm and disregard Lanczos Algorithms and Krylov Subspace until the very end.  
        \subsubsection{CG Objective and Framework}
            We introduce the algorithm as an attempt to minimize the energy norm of the error for a system of linear equations $Ax = b$ and we make the assumptions: 
            \begin{itemize}
                \item [1)] The matrix $A$ is symmetric positive definite.  
                \item [2)] Further assume another matrix $P_k = [p_0 \;p_1\;\cdots p_{k-1}]$ as a matrix whose columns is a basis for $x_k$.
            \end{itemize}
            \begin{align}
                \min_{w \in \mathbb{R}^k}\Vert 
                    A^{-1}b - (x_0 + P_kw)
                \Vert_A^2 \iff P^T_kr_0 = P_k^TAP_kw
            \end{align}
            Refer back to \hyperref[sec:Energy_Norm_Minimization_using_Gradient]{Energy Norm Minimization using Gradient (3.2.2)} for how to obtain the above minimization objective. Using the matrix from the \hyperref[sec:Subspace_Projection_Methods]{subspace projection method (3.2)} where $W, V$ are both $P_k$, we reformulate the norm minimizations conditions as: 
            \begin{align}
                \text{choose: }x \in x_0 + \text{ran}(P_k) \text{ s.t: } b - Ax \perp \text{ran}(P_k)    
            \end{align}
            Take note that the link between a norm minimzation and an equivalent subspace orthogonality conditions don't garantee to happen for other subspace projection methods, for example the FOM and Bi-Lanczos Methods are orthogonalizations method that doesn't directly link to a norm minimization objective (\textbf{CITATION NEEDED}). 
            \par
            To solve for $w$, we wish to make $P_k^TAP_k$ to be an easy-to-solve matrix. Let the easy-to-solve matrix to be a diagonal matrix and hence we let $P_k$ to be a \textit{matrix whose columns are A-Orthogonal vectors}.
            \begin{align}
                P^T_kAP_k &= D_k \text{ where: } (D_k)_{i,i} = \langle p_{i - 1}, Ap_{i - 1}\rangle
                \\
                P_kr_0 &= P^T_kAP_kw = D_kw
                \\
                w &= D^{-1}_kP_k^Tr_0
            \end{align}
            The idea here is: Accumulating vectors $p_j$ into the matrix $P_k$ and then iteratively improve the solution $x_k$ by reducing the error denote as $e_k$ defined as $A^{-1}b - x_k$. Then, we derive the following expression for $x_k$ and the residual $r_k = b - Ax_k$: 
            \begin{align}
                \begin{cases}
                    x_k = x_0 + P_kD^{-1}_kP^T_kr_0
                    \\
                    r_k = r_0 - AP_kD^{-1}_kP^T_k r_0
                    \\
                    P^T_kAP_k = D_k
                \end{cases}
            \end{align}
            Let this algorithm be the prototype. 
        \subsubsection{Using the Projector}
            Here, we consider the above prototype algorithm. Please observe that $AP_kD_k^{-1}P_k$ is a projector, and so is $P_kD^{-1}_kP_k^TA$. 
            \begin{proof}
                \begin{align}
                    AP_kD^{-1}_kP_k^T(AP_kD^{-1}_kP_k^T) &= AP_kD^{-1}_kP_k^TAP_kD^{-1}_kP_k^T 
                    \\
                    &= AP_kD_k^{-1}D_kD_k^{-1}P_k^{T}
                    \\
                    &= AP_kD_k^{-1}P_k^T 
                    \\[1.1em]
                    P_kD^{-1}_kP_k^{T}A(P_kD^{-1}_kP_k^{T}A) &= P_kD^{-1}_kD_kD_{k}^{-1}P^T_kA
                    \\
                    &= 
                    P_kD^{-1}_kP^T_kA
                \end{align}
            \end{proof}
            \noindent
            Both matrices are indeed projectors. Please take note that they are not Hermitian, which would mean that they are not orthogonal projector, hence, oblique projectors. For notational convenience, we denote $\overline{P}_k = P_kD_k^{-1}P_k^{T}$; then these 2 projectros are: 
            \begin{align}
                AP_kD^{-1}_kP_k^T &= A\overline{P}_k 
                \\
                P_kD^{-1}_kP_k^TA &= \overline{P}_kA
            \end{align}
            One immediate consequence is: 
            \begin{align}
                & \text{ran}(I - A\overline{P}_k )\perp \text{ran}(P_k)
                \\
                & \text{ran}(I - \overline{P}_kA) \perp \text{ran}(AP_k)
            \end{align}
            \begin{proof}
                \begin{align}
                    P_k^T(I - A\overline{P}_k) &= P_k^T - P_k^{T}A\overline{P}_k
                    \\
                    &= P_k^{T} - D_kD_k^{-1}P^T_k
                    \\
                    &= \mathbf{0}
                    \\
                    (AP_k)^T(I - \overline{P}_kA) &=P_k^TA - P_k^TA\overline{P}_kA
                    \\
                    &= P_k^TA - P_k^TAP_kD_k^{-1}P_k^TA
                    \\
                    &= P_k^TA - P^T_kA 
                    \\
                    &= \mathbf{0}
                \end{align}
            \end{proof}
            Using the properties of the oblique projector, we can proof 2 facts about this simple norm minimization method we developed: 
            \begin{prop}[Residuals are Orthogonal to $P_k$]
                \begin{align}
                    r_k &= r_0 - A\overline{P}_kr_0 = (I - A\overline{P}_k)r_0
                    \\
                    \implies 
                    r_k &\perp \text{ran}(P_k)
                \end{align}
            \end{prop}
            \begin{prop}[Generating $A$ Orthogonal Vectors]
                Given any set of basis vector, for example $\{u_k\}_{i = 0}^{n - 1}$, one can generate a set of A-Orthogonal vectors from it. More specifically: 
                \begin{align}
                    p_k &= (I - \overline{P}_kA)u_k
                    \\
                    \text{span}(p_k) &\perp \text{ran}(AP_k)
                \end{align}
            \end{prop}
            For above propositions, we used the immediate consequence of the range of these oblique projectors. 
        \subsubsection{Method of Conjugate Directions}
            So far, we have this particular scheme of solving the optimization problem, coupled with the way to computing the solution $x_k$ at each step, and the residual at each step, while also getting the residual vector at each step too. However, it would be great if we can accumulate on the same subspace $P_k$ and look for a chance to reuse the computational results from the previous iterations of the algorithm: 
            \begin{definition}[Method of Conjugate Directions]
                \begin{align}
                    \begin{cases}
                        x_k = x_0 + \overline{P}_k r_0
                        \\
                        r_k = (I - A\overline{P}_k) r_0
                        \\
                        P^T_kAP_k = D_k
                        \\
                        \overline{P}_k = P_kD^{-1}_kP_k^T
                        \\
                        p_k = (I - \overline{P}_kA)u_k & \{u_i\}_{i = 0}^{n - 1} \text{ is a Basis}
                    \end{cases}
                \end{align}
            \end{definition}
            With the assitance of a set of basis vector that span the whole space, this algorithm is possible to achieve the objective. Take note that we can accumulate the solution for $x_k$ accumulatively, instead of computing the whole projector process, we have the choice to update it recursively as the newest $p_k$ vector is introduced at that step. Let's Call this formulation of the algorithm: \textit{Conjugate Direction Method }(CDM). 
            \begin{remark}
                This CDM method is nothing new, in the original paper from Hestenes and Stiefel back in 1952(\textbf{CITATION NEEDED}), they commented on the method of Conjugate Direction, for each chose of basis $\{u_i\}_{i = 1}^n$ there resides a unique algorithm. If one were to choose the basis to be the set of standard basis vector, then the resulting algorithm will be the equivalent of a Gaussian Eliminations. 
            \end{remark}
            \begin{remark}[Geometric Intutions of CDM]
                What is happening geometrically is that the A-Orthogonal vectors are orthogonal if desribe it under the alternative eigensubspaces. Intuition one should think of a hyper dimensional ellipsoid that sits along the standard basis vector, and transformation of $A$ is stretching and rotating's axis, resulting in a new ellipsoid in a different orientations; if such a transformation is also applied to the axis of ellipsoid then the transformed axis are conjugate vectors. Tracing along the direction of these vectors will ensure minimum redundancy of search directions. 
            \end{remark}
        \subsubsection{Properties of CDM}
            Here we setup several useful lemma and propositions that can derive the short recurrences of A-Orthogonal vectors 
            \begin{prop}
                \begin{align}
                    p_{k + j}^Tr_k &= p_{k + j}^Tr_0 \quad \forall \; 0 \le j \le n - k
                    \\
                    p_{k + j}^Tr_k &= p_k^T(I - A\overline{P}_k)r_0
                    \\
                    &= (p^T_{k + j} - p^T_{k + j}A\overline{P}_k)r_0
                    \\
                    &= p_{k + j}^Tr_0    
                \end{align}
                This is true because the vector $p_{k + j}$, a conjugate vector in the future is orthogonal to all previous conjugte vectors. 
            \end{prop}
            As a consequence of the above proposition, we obtained a short recurrence for the residuals and solution $x$ of CDM: 
            \begin{prop}[CDM Recurrence]\label{prop:CDM_Recurrence}
                \begin{align}
                    r_k - r_{k - 1} &= r_0 - A\overline{P}_kr_0 - (r_0 - A\overline{P}_{k - 1}r_0)
                    \\
                    &= A\overline{P}_kr_0 - A\overline{P}_{k - 1}r_0
                    \\
                    &= - Ap_{k - 1}\frac{\langle p_{k - 1}, r_0\rangle}{\langle p_{k - 1}, Ap_{k - 1}\rangle}
                    \\
                    \implies 
                    x_{k} - x_{k - 1} &= 
                    p_{k - 1}\frac{\langle p_{k - 1}, r_0\rangle}{\langle p_{k - 1}, Ap_{k - 1}\rangle}
                    \\
                    \text{def: } a_{k - 1} &:= \frac{\langle p_{k - 1}, r_0\rangle}{
                        \langle p_{k - 1}, Ap_{k - 1}\rangle
                    } = 
                    \frac{\langle p_{k - 1}, r_{k - 1}\rangle}{
                        \langle p_{k - 1}, Ap_{k - 1}\rangle
                    }
            \end{align}
            \end{prop}
            
            We define the value of $a_{k - 1}$, and in above, we have 2 equivalent representation. Please take note that, this proposition remains true for the future CG algorithm that we are going to develop. 
        \subsubsection{Conjugate Gradient}
            Now, consider the case where, the set of basis vector: $\{u\}_{i = 0}^{n - 1}$ to be the residual vector generated from the CDM itself. Then there are a series of new added lemmas that are true. However, this is where things started to get exciting, because a short recurrence for $p_k$ during each iteration arised and residuals are all orthgonal. We wish to proceed to prove that part. 
            \begin{lemma}\label{lemma:CG_Lemma_1}
                \begin{align}
                    \langle p_{k + j}, Ap_k\rangle
                    &=\langle r_k, Ap_{k + j}\rangle
                    = \langle p_{k + j}, Ar_k\rangle \quad \forall\; 0 \le j \le n - k
                \end{align}
            \end{lemma}
            \begin{proof}
                \begin{align}
                    p_{k + j} Ap_k &= p_{k + j}^TAr_k - p_{k + j}^TA\overline{P}_{k}Ar_k \quad 
                    \forall\; 0 \le j \le n - k
                    \\
                    &= p_{k + j}^TAr_k
                    \\
                    \langle p_{k + j}, Ap_k\rangle
                    &= \langle r_k, Ap_{k + j}\rangle
                    = \langle p_{k + j}, Ar_k\rangle
                \end{align}
            \end{proof}
            \begin{lemma}\label{lemma:CG_Lemma_2}
                \begin{align}
                    \langle r_k, p_k\rangle &= \langle r_k, r_k\rangle
                \end{align}
            \end{lemma}
            \begin{proof}
                \begin{align}
                    \langle r_k, p_k\rangle &= 
                    \langle r_k, p_k\rangle
                    \\
                    &= \langle r_k, r_k\rangle - \langle r_k, \overline{P}_kAr_k\rangle
                    \\
                    &= \langle r_k, r_k\rangle
                \end{align}
                From the first line to the second line, we make use of the definition proposed. 
            \end{proof}
            \begin{prop}[CG Generates Orthogonal Residuals]\label{prop:CG_Generates_Orthogonal_Residuals}
                \begin{align}
                    \langle r_k , r_j \rangle = 0 \quad \forall\; 0 \le j \le k - 1 
                \end{align}
            \end{prop}
            \noindent
            Let this above claim be inductively true then consider the following proof: 
            \begin{proof}
                \begin{align}
                    r_{k + 1} &= r_k - a_kAp_k
                    \\
                    \implies 
                    \langle r_{k + 1}, r_k\rangle &= \langle r_k, r_k\rangle - 
                    a_k \langle r_k, Ap_k\rangle
                    \\
                    &= \langle r_k, r_k\rangle - 
                    \frac{\langle r_k, r_k\rangle}{\langle p_k, Ap_k\rangle}
                    \langle r_k, Ap_k\rangle
                    \\
                    &= 
                    0
                \end{align}
                The first line is from the reccurrence of CDM residuals, and then next we make use of the updated definition for $a_k$. Next we consider: 
                \begin{align}
                    p_j &= (I - \overline{P}_jA)r_j \quad \forall\; 0\le j \le k - 1
                    \\
                    \hspace{1.1em} & \implies r_j = p_j + \overline{P}_jAr_j
                    \\
                    r_k &= (I - A\overline{P}_k)r_0
                    \\
                    r_k\perp \text{ran}(P_k) & \implies 
                    \langle r_k, r_j\rangle =\langle r_k, p_j + \overline{P}_jAr_j\rangle = 0
                \end{align}
                The second line is a result of the first line. Here we again make use of the projector $I - A \overline{P}_k$. The base case of the argument is simple, because $p_0 = r_0$, and by the property of the projector, $\langle r_1, r_0\rangle = 0$. The theorem is now proven. 
            \end{proof}
            \begin{prop}[CG Recurrences]\label{prop:CG_Recurrences}
                \begin{align}
                    p_k &= r_k + b_{k - 1}p_{k - 1} \quad b_{k - 1} = \frac{\Vert r_k\Vert_2^2}
                    {\Vert r_{k - 1}\Vert_2^2}
                \end{align}
            \end{prop}
            \begin{proof}
                The proof is direct and we start with the definition of CDM, which is given as: 
                \begin{align}
                    p_k &= (I - \overline{P}_kA)r_k
                    \\
                    r_k - \overline{P}_kAr_k &= 
                    r_k - P_kD^{-1}_kP^T_kAr_k
                    \\
                    &= r_k - P_kD^{-1}_k(AP_k)^Tr_k
                \end{align}
                Observe:
                \begin{align}
                    (AP_k)^Tr_k &= 
                    \begin{bmatrix}
                        \langle p_0, Ar_k\rangle
                        \\
                        \langle p_1, Ar_k\rangle
                        \\
                        \vdots
                        \\
                        \langle p_{k - 1}, Ar_k\rangle
                    \end{bmatrix}
                \end{align}
                Next, we can make use of \hyperref[lemma:CG_Lemma_1]{lemma 3.3.1} to get rid of $Ar_k$: 
                \begin{align}
                    \langle p_j, Ar_k\rangle& \quad \forall\; 0 \le j \le k -2 
                    \\
                    \langle p_j, Ar_k\rangle&= \langle r_k, Ap_j\rangle
                    \\
                    &= \langle r_k, a_j^{-1}(r_j - r_{j + 1})\rangle
                    \\
                    &= a_j^{-1}\langle r_k, (r_j - r_{j + 1})\rangle = 0
                \end{align}
                The second line is also using the property that the matrix $A$ is symmetric, the third line is using the recurrence of the residual established for CDM (\hyperref[prop:CDM_Recurrence]{CDM Recurrences (Proposition 3.6)}), and the last line is true for all $0 \le j \le k - 2$ by the orhogonality of the residual proved in \hyperref[prop:CG_Generates_Orthogonal_Residuals]{CG Generates Orthogonal Residuals (Proposition 3.8)}. Therefore we have: 
                
                \begin{align}
                    (AP_k)^Tr_k &= 
                    \begin{bmatrix}
                        \langle p_0, Ar_k\rangle
                        \\
                        \langle p_1, Ar_k\rangle
                        \\
                        \vdots
                        \\
                        \langle p_{k - 1}, Ar_k\rangle
                    \end{bmatrix}
                    = 
                    a_{k - 1}^{-1}\langle r_k, (r_{k - 1} - r_{k})\rangle \xi_k
                \end{align}
                Take note that the vector $\xi_k$ is the k th standard basis vector in $\mathbb{R}^k$, keep in mind that $r_k\perp r_{k - 1}$ as well. Using these facts we can simplify the expression for $p_k$ into: 
                \begin{align}
                    p_k &= r_k - P_kD^{-1}_k(AP_k)^Tr_k
                    \\
                    &= r_k - P_kD_k^{-1}a_{k - 1}^{-1}(\langle r_k, (r_{k - 1} - r_{k})\rangle) \xi_k
                    \\
                    &= 
                    r_k - \frac{a_{k -1}^{-1}\langle -r_k, r_k\rangle}
                    {\langle p_{k - 1}, Ap_{k - 1}\rangle}p_k
                    \\
                    &= r_k + \frac{a_{k -1}^{-1}\langle r_k, r_k\rangle}
                    {\langle p_{k - 1}, Ap_{k - 1}\rangle}p_k
                    \\
                    &= r_k + 
                    \left(
                        \frac{\langle r_{k - 1}, r_{k - 1}\rangle}{\langle p_{k - 1}, Ap_{k - 1}\rangle}
                    \right)^{-1}
                    \frac{\langle r_k, r_k\rangle}{\langle p_{k - 1}, Ap_{k - 1}\rangle}p_k
                    \\
                    &= 
                    r_k + \frac{\langle r_k, r_k\rangle}{\langle r_{k - 1}, r_{k - 1}\rangle}p_k
                \end{align}
                We make use of the definition for $a_{k-1}$ for the CDM algorithm. At this point, we have proven the short CG recurrences for $p_k$. 
            \end{proof}
            Up until this point we have proven the usual of conjugate gradient proposed by Hestenes \& Stiefe (\textbf{CITATION NEEDED}), we started with the minimizations objective and the properties of $P_k$, then we define a recurrences for the residual (Simultaneously the solution $x_k$), and the A-Orthogonal vectors using a basis as assistance for the generations process. Next, we make the key changes of the assistance basis, making it equal to the set of residuals vector generated from the algorithm itself; after some proofs, we uncovered the exact same parameters found in most of the definitions of the CG algorithm, which we refers to as Residual Assisted Conjugate Gradient. Here we proposed the CG: 
            \begin{definition}[CG]\label{def:CG}
                \begin{align}
                    & p^{(0)} = b - Ax^{(0)} 
                    \\&
                    \text{For } i = 0,1, \cdots
                    \\&\hspace{1.1em}
                    \begin{aligned}
                        & a_{i} = \frac{\Vert r^{(i)}\Vert^2}{\Vert p^{(i)}\Vert^2_A}
                        \\
                        & x^{(i + 1)} = x^{(i)} + a_i p^{(i)}
                        \\
                        & r^{(i + 1)} = r^{(i)} - a_iAp^{(i)}
                        \\
                        & b_{i} = \frac{\Vert r^{(j + 1)}\Vert_2^2}{\Vert r^{(i)}\Vert_2^2}
                        \\
                        & p^{(i + 1)} = r^{(i + 1)} + b_{i}p^{(i)}
                    \end{aligned}
                \end{align}
            \end{definition}
            That is the algorithm, stated with all the iteration number listed as a super script inside of a parenthesis. Which is equivalent to what we have proven for the Residual Assisted Conjugate Gradient. 
        \subsubsection{CG and Krylov Subspace}\label{sec:CG_and_Krylov_Subspace}
            The conjugate Gradient Algorithm is actually a residual assisted conjugate gradient, a special case of the algorithm we derived at the start of the excerp. The full algorithm can be seem by the short recurrence for the residual and the conjugation vector. This part is trivial. Next, we want to show the relations to the Krylov Subspace, which only occurs for the Residual Assisted Conjugate Gradient algorithm. 
            \begin{prop}
                \begin{align}
                    p_k \in \mathcal K_{k + 1}(A|r_0)
                    \\
                    r_k \in \mathcal K_{k + 1}(A|r_0)
                \end{align}    
            \end{prop}
            \begin{proof}
                The base case is tivial and it's directly true from the definition of Residual Assisted Conjugate Gradient: $r_0 \in \mathcal K_1(A|r_0), p_0 = r_0 \in \mathcal K_1(A|r_0)$. Next, we inductively assume that $r_k \in \mathcal K_{k + 1}(A|r_0), p_k \in \mathcal K_{k + 1}(A|r_0)$, then we consider: 
                \begin{align}
                    r_{k + 1} &= r_k - a_kAp_k
                    \\
                    &\in r_k + A\mathcal K_{k + 1}(A|r_0)
                    \\
                    &\in r_k + \mathcal K_{k + 2}(A|r_0)
                    \\
                    r_k 
                    &\in 
                    \mathcal K_{k + 1}(A|r_0) \subseteq \mathcal K_{k + 2}(A|r_0)
                    \\
                    \implies r_{k + 1}
                    &\in 
                    \mathcal K_{k + 2}(A|r_0)
                \end{align}
                At the same time the update of $p_k$ would asserts the property that: 
                \begin{align}
                    p_{k + 1} &= r_{k + 1} + b_kp_k
                    \\
                    &\in 
                    r_{k + 1} + \mathcal K_{k + 1}(A|r_0)
                    \\
                    &\in \mathcal K_{k + 2}(A|r_0)
                \end{align}
                This is true because $r_{k + 1}$ is already a member of the expanded subspace $\mathcal K_{k + 2}(A|r_0)$. And from this formulation of the algorithm, we can update the Petrov Galerkin's Conditions to be: 
                \begin{theorem}[CG and Krylov Subspace]\label{theorem:CG_and_Krylov_Subspace}
                    \begin{align}
                        \text{choose: } x_k\in x_0 + \mathcal K_{k}(A|r_0) \text{ s.t: } r_k \perp \mathcal K_{k}(A|r_0)
                    \end{align}    
                \end{theorem}
                Take note that, $\text{ran}(P_k) = \mathcal K_k(A|r_0)$ because the index starts with zero. The above formulations gives theoretical importance for the Conjugate Gradient Algorithm. 
            \end{proof}
    \subsection{Arnoldi Iterations and Lanczos}
        In this section, we introduce another important algorithm: The Lanczos Algorithm. However, to give more context for the discussion, the Arnoldi iteration is considered as well and it's used to amphasize that Lanczos Iterations is just Arnoldi but with the matrix $A$ being a symmetric matrix. Finally we make the link between Lanczos Iterations and Krylov Subspace, which wil inevitably linked back to CG and plays an important role for the analysis of CG. 
        \subsubsection{The Arnoldi Iterations}
            We first define the Arnoldi Algorithm, and then we proceed to derive it using the idea of orthgonal projector. Next, we discuss a special case of the Arnoldi Iteration: the Lanczos Algorithm, which is just Arnoldi applied to a symmetric matrix. And such algorithm will inherit the properties of the Arnoldi Iterations. 
            \par
            Before stating the algorithm, I would like to point out the interpreations of the algorithm and its relations to Krylov Subspace. Consider a matrix of Hessenberg Form:
            \begin{align}
                \tilde{H}_k = 
                \begin{bmatrix}
                    h_{1, 1} & h_{1, 2} & \cdots & h_{1, k} 
                    \\
                    h_{1, 2} & h_{2, 2} & \cdots & h_{2, k}
                    \\
                    \\
                    & \ddots & &\vdots
                    \\
                    & & h_{k, k - 1}& h_{k, k}
                    \\
                    & & & h_{k + 1, k}
                \end{bmatrix}
            \end{align}
            We initialize the orhtogonal projector with the vector $q_1$, which is $q_1q_1^H$, next, we apply the linear operator $A$ on the current range of the projector: $Aq_1$, then, we orthogonalize it against $q$. Let the projection of $Aq_1$ onto $I - q_1q_1^H$ be $h_{1, 2}q_2$, and let the projection onto $q_1q_1^H$ be $h_{1,1}$. This completes the first column of $H_k$, we do this recursively. Please allow me to demonstrate: 
            \begin{align}
                (\tilde{H}_k)_{2, 1}q_2 &= (I - q_1q_1^H)Aq_1
                \\
                (\tilde{H}_k)_{1, 1}q_1 &= q_1q_1^HAq_1
                \\
                Q_2 &:= \begin{bmatrix}
                    q_1 & q_2
                \end{bmatrix}
                \\
                (\tilde{H}_k)_{3, 2}q_3 &= (I - Q_2Q_2^H)Aq_2
                \\
                (\tilde{H}_k)_{1:2, 2} &= Q_2Q_2^HAq_2
                \\
                Q_3 &:= \begin{bmatrix}
                    q_1 & q_2 & q_3
                \end{bmatrix}
                \\
                & \vdots 
                \\
                Q_j &:= \begin{bmatrix}
                    q_1 & q_2 & \cdots & q_j
                \end{bmatrix}
                \\
                (\tilde{H}_k)_{j + 1, j}q_{j + 1}&= (I - Q_jQ_j^H)Aq_j
                \\
                (\tilde{H}_k)_{1:j, j} &= Q_jQ_j^HAq_j
                \\
                &\vdots 
                \\
                Q_k &:= \begin{bmatrix}
                    q_1 & q_2 & \cdots & q_k
                \end{bmatrix}
                \\
                (\tilde{H}_k)_{k + 1, k}q_{k + 1}&= (I - Q_kQ_k^H)Aq_k
                \\
                (\tilde{H}_k)_{1:k, k} &= Q_kQ_k^HAq_k
            \end{align}
            Reader please observe that $Q_k$ is going to be orthogonal because how at the start, $q_1q_1^H$ and $I - q_1q_1^H$ is giving us an orthogonal subspace. As a consequence, we can express the recurrences of the subspace vector in matrix form: 
            \begin{align}
                AQ_{k} &= Q_{k + 1}\tilde{H}_k
                \\
                Q_{k}^HAQ_{k} &= H_k
            \end{align}
            And here, we explicitly define $H_k$ to be the principal submatrix of $\tilde{H}_k$. Reader please immediately observe that, if $A$ is symmetric, then it has to be the case that $Q^H_kAQ_k$ is also symmetric, which will make $H_k$ to be symmetric as well, which implies that $H_k$ will be a Symmetric Tridiagonal Matrix. And under that assumption, we can develop the Lanczos Algorithm. Instead of orthogoalizing against all previous vectors, we have the option to simply orthogonalize against the previous $q_k, q_{k - 1}$ vector. And we can reuse the sub-diagonal elements for $q_{k - 1}$; giving us the Lanczos Algorithm. 
        \subsubsection{Arnoldi Produces Orthogonal Basis for Krylov Subspace}
            One important observations reader should make about the idea of Arnoldi Iteration is that, during each iteration, the matrix $Q_k$ spans the same range as $\mathcal K_k(A|q_1)$. 
            \begin{prop}
                \begin{align}
                    \text{ran}(Q_k) &= \mathcal K_k(A|q_1)
                \end{align}
            \end{prop}
            \begin{proof}
                The base case is simple: $q_1 \in \mathcal K_1(A|q_1)$, inductively assuming the proposition is true, using the polynomial property of Krylov Subspace we consider: 
                $$
                \begin{aligned}
                    Q_k &\in \mathcal K_k(A|q_1)
                    \\
                    \iff w_k^+: \exists\; p_k(A|w_k^+)q_1 &= q_k
                    \\
                    \implies Aq_k &= \; Ap_k(A|w_k^+)q_1 \in \mathcal K_{k + 1}(A|w_k^+)
                    \\
                    q_{k + 1} &\in \mathcal K_{k + 1}(A|q_{1})
                    \\
                    \implies \text{ran}(Q_{k + 1}) &= \mathcal K_{k + 1}(A|q_1)
                \end{aligned}
                $$
                The Arnoldi Algorithm terminates if the value $h_{k + 1, k}$ is set to be zero. This is the case because the normalization process is dividing by $h_{k + 1, k}$ to get $q_{k + 1}$. This only happens when $Aq_{k}\in \text{ran}(Q_k)$; because $h_{k +1, k}$ is given by the projector of $I - Q_kQ^H$ applied to $Aq_k$ and the null space of this projector is $\text{ran}(Q_k)$, resulging in $h_{k + 1, k} = 0$. 
            \end{proof}
            \begin{remark}[Arnoldi Produces Minimal Monic in Krylov Subspace]\label{remark:Arnoldi_Produces_Minimal_Monic_in_Krylov_Subspace}
                The characteristic polynomial of $H$, minimizes $\Vert p(A|w)q_1\Vert_2$. For more information, Trefenthen has a coverage on the topic in his works(\textbf{CITATION NEEDED}). The minimization property in Arnoldi translate to Lanczos Iterations as well. 
            \end{remark}
        \subsubsection{The Lanczos Iterations}
            \begin{definition}[Lanczos Iterations]
                \begin{align}
                    & \text{Given arbitrary: } q_1 \text{ s.t: } \Vert q_1\Vert = 1
                    \\
                    & \text{set: }\beta_0 = 0
                    \\
                    & \text{For } j = 1, 2, \cdots 
                    \\
                    &\hspace{1.1em}\begin{aligned}
                        & \tilde{q}_{j + 1} := Aq_j - \beta_{j - 1}q_{j - 1}
                        \\
                        & \alpha_j := \langle q_j,\tilde{q}_{j + 1}\rangle
                        \\
                        & \tilde{q}_{j + 1} \leftarrow \tilde{q}_{j + 1} - \alpha_j q_j
                        \\
                        & \beta_j = \Vert \tilde{q}_{j + 1}\Vert
                        \\
                        & q_{j + 1} := \tilde{q}_{j + 1}/\beta_j
                    \end{aligned}
                \end{align}
                Here, let it be the case that $H_k$ is a Symmetric Tridiagonal Matrix with $\alpha_i$ on the diagonal, $\beta_i$ on the sub and super diagonal; the lanczos is Arnoldi, but we make use of the symmetric properties to orthogonalize $Aq_j$ against $q_{j - 1}$ using $\beta_{j-1}$, and in this case, each iteration only consists of one vector inner product. 
                Note that another equivalent algorithm where I tweaked it to handle the base case of $T_k$ being a $1\times 1 $ matrix can be phrased in the following way: 
                \begin{align}
                    \begin{aligned}
                        & \text{Given arbitrary: }q_1 \text{ s.t: } \Vert q_1\Vert = 1
                        \\
                        & \alpha_1 := \langle q_1, Aq_1\rangle
                        \\
                        & \beta_0 := 0
                        \\
                        & \text{Memorize}: Aq_1
                        \\
                        & \text{For }j = 1, 2, \cdots
                        \\
                        &\hspace{1.1em}
                        \begin{aligned}
                            & \tilde{q}_{j + 1} := Aq_j - \beta_{j - 1} q_{j - 1}
                            \\
                            & \tilde{q}_{j + 1} \leftarrow \tilde{q}_{j + 1} - \alpha_jq_j
                            \\
                            & \beta_j = \Vert \tilde{q}_{j + 1}\Vert
                            \\
                            & q_{j + 1}:= \tilde{q}_{j + 1}/\beta_j
                            \\
                            & \alpha_{j + 1} := \left\langle q_{j + 1}, Aq_{j + 1} \right\rangle
                            \\
                            & \text{Memorize: }Aq_{j + 1}
                        \end{aligned}
                    \end{aligned}
                \end{align}
            \end{definition}
            The algorithm generates the following 2 matrices, $Q_k$ which is orthogonal and it spans $\mathcal K_k(A|q_1)$, and a Symmetric Tridiagonal Matrix: 
            \begin{align}
                Q_k &= \begin{bmatrix}
                    q_1 & q_2 & \cdots & q_k
                \end{bmatrix}
                \\
                T_k &= 
                \begin{bmatrix}
                    \alpha_1 & \beta_1 & & 
                    \\[0.8em]
                    \beta_1 & \ddots & \ddots & 
                    \\[0.8em]
                    &\ddots &\ddots & \beta_{k - 1}
                    \\[0.8em]
                    & & \beta_{k - 1} & \alpha_k
                \end{bmatrix}
            \end{align}
            Similar to the recurrence from the Arnoldi Algorithm, the lanczos also create a recurrence between $Aq_k$ and $Q_k$ and $q_{k + 1}$, but the recurrence is shorter as it simply make use of the previous 2 vectors: 
            \begin{theorem}[Lanczos Recurrenes]
                \begin{align}
                    AQ_k &= Q_kT_k + \beta_k q_{k + 1}\xi_k^T = Q_{k + 1}\tilde{T}_k
                    \\
                    \implies Aq_k
                    &= \beta_{j - 1}q_{j - 1} + \alpha_j q_j + \beta_{j}q_{j + 1} \quad \forall\; 2\le j\le k
                    \\
                    \implies Aq_1 &= \alpha_1q_1 + \beta_1 q_2
                \end{align}    
            \end{theorem}
            \par
            Often time, we refers the $k\times k$ symmetric tridiagonal matrix generated from Iterative Lanczos as $T_k$. Finally; I wish to make the following important remark about the algorithm for later use. Given a matrix $A$ and an initial vector $q_1$, The lanczos algorithm produces an irreducible Symmetric Tridiagonal Matrix that has unique eigenvalues. The proof for the fact that any Symmetric Tridiaogonal Matrices with Non-zeros on the sub/super diagonal must have unique non-zero eigenvalues is skipped. What we can immediate show here is the fact that Lanczos Algorithm will produce such a matrix. 
            \begin{prop}[Lanczos Termination Conditions]\label{prop:Lanczos_Termination_Conditions}
                The Lanczos Iteration produces a Symmetric Tridiagonal Matrix that has no zero element on its super and sub-diagonal, and if $\beta_k$ is zero, then the algorithm must terminate, and $k$ would equal to $\text{grade}(A|q_1)$, the grade of the Krylov Subspace. 
            \end{prop}
            \begin{proof}
                It's true because the $\beta_{k}$ in the Lanczos is equivalent to $h_{k + 1, k}$. It's been discussed previously that if $h_{k + 1, 1} = 0$ for the Arnoldi's Iteration, then the Krylov Subspace $\mathcal K_k(A|q_1)$ became an invariant subpace under $A$, and in that sense, the algorithm has to terminate due to a divides by zero error. 
            \end{proof}
            \begin{remark}[Minimal Polynomial from Lanczos Iterations]\label{remark:Minimal_Polynomial_from_Lanczos_Iterations}
                The characteristic polynomial of $T_k$ has a special minimization property. Here recall \hyperref[remark:Arnoldi_Produces_Minimal_Monic_in_Krylov_Subspace]{remark 3.4.1}, we make use of the minimization property of the characterstic polynomial of the Hessenberg matrix from the Arnoldi Iterations. Under Lanczos iterations the matrix $H_k$ becomes the tridiagonal $T_k$. Since matrix $A$ is symmetric, we consider its eigen decomposition in the form: $A = V\Lambda V^T$, we let $\bar{p}_k(x)$ denote the characteristic polynomial of matrix $T_k$, then using the 2 norm minimization properties we have: 
                \begin{align}
                    & \min_{p_k:\text{monic}} \Vert p(A)q_1\Vert_2
                    \\
                    & =\Vert \overline{p}_k(A)q_1\Vert_2
                    \\
                    & = \Vert V \bar{p}_k(\Lambda)V^Tq_1\Vert_2
                    \\
                    & = \Vert \bar{p}_k(\Lambda)V^Tq_1\Vert_2
                    \\
                    &= \min_{i = 1, \cdots, n} \Vert \bar{p}_n(\lambda_i)(V^Tq_1)_i\Vert_2
                \end{align}
                The last line is saying the conditions that the characterstic polynomial for $T_k$ from the Lanczos iterations is minimizing the weighted sum at the eigenvalues of the matrix $A$. A direct consequence of this is a more refined terminations conditions than \hyperref[prop:Lanczos_Termination_Conditions]{proposition 3.12} for the Lanczos iterations, giving the information for initial vector $q_1$. Under exact arithematic, the number of iterations underwent by Lanczos equals to the unique number of eigenvalues $\lambda_i$ where $(V^Tq_1)_i \neq 0$. 
                
            \end{remark}
            
\section{Analysis of Conjugate Gradient and Lanczos Iterations}
    In this section we state the terminations conditions for the Lanczos iterations and the CG algorithm we developed using the property of Krylov Subspace. This is just a throughly discussion about these 2 algorithm and applying the foundations and it's not following any references. 
    \subsection{Conjugate Gradient and Matrix Polynomial}
        One important result of the optimization objective listed \hyperref[theorem:CG_and_Krylov_Subspace]{CG and Krylov Subspace} is the connections to matrix polynomial of $A$ and Conjugate Gradient. More specifically we consider the following proposition: 
        \begin{prop}[CG Relative Energy Error]\label{prop:CG_Relative_Energy Error}
            \begin{align}
                & x_k \in \mathcal{K}(A|r_0)w + x_0
                \\
                & \frac{\Vert e_k\Vert_A^2}{\Vert e_0\Vert_A^2}
                = 
                \min_{w\in \mathbb R^k} 
                \Vert
                    (1 + Ap_k(A|w))A^{1/2}e_0
                \Vert_2^2
                \le
                \min_{p_{k}: p_{k}(0) = 1}\max_{x\in [\lambda_{\text{min}}, \lambda_{\text{max}}
                ]} |p_k(x)|
            \end{align}
            Here we use the notation $e_k = A^{-1}b - x_k$ to denotes the error vector. 
        \end{prop}
        \begin{proof}
            \begin{align}
                \Vert e_k\Vert_A^2 & =
                \min_{x_k \in x_0 + \mathcal K_k(A|r_0)}
                \Vert 
                    x^+ - x_k
                \Vert_A^2
                \\
                x_k \in x_0 + \mathcal K_k(A|r_0) 
                & \implies
                e_k = e_0 + p_{k - 1}(A|w)r_0
                \\
                \implies  &=
                \min_{w\in \mathbb R^k}
                \Vert 
                    e_0 + p_{k - 1}(A|w)r_0
                \Vert_A^2
                \\
                &= \min_{w\in \mathbb R^k}
                \Vert 
                    e_0 + Ap_{k - 1}(A|w)e_0
                \Vert_A^2
                \\
                &= \min_{w\in \mathbb R^k}
                \Vert 
                    A^{1/2}(I + Ap_{k - 1}(A|w))e_0
                \Vert_2^2
                \\
                &\le
                \min_{w\in \mathbb R^k}
                \Vert 
                    I + Ap_{k - 1}(A|w)
                \Vert_2^2\Vert e_0\Vert_A^2 \quad 
                \\
                & = 
                \min_{w\in \mathbb R^k}
                \left(
                    \max_{i = 1, \dots, n}
                    |1 + \lambda_i p_{k - 1}(\lambda_i|w)|^2
                \right)\Vert e_0\Vert_A^2
                \quad
                \\
                & \le 
                \min_{w\in \mathbb R^k}
                \left(
                    \max_{x\in [\lambda_{\min}, \lambda_{\max}]}
                    |1 + \lambda_i p_{k - 1}(\lambda_i|w)|^2
                \right)\Vert e_0\Vert_A^2
                \quad 
                \\
                &= 
                \min_{p_{k}: p_{k}(0) = 1}
                \max_{x\in [\lambda_{\min}, \lambda_{\max}]}
                | p_{k}(x)|^2 \Vert e\Vert_A^2
                \\
                \implies
                \frac{\Vert e_k\Vert_A}{\Vert e_0\Vert_A} &\le 
                \min_{p_{k}: p_{k}(0) = 1}\max_{x\in [\lambda_{\text{min}}, \lambda_{\text{max}}]} |p_{k}(x)|
            \end{align}
            (4.1.3) is the Error Energy norm minimization objective of CG, we proceed with writing up the affine subspace where $x_k$ is from: $x_0 + \mathcal K_k(A|r_0)$ at (4.1.4), putting Krylov subspace in terms of a matrix polynomial mulitplied by $r_0$ and then use $A^{-1}b$ to subtract both side to get the expression for $e_k$. From the (4.15) line to the (4.16), we use the fact that $r_0 = Ae_0$, allowing us to extract out a factor $A$. 
            \par
            Next, from (4.1.6) to (5.1.7), we use the fact that every symmetric definite matrix $A$ has the factorization of $A^{1/2}A^{1/2}$ where $A^{1/2}$ is also a symetric definite matrix. After that we moved the $A^{1/2}$ to $e_0$ to get $\Vert e_0\Vert_A^2$ from (4.1.7) to (4.1.8), the matrix polynomial part is left with the 2-norm. From (4.1.8) to (4.1.9) we use the eigendecompoition of $A$ which is diagonalizable with a unitary transform, giving us the form of $Q\Lambda Q^T = A$ where $Q$ is an Unitary Matrix and diagonals of $\Lambda$ are the eigenvalues of $A$. Allow me to explain: 
            \begin{align}
                \Vert I + Ap_{k - 1}(A|w)\Vert_2^2
                &= \Vert Q(I + \Lambda p_{k - 1}(\Lambda|w))Q^T\Vert_2^2
                \\
                &= \Vert I + \Lambda p_{k - 1}(\Lambda|w)\Vert_2^2
                \\
                &= \max_{i=1,\cdots, n}|1 + \lambda_ip_{k - 1}(\lambda_i|w)|^2
            \end{align}
            Where, the 2-norm of a diagonal matrix $\Lambda$ is just its biggest diagonal element. And then we relax the conditions for $\lambda_i$ by reducing it to be some element in the interval between theminimum and the maximum of the eigenvalues for the matrix $A$ (from (4.1.9) to (4.1.10)). Finally, please notice thta we use an monic $p_{k+1}(x)$ at the and to simplify things. 
        \end{proof}
        The above results will be useful for proving the convergenc of CG. 
        \begin{remark}[The CG Relative Energy Error Norm is Tight]
            The bound is tight refers to the fact that the matrix is SPD, therefore $\Vert Ax\Vert \le \Vert A\Vert\Vert x\Vert$ is tight, which justifies that (4.1.10) is tight in the sense that for any iteration $k$, we can choose an initial vector $e_0$ such that the equality is achieved. (4.1.12) can still be tight if we have the freedom to choose the eigenvalues of the matrix $A$. However, the bound is rarely tight if the initial error vector $e_0$ and the matrix $A$ is fixed. 
        \end{remark}
    \subsection{Termination Conditions of CG}
        \begin{prop}[Termination of CG]\label{prop:Termination_of_CG}
            For all initial guess, the maximum iterations underwent by the CG algorithm is the number of unique eigenvalues for the matrix $A$. 
        \end{prop}
        This result is direct from (4.1.8), the CG algorithm terminates when a polynomial that interpolates all the unique eigenvalue is found. This bound is true for all initial guess, and sometimes for some given $e_0$, the terminations can come with fewer iterations. 

    \subsection{Convergence Rate of CG under Exact Arithematic}
        In this section we make heavy use of Greenbaum's Analysis for convergence rate of the algorithm. The core is idea is to use a Chebyshev Polynomial to establish a bound and it's applicable when the linear operator has extremely high dimension and we limit the number of iterations to $k$ where $k$ is much smaller than $n$, the size of the matrix. We will follow Greenbaum's Analysis but with some more details. 
        \subsubsection{Uniformly Distributed Eigenvalues}
            \begin{theorem}[CG Convergence Rate]\label{theorem:CG_Convergence_Rate}
                The relative error squared measured over the energy norm is bounded by: 
                \begin{align}
                    \frac{\Vert e_k\Vert_A}{\Vert e_k\Vert_A}
                    \le 2 \left(
                            \frac{\sqrt{\kappa} + 1}{\sqrt{\kappa} - 1}
                    \right)^k
                \end{align}
                Where $k$ is the number of iterations, and $e_k = A^{-1}b - x_k$, the upper bound is the most general and it's able to bound the convergence given $\lambda_{\min}, \lambda_{\max}$ of the operator $A$. The bound is loose if there are some kind of clustering of the eigenvalue of matrix $A$, and the bound would be tighter given that $k<<n$ and the eigenvalues of $A$ are evenly spread out on the spectrum. 
            \end{theorem}
            Before the proof, I need to point out the analysis draws inspiration from the interpolating mononic polynomial for the spectrum of the matrix $A$, and we make use of the Inf Norm minimization property of the Chebyshev Polynomial. Here, we order all the eigenvalues of matrix $A$ so that $\lambda_1, \lambda_n$ denotes the maximum and the minimum eigenvalues for $A$. 
            \begin{proof}
                We start by adapting the Chebyshev Polynomial to the convex hull of the spectrum for matrix $A$, while also making it monic: 
                \begin{align}
                    T_k(x) &= \arg\min_{
                        \substack{
                            p\in \mathcal{P}_{k}
                            }
                        }\max_{x\in [-1, 1]}|p(x)|
                    \\
                    p_k(x) &:= 
                    \frac{T_k(\varphi(x))}{T_k(\varphi(0))}
                    \quad 
                    \text{where: } 
                    \varphi(x) := \frac{2x - \lambda_1 - \lambda_n}{\lambda_n - \lambda_1}
                    \\
                    \implies p_k(x) &= \arg \min_{
                        \substack{
                            p\in \mathcal{P}_{k}
                            \\
                            \text{s.t: } p(0)  = 1
                        }
                    }\max_{x\in [\lambda_1, \lambda_n]}|p(x)|
                \end{align}
                At this point, we have defined a new polynomial $p_k$ that minimizes the inf norm over the convex hull of the eigenvalues and it's monic. Note that here we use $T_k$ for the type T Chebyshev Polynomial of degree k and it's not the tridiagonal symmetric matrix from Lanczos iterations. Next, we use the property that the range of the Chebyshev is bounded within the interval $[-1, 1]$ to obtain inequality: 
                \begin{align}
                    \forall x \in [\lambda_1, \lambda_n]: \left|
                    \frac{T_k(\varphi(x))}{T_k(\varphi(0))}
                    \right|
                    \le 
                    \left|
                        \frac{1}{T_k(\varphi(0))}
                    \right|
                \end{align}
                Next, our objective is to find any upper bound for the quantities on the RHS in relations to the Condition number for matrix $A$ and the degree of the Chebyshev polynomial. Firstly observe that $\varphi(0) < -1, \varphi(0) \not\in [\lambda_1, \lambda_n]$, because all Eigenvalues are larger than zero, therefore it's out of the range of the Chebyshev polynomial and we need to find the actual value of it by considering alterantive form of Chebyshev T for values outside of the $[-1,1]$: 
                \begin{align}
                    T_k(x) &= \cosh(k\text{ arccosh}(z)) \quad \forall z \ge 1
                    \\
                    \implies
                    T_k(\cosh(\zeta)) &= \cosh(k\zeta) \quad z := \cosh(\zeta)
                \end{align}
                We need to match the form of the expression $T_k(\varphi(0))$ with the expression of the form $T_k(\cosh(\zeta))$ given the freedom of varying $\zeta$. To do that we consider a substitution of $\zeta = \ln(y)$, so that we only need to match $\varphi(0)$ with the form $(y + y^{-1})/2$, which is just a quadratic equation. 
                \begin{align}
                    \varphi(0) &= \cosh(\zeta) = \cosh(\ln(y)) \quad \ln(y) := \zeta
                    \\
                    \text{recall: } \cosh(x) &= (\exp(-x) + \exp(x))/2
                    \\
                    \implies 
                    \cosh(\ln(y)) &= (y + y^{-1})/2
                    \\
                    \varphi(0) &= (y + y^{-1})/2
                \end{align}
                Recall the definition of $\varphi(x)$ and then simplifies: 
                $$
                \begin{aligned}
                    \varphi(0) &= \frac{-\lambda_n - \lambda_1}{\lambda_n - \lambda_1}
                    \\
                    &= \frac{-\lambda_n/\lambda_1 - 1}{\lambda_n/\lambda_1 - 1}
                    \\ 
                    &= - 
                    \frac{\lambda_n/\lambda_1 + 1}{\lambda_n/\lambda_1 - 1} 
                    \\
                    \implies \varphi(0) &=
                    -\frac{\kappa + 1}{\kappa - 1}
                \end{aligned}
                $$
                Our objective is now simple. We know what $\varphi(0)$ is, we want it to form match with $\cosh(\ln(y))$, and hence we simply solve for $y$: 
                \begin{align}
                    -\frac{\kappa + 1}{\kappa - 1} &= 
                    \frac{1}{2}(y + y^{-1})
                    \\
                    y = \frac{\sqrt{\kappa}\pm 1}{\sqrt{\kappa}\mp 1}
                \end{align}
                It's a quadratic and we solved it. The above $\pm, \mp$ are correlated, meaning that they are of opposite sign, which gives us 2 roots for the quadratic expression. Now, given the hyperbolic form for $\varphi(0)$, we can substitute and get the value of $T_k(\varphi(0))$ in terms of $y$ and then $\kappa$: 
                \begin{align}
                    \varphi(0)&= \frac{1}{2}(y + y^{-1})
                    \\
                    \implies 
                    T_k(\varphi(0)) &= 
                    T_k(\cosh(\ln(y)))
                    \\
                    &= \cosh(k\ln(y))
                    \\
                    &= (y^k + y^{-k})/2
                \end{align}
                Then, substituting the value of $y$, and invert the quantity we have: 
                \begin{align}
                    \frac{1}{T_k(\varphi(0))} &= 2(y^k + y^{-k})^{-1}
                    \\
                    &= 
                    2\left(
                        \left(
                            \frac{\sqrt{\kappa}\pm 1}{\sqrt{\kappa}\mp 1}
                        \right)^{k} + 
                        \left(
                            \frac{\sqrt{\kappa}\mp 1}{\sqrt{\kappa}\pm 1}
                        \right)^{-k}
                    \right)^{-1}
                    \\
                    &= 2\left(
                        \underbrace{\left(
                            \frac{\sqrt{\kappa}+ 1}{\sqrt{\kappa}- 1}
                        \right)^{k}}_{> 1} + 
                        \underbrace{\left(
                            \frac{\sqrt{\kappa}- 1}{\sqrt{\kappa}+ 1}
                        \right)^{-k}}_{ < 1}
                    \right)^{-1}
                    \\
                    & \le 2 \left(
                        \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}
                    \right)^k
                \end{align}
                Which completes the proof. Recall from the previous discussion for the squared of the relative error, we have: 
                \begin{align}
                    \frac{\Vert e_k\Vert_A}{\Vert e_0\Vert_A} &\le 
                    \min_{p_{k + 1}: p_{k + 1}(0) = 1}\max_{x\in [\lambda_{1}, \lambda_{n}]} |p_{k + 1}(x)| \le 
                    2 \left(
                        \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}
                    \right)^k
                \end{align}

            \end{proof}
        \subsubsection{One Outlier Eigenvalue}
            Using the derived theorem, we can extend it to other type of distributions of eigenvalues. Imagine one of the extreme case where some matrices that have one group of eigenvalues that are close together and one signle eigenvalue that is far away from the cluster. In that case, we can use Chebyshev differently by focusing its minimizing power across the clustered eigenvalues and use a simple polynomial to interpolate the outlier eigenvalue. Consider the following proposition: 
            \begin{prop}[Big Outlier CG Convergence Rate]
                If, there exists a $\lambda_n$ that is much later than all previous $n - 1$ eigenvalues for the matrix $A$, then a tigher convergence bound that being only paramaterized by the range of clustered eigenvalues can be obtained and it is: 
                \begin{align}
                    \frac{\Vert e^{(k)}\Vert_A}{\Vert e^{(0)}\Vert_A} \le 
                    2 \left(
                        \frac{\sqrt{\kappa_{n - 1}} - 1}{\sqrt{\kappa_{n - 1} + 1}}
                    \right)^{k - 1}\quad 
                    \kappa_{n - 1} =  \frac{\lambda_{n - 1}}{\lambda_1}
                \end{align}
                Reader please observe that, the outlier eigenvalue $\lambda_n$ plays a smaller role in determining the convergence rate of the algorithm compare to the previous bound. 
            \end{prop}
            \begin{proof}
                Here, we wish to show that a more focused use of the Chebyshev will introduce a better convergence rate for the Conjugate Gradient. We define the notation for the adapted k-th degree Chebyshev Polynomial over an closed interval: $[a, b]$ as: 
                \begin{align}
                    \hat{T}_{[a, b]}^{(k)}(x) := 
                    T_k\left(
                        \frac{2x - b - a}{b - a}
                    \right)
                \end{align} 
                Next, we consider the following polynomial: 
                \begin{align}
                    p_k(x) := 
                    \frac
                    {
                        \hat{T}_{[\lambda_1, \lambda_{n - 1}]}^{(k - 1)}
                        \left(
                            x
                        \right)
                    }{
                        T^{(k - 1)}_{[\lambda_1, \lambda_{n - 1}]}
                        \left(
                            0
                        \right)
                    }
                    \left(
                        \frac{\lambda_n - x}{x}
                    \right)
                \end{align}
                Where, we use an $k-1$ degree polynomial for the clustered eigenvalues, and then we multiply that by a linear function $(\lambda_n - z)/\lambda_n$ which is zero at right boundary $\lambda_n$ and it's less than one at the left boundary $\lambda_1$. Next, observe the following facts about the above polynomials: 
                \begin{align}
                    &
                    \frac{\lambda_n - z}{\lambda_n} \in [0, 1]
                    \quad \forall z \in [\lambda_1, \lambda_n]
                    \\
                    & 
                    |p_k(x)| \le
                    \left|
                        \frac{
                            \hat{T}_{[\lambda_1, \lambda_{n-1}]}^{(k - 1)}(x)
                        }{
                            \hat{T}_{[\lambda_1, \lambda_{n - 1}]}^{(k - 1)}(0)
                        }
                        \frac{\lambda_n - z}{\lambda_n}
                    \right|
                    \le 
                    \frac{1}{
                    \left|
                        \hat{T}_{[\lambda_1, \lambda_{n - 1}]}^{(k - 1)}(0)
                    \right|}
                \end{align}
                As a result, we can apply the convergence rate we proven for the uniform case, giving us: 
                \begin{align}
                    T^{(k - 1)}_{[\lambda_1, \lambda_{n - 1}]}
                    \left(
                        0
                    \right)
                    &= \left|
                        T_{k-1}\left(
                            \frac{
                                -\lambda_{n-1} - \lambda_1
                            }
                            {\lambda_{n-1}- \lambda_1}
                        \right)
                    \right|
                    \\ 
                    &=
                    \frac{1}{2}(y^{k - 1} + y^{-(k - 1)})
                    \\
                    \text{ where: } y &= \frac{\sqrt{\kappa_{n - 1}} + 1}{\sqrt{\kappa_{n - 1}} - 1}, \kappa_{n - 1} = \frac{\lambda_{n - 1}}{\lambda_1}
                \end{align}
                Substituting the value for $y$ we obtain the bound: 
                \begin{align}
                    \frac{\Vert e_k\Vert_A}{\Vert e_0\Vert_A} \le 
                    2\left(
                        \frac{\sqrt{\kappa_{n - 1}} - 1}{\sqrt{\kappa_{n - 1}} + 1}
                    \right)^{k - 1}
                \end{align}
            \end{proof}
            Another case that is worth considering is when there is one eigenvalue that is smaller than all the other eigenvalues which are clustered at a way larger value than it, by which I mean the value of $\lambda_1$ is much smaller than all other eigenvalues and the other eigenvalues are clustered close together in an interval uniformly. 
            \begin{prop}[Small Outlier CG Convergence Rate]
                The convergence rate is: 
                \begin{align}
                    \frac{\Vert e_k\Vert_A}{\Vert e_0\Vert_A} \le 
                    2\left(
                        \frac{\lambda_n - \lambda_1}{\lambda_1}
                    \right)
                    \left(
                        \frac{\sqrt{\kappa_0} - 1}{\sqrt{\kappa_0} + 1}
                    \right)^{k - 1}
                \end{align}
                Where $\kappa_0$ is $\lambda_n/\lambda_2$. 
            \end{prop}
            \begin{proof}
                \begin{align}
                    w(z) &:=  \frac{\lambda_1 - z}{\lambda_1} 
                    \\
                    p_k(z) &:= w(z)\left(
                        \frac{\hat{T}_{[\lambda_2, \lambda_n]}^{(k - 1)}(z)}
                        {
                            \hat{T}_{[\lambda_2, \lambda_n]}^{(k - 1)}(z)
                        }
                    \right)
                    \\\implies
                    \max_{x\in[\lambda_2, \lambda_n]} |w(x)| &=
                    \frac{\lambda_n - \lambda_1}{\lambda_1}
                \end{align}
                In this case, the maximal value of the linear function $w$ is achieved via $x = \lambda_1$, and the absolute value swapped the sign of the function. Therefore, we have: 
                \begin{align}
                    |p_k(x)| &= 
                    \left|
                        w(x) 
                        \frac{\hat{T}_{[\lambda_2, \lambda_n]}^{(k - 1)}(x)}
                        {
                            \hat{T}_{[\lambda_2, \lambda_n]}^{(k - 1)}(0)
                        }
                    \right|
                    \\
                    &\le 
                    \left|
                        \frac{w(x)}{\hat{T}_{[\lambda_2, \lambda_n]}^{(k - 1)}(0)}
                    \right|
                    \\
                    & \le
                    \left| 
                        \left(
                            \frac{\lambda_n - \lambda_1}{\lambda_1}
                        \right)
                        \hat{T}_{[\lambda_2, \lambda_n]}^{(k - 1)}(0)
                    \right|
                    \\
                    \implies 
                    & \le   
                    \left(
                        \frac{\lambda_n - \lambda_1}{\lambda_1}
                    \right)
                    2\left(
                        \frac{\sqrt{\kappa_0} + 1}{\sqrt{\kappa_0} - 1}
                    \right)^{k - 1}
                \end{align}
                We applied the Chebyshev Bound theorem proved in the previous part. And $\kappa_0 = \lambda_n/\lambda_2$, and that is the maximal bound for the absolute value of the polynomial.
                \par
                Take notice that it's not immediately clear which type of outlier eigenvalue make the convergence better or worse, but in this case, the weight $w(x)$ introduces a term that grows inversely proportional to $\lambda_1$. 
            \end{proof}
    \subsection{From Conjugate Gradient to Lanczos}
        Up until this point of disucssion, we had been brewing the fact that, the Iterative Lanczos Algorithm and the Conjugate gradient algorithm are pretty much the same thing. From the previous discussion we can observe that: 
        \begin{enumerate}
            \item [1.)] Both Lanczos and CG terminates when the grade of Krylov subspace is reached. For lanczos it's $\mathcal K_k(A|q_1)$ and for CG it's $\mathcal K_k(A|r_0)$.
            \item [2.)] Both Lanczos and CG generates orthogonal vectors, for Lanczos they are the $q_i$ vector and for CG they are the $r_i$ vectors. 
        \end{enumerate}
        These 2 properties in particular, is hinting at an equivalence between the residual vectors $r_j$ from CG and the orthogonal vectors $q_j$ from Lanczos. However, it's also not entirely obvious because CG is derived as CG in the first section, and yet it doesn't make use of any orthogonal projector. Further more, one might also notice that Iterative Lanczos are for General Symmetric Matrices whie CG are only for positive definite matrices. To see how everything ties together, we have to go both directions way to show the connections between these 2 iterative algorithms, which are what this section and the subsequent section about. Here, we refers Lanczos vectors as the sequence of $q_j$ generated by the Iterative Lanczos Algorithm. 
        \par
        For this subsection, our objective is to establish the equivalence between the parameters from the lanczos algorithm: $\alpha_k, \beta_k, q_k$ and the $a_j, b_j, r_j$ from the conjugate gradient algorithm. We establish it by going from the conjugate gradient to the Lanczos Algorithm. 
        \begin{prop}
            The residual and the lanczos vectors have the following relations: 
            \begin{align}
                q_1 &= \hat r_0\\
                q_2 &= -\hat r_1
                \\
                \vdots
                \\
                q_j &= (-1)^{j + 1}\hat r_{j + 1}
            \end{align}
            Here, $\hat{r}_j:= r_j/\Vert r_j\Vert$ and we can fill in the Lanczos Tridiagonal matrix using the CG parameters to obtain the tridiagonalization of the Positive definite matrix $A$: 
            \begin{align}
                \begin{cases}
                    \alpha_{j + 1} = \frac{1}{a_j} + \frac{b_{j - 1}}{a_{j - 1}}
                    & \forall 1 \le j \le k - 1
                    \\
                    \beta_{j} = \frac{\sqrt{b_{j - 1}}}{a_{j - 1}}
                    & \forall 2 \le j \le k - 2 
                    \\
                    \alpha_1 = a_0^{-1} & 
                    \\
                    \beta_1 = \frac{\sqrt{b_0}}{\alpha_0}
                \end{cases}
            \end{align}
            Where $\alpha_j$ for $1\le j \le n - 1$ are the diagonal of the tridiagonal matrix $T_k$ generated by Lanczos, and $\beta_j$ for $2\le j \le k - 2$ are the lower and upper subdiagonals of the matrix $T_k$. 
        \end{prop}
        We will break the proof into several parts. Firstly we address the base case, and then we address the inductive case to establish the parameters between the Tridiaogonal matrix and $a_k, b_k$, finall we resolve the sign problem between the Lanczos vectors and the residual vectors. 
        \subsubsection{The Base Case}
            Right from the start of the CG iteration we have: 
            \begin{align}
                r_0 &= p_0
                \\
                r_1 &= r_0 - a_0Ar_0
                \\
                Ar_0 &= a_0^{-1}(r_0 - r_1)
                \\
                Ar_0 &= \frac{\Vert r_0\Vert_A^2}{\Vert r_0\Vert^2}(r_0 - r_1)
            \end{align}
            Consider subtituting $r_0 = \Vert r_0\Vert q_1, r_1 = -\Vert r_1\Vert q_2$, then: 
            \begin{align}
                A\Vert r_0\Vert q_1 
                &= \frac{\Vert r_0\Vert_A^2}{\Vert r_0\Vert^2}\left(
                    \Vert r_0\Vert q_1 + \Vert r_1\Vert q_2
                \right)
                \\
                &= 
                \frac{\Vert r_0\Vert_A^2}{\Vert r_0\Vert^2}\Vert r_0\Vert q_1 + 
                    \frac{\Vert r_1\Vert}{\Vert r_0\Vert} q_2
            \end{align}
            And from this relation, using the Lanczos recurrence theoremit would imply that $\alpha_1 = a_0^{-1}$; $\beta_1 = \frac{\sqrt{b_0}}{\alpha_0}$. So far so good, we have shown that there is an equivalence between the Lancozs and the CG for the first iterations of the CG algorithm. 
        \subsubsection{The Inductive Case}
            \begin{lemma}
                Inductively we wish to show the relation that: 
                \begin{align}
                    \begin{cases}
                        \alpha_{j + 1} = \frac{1}{a_j} + \frac{b_{j - 1}}{a_{j - 1}}
                        & \forall 1 \le j \le n - 1
                        \\
                        \beta_{j} = \frac{\sqrt{b_{j - 1}}}{a_{j - 1}}
                        & \forall 2 \le j \le n - 2 
                    \end{cases}
                \end{align}
            \end{lemma}
            \begin{proof}
                We start by consiering: 
                \begin{align}
                    r_j &= r_{j - 1} - a_{j -1 }Ap_{j - 1}
                    \\
                    & =r_{j - 1} - a_{j - 1} A(r_{j - 1} + b_{j - 2}p_{j - 1})
                    \\
                    &= r_{j - 1} - a_{j - 1}Ar_{j - 1} - a_{j - 1}b_{j - 2}Ap_{j - 1}
                \end{align}
                We make use of the recurrence asserted by the CG algorithm, giving us: 
                \begin{align}
                    r_{j - 1} &= r_{j - 1} - a_{j - 2}Ap_{j - 1}
                    \\
                    r_{j - 1} - r_{j - 1} &= a_{j - 2} Ap_{j - 1}
                    \\
                    Ap_{j - 1} &= a^{-1}_{j -2} 
                    \left(
                        r_{j - 2} - r_{j - 1}
                    \right)
                \end{align}
                Here, we can subtitute the results of for the term $Ap_{j - 1}$, and then we can express the recurrence of residual purely in terms of residual. Consider: 
                \begin{align}
                    r_{j} &= r_{j - 1} - a_{j - 1}Ar_{j - 1} - a_{j - 1}b_{j - 2}Ap_{j - 2}
                    \\
                    &= 
                    r_{j - 1} - a_{j - 1}Ar_{j - 1} - \frac{a_{j-1}b_{j-2}}{a_{j-2}}\left(
                        r_{j - 2} - r_{j - 1}
                    \right)
                    \\
                    &= \left(
                        1 + \frac{a_{j - 1}b_{j -2}}{a_{j - 2}}r_{j - 1}
                    \right)- a_{j - 1}Ar_{j - 1} - \frac{a_{j-1}b_{j-2}}{a_{j-2}}r_{j - 2}
                    \\
                    a_{j - 1}Ar_{j - 1} &= 
                    \left(
                        1 + \frac{a_{j - 1}b_{j -2}}{a_{j - 2}}r_{j - 1}
                    \right)
                    - \frac{a_{j-1}b_{j-2}}{a_{j-2}}r_{j - 2}
                    \\
                    Ar_{j - 1} &=
                    \left(
                        \frac{1}{a_{j - 1}} + \frac{b_{j - 2}}{a_{j- 2}}
                    \right)r_{j - 1} + 
                    \frac{r_{j}}{a_{j-1}} - 
                    \frac{b_{j - 2}}{a_{j - 2}}r_{j - 2}
                \end{align}
                Finally, we increment the index $j$ by one for notational convenience, and therefore we establish the following relations between the residuals of the conjugate gradient algorithm: 
                \begin{align}
                    Ar_{j} &=
                    \left(
                        \frac{1}{a_{j}} + \frac{b_{j - 1}}{a_{j-  1}}
                    \right)r_{j} + 
                    \frac{r_{j + 1}}{a_{j}} - 
                    \frac{b_{j - 1}}{a_{j - 1}}r_{j - 1}
                \end{align}
                Reader please observe that, this is somewhat similar to the recurrence relations between the Lanczos vectors, however it's failing to match the sign, at the same time, it's not quiet matching the form of the recurrence of $\beta_k$ from the lanczos algorithm. To match it, we need the coeffcients of $r_{j - 1}$ and $r_{j + 1}$ to be in the same form, paramaterized by the same iterations parameter: $j$. To do that, consider the doing this:  
                \begin{align}
                    q_{j + 1} &:= \frac{r_{j}}{\Vert r_j\Vert}
                    \\
                    q_{j} &:= -\frac{r_{j - 1}}{\Vert r_{j - 1}\Vert} \quad 
                    \text{Note: This is Negative}
                    \\
                    q_{j + 2} &:= \frac{r_{j + 1}}{\Vert r_{j + 1}\Vert}
                    \\
                    \implies 
                    A\Vert r_j\Vert q_{j + 1} 
                    &= 
                    \left(
                        \frac{1}{a_j} + \frac{b_{j - 1}}{a_{j - 1}}
                    \right)\Vert r_j\Vert q_{j + 1}
                    + 
                    \frac{\Vert r_{j + 1}\Vert q_{j + 2}}{a_j}
                    +
                    \frac{b_{j - 1}\Vert r_{j - 1}\Vert}{a_{j - 1}}q_{j}
                    \\
                    Aq_{j + 1} &= 
                    \left(
                        \frac{1}{a_j} + \frac{b_{j - 1}}{a_{j - 1}} 
                    \right)
                    q_{j + 1}
                    + 
                    \frac{\Vert r_{j + 1}\Vert}{a_j \Vert r_j\Vert}q_{j + 2} + 
                    \frac{b_{j - 1}\Vert r_{j - 1}\Vert}{a_{j - 1}\Vert r_j\Vert}q_j
                \end{align}
                Recall that parameters from Conjugate Gradient, $\sqrt b_j = \Vert r_{j + 1}\Vert/\Vert r_j\Vert$, and $a_j = \frac{\Vert r_j\Vert^2}{\Vert p_j\Vert_A^2}$, and we can use the substitution to match the coefficients for $q_{j + 2}$ and $q_j$, giving us: 
                \begin{align}
                    \frac{\Vert r_{j + 1}\Vert}{a_j\Vert r_j\Vert} &= 
                    \frac{1}{a_j}\sqrt{b_j}
                    \\
                    \frac{b_{j - 1}\Vert r_{j - 1}\Vert}{a_{j - 1}\Vert r_j \Vert}
                    &= 
                    \frac{b_{j - 1}}{a_{j - 1}}\frac{1}{\sqrt{b_{j - 1}}}
                    = 
                    \frac{\sqrt{b_{j - 1}}}{a_{j - 1}}
                    \\
                    \implies& 
                    \begin{cases}
                        \alpha_{j + 1} = \frac{1}{a_j} + \frac{b_{j - 1}}{a_{j - 1}}
                        & \forall 1 \le j \le n - 1
                        \\
                        \beta_{j} = \frac{\sqrt{b_{j - 1}}}{a_{j - 1}}
                        & \forall 2 \le j \le n - 2 
                    \end{cases}
                \end{align}
                Take notes that the form is now matched, but the expression for $\alpha_{j + 1}$ has an extra $b_{j - 1}/a_{j - 1}$, to resolve that, we take the audacity to make $b_0$ so that it's consistent with the base case. 
            \end{proof}
        \subsubsection{Fixing the Sign}
            We can't take the triumph yet; we need to take a more careful look into the sign between $q_j$ the Lanczos Vector and its equivalcne residual: $r_{j - 1}$ in CG. Here, I want to point out the fact that, there are potentially 2 substitution possible for the above derivation for the inductive case and regardless of which one we use, it would still preserve the correctness for the proof. By which I mean the following substitutions would have both made it work: 
            \begin{align}
                \begin{cases}
                    q_{j + 1} := \pm \frac{r_j}{\Vert r_j\Vert}
                    \\
                    q_{j} := \mp \frac{r_{j - 1}}{\Vert r_{j - 1}\Vert}
                    \\
                    q_{j + 2} := \pm \frac{r_{j + 1}}{\Vert r_{j + 1}\Vert}
                \end{cases}
            \end{align}
            Under the context, the operations $\pm, \mp$ are correlated, choose a sign for one, the other must be of opposite sign. In this case both substitutions work the same because multiplying the equation by negative one would give the same equality, and we can always mulitply by and other negative sign to get it back. The key here is that, the sign going from $q_{j}$ to the next $q_{j - 1}$ will have to alternate. To find out precisely which one it is, we consider the base case for the Lanczos Vectors and Residuals: 
            \begin{align}
                q_1 &= \hat r_0\\
                q_2 &= -\hat r_1
                \\
                \vdots
                \\
                q_j &= (-1)^{j + 1}\hat r_{j + 1}
            \end{align}
            And at this point, we have established the equivalence going from the Conjugate Gradient algorithm to the Lanczos Algorithm. And the moral of the story is, CG is a special case of applying the Lanczos Iterations with $q_1 = r_0$ to a positive definite matrix. However, something is still off and one can ask the following questions to inquiry further, leading us to more discussion between these 2 algorithms. 
            \begin{enumerate}
                \item [1.)] How are the solutions $x_k$ generated by CG relates to the Lanczos Iterations? 
                \item [2.)] How are the A-Orthogonal vectors $p_k$ from CG relates to Lancozs? 
            \end{enumerate}
            \begin{remark}[A Better Terminations Conditions for CG]
                The derivation hinted at a better terminations conditions for the CG algorithm. Because the algorithm is equivalent to the Lanczos Iterations initialized with $q_1 = r_0$, and we can directly apply from \hyperref[remark:Minimal_Polynomial_from_Lanczos_Iterations]{proposition 3.4.2} to get the precise number of iterations of CG under exact arithematic given $r_0$, improving the bound we got from \hyperref[prop:Termination_of_CG]{proposition 4.2}. 
            \end{remark}
    \subsection{From Lanczos to Conjugate Gradient}
            In this section, we show the equivalence of the Conjugate Gradient and Lanczos iterations by deriving the CG using the Lanczos. 
            \par
            The goal is to show that CG is a special case of Lanczos. In the end we discuss the key that it can inspire solvers for symmetric indefinite systems of linear equations. 
            \par
            We start of by considering the LU decomposition of the the Tridiagonal matrix of Lanczos and us its entries to express the scalars $a_k, b_k$ in CG. In addition, we express the conjugate vectors $p_k$ as a short recurrence of the lanczos vectors $q_k$. 
        \subsubsection{Matching the Residual and Conjugate Vectors}
            In this section, we go from the Iterative Lanczos algorithm to the Conjugate Gradient algorithm and we seek to establish a link between the solution $x_k, p_k$ from CG with the lanczos vectors and the symmetric tridiagonal matrix generated by Lanczos. This section will also play an important role for the backwards analysis for the floating point behaviors for the CG algorithm in the later parts of the paper. At the end, we highlight some of the hidden insights that this particular derivation of quivalence leads to, and how it inspires algorithms that directly solves symmetric indefinite systems. 
            \begin{prop}[Lanczos Vectors and Residuals]\label{prop:Lanczos_Vectors_and_Residuals}
                The $Q_k$ is the orthogonal matrix generated by Lanczos Iteration. To match the Krylov Subspace generated by the Lanczos iterations and CG, we initialize $q_1 = \hat{r}_0$, then the following relationship between Lanczos and CG occurs between their parameters: 
                \begin{align}
                    \begin{cases}
                        y_k = T^{-1}_k \beta\xi_1
                        \\
                        x_k = x_0 + Q_k y_k
                        \\
                        r_k = -\beta_{k}\xi_k^T y_k q_{k +1}
                    \end{cases}
                \end{align}
            \end{prop}
            The quantities $\alpha, \beta$ are the diaognal and the sub or super diagonal of the matrix $T_k$ from the Iterative Lanczos Algorithm, and $r_k$ is the residual from the Conjugate Gradient Algorithm, and $Q_k$ is the orthogonal matrix generated from the Lanczos Algorithm. For notations, we use $\xi_i$ to denote the ith canonical basis vector. $\beta$  without the subscript denotes $\Vert r_0\Vert$.
            \begin{proof}
                To start recall that the Lanczos Algorithm Asserts the following recurrences:
                \begin{align}
                    AQ_k = Q_{k + 1} \begin{bmatrix}
                        T_k
                        \\
                        \beta_k \xi_k^T
                    \end{bmatrix}
                \end{align}
                Recall that the Conjugate Gradient algorithm takes the guesses from the affine span of $x_0 + \mathcal{K}_k(A|r_0)$, from section \hyperref[sec:CG_and_Krylov_Subspace]{CG and Krylov Subspace (3.3.6)} we know that that: $p_k \in \mathcal K_{k + 1}(A|r_0)$, the matrix $P_k, Q_k$ spans the same subspace, and that means: 
                \begin{align}
                    x_{k + 1} &= x_0 + Q_ky_k
                    \\
                    r_{k + 1} &= r_0 - AQ_k y_k
                    \\
                    Q^H_kr_{k + 1} &= Q_k^H r_0 - Q_k^HAQ_k y_k
                    \\
                    \implies
                    0 &= \beta\xi_1 - T_k y_k
                    \\
                    y_k &= T_k^{-1}\beta \xi_1
                \end{align}
                Now to get the residual we simply consider: 
                \begin{align}
                    r_{k + 1} &= r_0 - AQ_k y_k
                    \\
                    &= r_0 - AQ_k T_k^{-1}\beta \xi_1
                    \\
                    \implies
                    &= \beta q_1 - AQ_k T_k^{-1} \beta\xi_1
                    \\
                    &= \beta q_1 - Q_{k + 1}\begin{bmatrix}
                        T_k \\ \beta_k \xi_k^T
                    \end{bmatrix}T_k^{-1} \beta\xi_1
                    \\
                    &= \beta q_1 - 
                    (Q_k T_k + \beta_k q_{k + 1}\xi_k^T)T_k^{-1} \beta\xi_1
                    \\
                    &= 
                    \beta q_1 - (Q_k \beta \xi_1 + \beta_k q_{k + 1}\xi_{k + 1}T_k^{-1}\beta \xi_1)
                    \\
                    &= -\beta_k q_{k + 1}\xi_k^TT_k^{-1} \beta \xi_1
                \end{align}
                On the third line we racall the fact that $q_1 = \hat{r}_0$ which initialized the Krylov Subspace for the Lanczos Iteration. At the 4th line, we make use of the Lanczos Vector recurrences and we simply substituted it.

                By observing the fact that $\xi^T_kT^{-1}_k\xi_1$ the $(k, 1)$ element of the matrix $T_k^{-1}$ which is a scalar, we can conclude that the residual from CG and the Lanczos vector are scalar multiple of each other, therefore, $r_k$ from the CG must be orthgonal as well. 
            \end{proof}
            \begin{prop}[Lanczos Vectors and Conjugate Vectors]
                The $P_k$ matrix as derived in the CG algorithm can be related to the Lanczos iterations by the formula: 
                \begin{align}
                    P_k = Q_k U_k^{-1}
                \end{align}
                Where $T_k = L_kU_k$, representing the LU decomposition of the tridiagonal matrix $T_k$ from the Lanczos Iterations. Because of the Tridiagonal nature of the matrix $T_k$, $L_k$ will be a unit bi-diagonal matrix and $U_k$ will be an upper bi-diagonal matrix. 
            \end{prop}
            \begin{proof}
                To prove it, we start by considering the $x_k$ at step k of the iterations: 
                \begin{align}
                    x_k &= x_0 + Q_k y_k 
                    \\
                    &= x_0 + Q_k T_{k}^{-1} \beta \xi_1
                    \\
                    &= x_0 + Q_k U_k^{-1}L_k^{-1}\beta \xi_1
                    \\
                    &= x_0 + P_k L_k^{-1}\beta\xi_1
                \end{align}
                So far we have written the solution vector $x_k$. Next, we are going to prove that the matrix $P_k$ indeed consists of vectors that are A-orthgonal. To show that we consider: 
                \begin{align}
                    & P_k^TAP_k
                    \\
                    &= (Q_k U_k^{-1})^T AQ_kU_k^{-1}
                    \\
                    &= U_k^{-T}Q_k^{T}AQ_k U_k^{-1}
                    \\
                    &= U_k^{-T}T_kU_k^{-1}
                    \\
                    &= U^{-T}_kL_k
                \end{align}
                Reader please observe that $U_k$ is upper triangular, therefore, it's inverse it's also upper triangular, therefore, $U_k^{-T}$ is lower triangular, and because $L_k$ is also lower triangular, their product is a lower triangular matrix, and therefore, the resulting matrix above is lower triangular, however, given that $P_k^TAP_k$ is symmetric, therefore, $U_k^{-T}L_k$ will have to be symmetric as well, and a matrix that is lower triangular and symmetric has to be diagonal. Therefore, the columns of $P_k$ are conjugate vectors. 
            \end{proof}
        \subsubsection{Matching the $a_k, b_k$ in CG}
            Similar to how we can generate the tridiagonal matrix for the Lanczos iterations with $q_1 = \hat{r}_0$, we can also generate the parameters $a_k, b_k$ in the CG algorithm using parameters from the Lanczos Iterations. To achieve it, one can simply build up the recurrences for the $y_k$ vectors using the elements from the $L_k, U_k$ matrix which comes from $LU$ decomposition of the $T_k$ matrix. This will come at the expense of losing some degree of accuracy because it's equivalent to doing the LU decomposition of $T_k$ without pivoting, but it comes at the advantange computing $\xi_k^TT_k^{-1}\xi_1$  with as little efforts as possible. Let's take a look. 
            \par
            For discussion in this section, we briefly switch the indexing and let it start counting from one instead of zero. 
            \begin{align}
                P_k = \begin{bmatrix}
                    p_1 & p_2& \cdots & p_k
                \end{bmatrix} \quad
                Q_k = \begin{bmatrix}
                    q_1 & q_2 & \cdots & q_k
                \end{bmatrix}
            \end{align}
            Using the invertibility of the matrix $A$ and Cauchy Interlace Theorem, $T_k$ is invertible, we consider the LU decomposition of the symmetric tridiagonal matrix: 
            \begin{align}
                T_k = L_k U_k =
                \begin{bmatrix}
                    1 & & & \\
                    l_1 & 1 & & \\
                    & \ddots& \ddots & 
                    \\
                    & & l_{k - 1} & 1
                \end{bmatrix}\begin{bmatrix}
                    u_1& \beta_1 & & \\
                    & u_2 &\beta_2 & \\
                    & &\ddots & \beta_{k - 1}\\
                    & & & u_k
                \end{bmatrix}
            \end{align}
            Reader should agree with some considerations that the upper diagonal of $U_k$ are indeed the same as the upper diagonal of the SymTridiagonal matrix $T_k$. And reacall the expression for $x_k$ from the previous section, we have: 
            \begin{align}
                x_k &= x_0 + P_k L_k^{-1}\beta \xi_1 \\ 
                x_{k} - x_{k - 1} &= 
                P_k L_k^{-1} \beta \xi_1 - P_{k - 1}L^{-1}_{k - 1}\beta \xi_1
                \\
                &= P_k \beta(L^{-1}_k)_{:, 1} - P_{k - 1}\beta(L^{-1}_{k - 1})_{:, 1}
                \\
                &= \beta(L^{-1}_k)_{k, 1}P_k
                \\
                \implies x_k &= x_{k - 1} + \beta(L^{-1}_k)_{k, 1}p_k
            \end{align}
            On the third line, we factor out the last column for the matrix $P_k$. Next, we wish to derive the recurrence between $p_{k + 1}$ and $p_k$. Which is: 
            \begin{align}
                P_k &= Q_k U^{-1}_k
                \\
                P_kU_k &= Q_k
                \\\implies
                \beta_{k - 1}p_{k - 1} + u_k p_k &= q_k 
                \\
                u_k p_k &= q_k - \beta_{k - 1}p_{k - 1}
                \\
                p_k &= u^{-1}_k(q_k - \beta_{k - 1}p_{k - 1})
            \end{align}
            We made use of the fact that the matrix $U_k$ is unit upper bidiagonal. Next, we seek for the recurrences of the parameters $u_k, l_k$. Let's consider the recurrence using the block structure of the matrices: 
            \begin{align}
                T_k &= L_kU_k
                \\
                T_{k + 1} &= \begin{bmatrix}
                    T_k & \beta_k \xi_k \\
                    \beta_k \xi^T_k & \alpha_{k + 1}
                \end{bmatrix} = 
                \begin{bmatrix}
                    L_k & \mathbf{0} \\ l_k \xi_k^{-1} & 1
                \end{bmatrix}
                \begin{bmatrix}
                    U_k & \eta_k \xi_k \\
                    \mathbf{0} & u_{k + 1}
                \end{bmatrix}
                \\
                &= 
                \begin{bmatrix}
                    L_kU_k & \eta_k L_k \xi_k 
                    \\
                    l_k \xi_k^TU_k & \eta_k l_k \xi_k^T \xi_k + u_{k + 1}\alpha_k
                \end{bmatrix}
                \\
                &= 
                \begin{bmatrix}
                    T_k & \eta_k (L_k)_{:, k} \\ 
                    l_k(U_k)_{k, :} & \eta_k l_k + u_{k + 1}
                \end{bmatrix}
                \\
                &= 
                \begin{bmatrix}
                    T_k & \eta_k \\
                    l_k u_k & \eta_k l_k + u_{k + 1}
                \end{bmatrix}
            \end{align}
            Note that I changed the upper diagonal of matrix $U$ at the top to be $\eta_k$ instead of $\beta_k$ so we have a chance to convince ourselves that $\beta_k$ for the upper diagonal of $T_k$ are indeed the same as the $\eta_k$ for the upper diagonal of matrix $U_k$. From the results above, $\eta_k = \beta_k$ as expected, and $l_k = \beta_k/u_k$, $u_{k + 1} = \alpha_{k +1} - \beta_{k}l_k$, and hence, to sum up the recurrence relation we have: 
            \begin{align}
                \begin{cases}
                    u_{k + 1} &= \alpha_{k + 1} - \beta_k^2/u_k
                    \\
                    l_k &= \beta_k/u_k
                \end{cases}
            \end{align}
            The base case is $u_1 = \alpha_1$. The recurrence of the parameter $u_k$ is immediately useful for figuring out the recurrence for $x_k$, And to figure out the recurrence relations of $(L^{-1}_k)_{k, 1}$, we consider the following fact: 
            \begin{align}
                L^{-1}_k L_k &= I 
                \\
                \begin{bmatrix}
                    L^{-1}_k & \mathbf{0} \\
                    s_k^T & d_{k + 1}
                \end{bmatrix}
                \begin{bmatrix}
                    L_k & \mathbf{0} \\
                    l_k \xi_k^T & 1
                \end{bmatrix} &= I
                \\
                \begin{bmatrix}
                    I & \mathbf{0} \\ 
                    s_k^TL_k + d_{k + 1}l_k \xi_k^T &d_{k + 1}
                \end{bmatrix} &= I
            \end{align}
            It equals to the identity matrix therefore $d_{k + 1} = 1$, and it has to be that the the lower diagonal sub vector in the results has to be zero. For the bi-lower unit diagonal matrix $L_k$, we cannot predict the structure, most of the time it's likely to be dense and unit lower  triangular. We are interested in look for the first element of the vector $s_k^T$, the equality will assert: 
            \begin{align}
                s^TL_k + d_{k + 1}l_k \xi_k^T &= \mathbf{0}
                \\
                L_k^{T}s_k + d_{k + 1}l_k \xi_k &= \mathbf{0}
                \\
                s_k + L^{-T} d_{k + 1}l_k \xi_k &= \mathbf{0}
                \\
                (s_k)_1 + d_{k + 1}l_k ((L^{-1}_k)\xi_k)_1 &= 0
                \\
                (s_k)_1 + d_{k + 1}l_k(L^{-1}_k)_{k , 1} &= 0
                \\\implies
                (s_k)_1 &= - l_k(L^{-1}_k)_{k, 1} 
                \\
                (s_k)_1 &= (L^{-1}_{k + 1})_{k + 1, 1}  \quad \text{by definition}
                \\
                \implies
                (L^{-1}_{k + 1})_{k + 1, 1} &=
                -l_k(L^{-1}_k)_{k, 1}
            \end{align}
            To assert the fact that $L_k^{-1}L_k$ is identity, we carefully consider the last row excluding the bottom right element vector: $s^TL_k + d_{k + 1}l_k\xi_k^T$ which will have to be a zero vector. From the first line to the second line, we took the transpose of the vector. From the second to the third line we multiplied both side by inverse of $L_k^{T}$. And from the third to the forth line, we took out only the first element in the vector. Observe that the first element of the vector $L_k^{-1}\xi_k$ is just $(L_k^{-1})_{k, 1}$. 
            \par
            Therefore the recurrence for the step size into the direction of the conjugate vector requires us to use the newest element $l_k$ from $L_{k + 1}$ and the previous step size in the direction of the conjugate vector $p_k$. The short recurrence allows us to build up another algorithm that is just as efficient as CG algorithm but running Lanczos Algorithm at the same time. 
            \begin{remark}
                The derivation might seems excessive for the discussion, but it's part of the analysis of the Conjugate Gradient. It derived the conjugate gradient without using the fact that $A$ is symmetric positive definite providing potential to new algorithm that can solve symmetric indefinite system directly. The Lanczos Algorithm for linear system (we refer to the method derived in the above section) is a special case of FOM (\textbf{CITATION NEEDED}) when the matrix $A$ is symmetric. The above algorithm is just FOM with a short term recurrences for its parameters, and it's based on the fact that $A$ is symmetric. It's implied from the above derivation that under exact arithemtic, CG can be applied to symmetric indefinite system, if we have the luck where $T_j$ is non-singular for all $j\le k$. Recall that when we derived the CG algorithm, we convert it into solving the system: $P^T_kr_0 = P_k^TAP_kw$, and when the matrix $A$ is indefinite, we can still solve the system and get the saddle points for the indefinite error norm. However there are 2 problems solving Symmetric Indefinite using the CG is that, the following recurrence doesn't hold anymore and it should be restated as: 
                \begin{align}
                    AQ_k &\approx Q_{k + 1}
                    \begin{bmatrix}
                        T_k \\ \beta_k \xi_k^T
                    \end{bmatrix}
                \end{align}
                Another problem is when $T_k$ is singular during one iteration of the Lanczos Algorithm. For example, $T_j$ will become singular when $A$ is a symmetric tridiagonal matrix with zeros as its diagonals and all ones on it's subdiagonals. However, these problems can be solved by considering something other than LU without pivoting. In fact, there are works of Paige, Y. Saad extending the idea and derived algorithms that can solve a symmetric indefinite system without using the norm equation. 
                \textbf{CITATION NEEDED}. 
            \end{remark}
        
\section{Effects of Floating Point Arithematic}
    In this section, we highlights the practical concerns of the algorithm and showcase it with numerical experiments with analysis, in hope to get deeper insights about the behaviors of Lanczos and Conjugate Gradient under floating point arithematic. 
    % TODO: ADD SOME MORE STUFF HERE. 
    \subsection{Partial Orthogonalization and Full Orthogonalization}
        In this section, we use some of the results and insight obtained from the derivation of the CG algorithm to develop a theoretical fix for the loss of precision under floating point arithmetic for the CG algorithm. The floating errors inside of the CG algorithm is manifested as a loss of orthogonality and loss of conjugacy for the vectors $r_k, p_k$. To fix this, we simply use the CDM algorithm's projector to re-orthogonalize the conjugate vectors and the residual vectors so the A-Orhtogonality and Orthogonality are both preserved for the $r_k, p_k$ vectors. Such idea is not new and it's stated in the original paper by Hestenes and Stiefel back in 1952; here we present the second more computationally expensive idea in the paper by Hestenes and Stieful, but using the quantities we derived in the first section. Firstly recall the proof for \hyperref[prop:CG_Recurrences]{proposition 3.9}. Next, we inductively consider the case where the newest residual vector is involving some round off error $\overline{r}_{k + 1}$ and it breaks the orhogonality conditions $\overline{r}_{k + 1} \perp r_{j} \; \forall \; 0 \le j \le k$: 
        \begin{align}
            (AP_k)^T\overline{r}_k &= 
            \begin{bmatrix}
                \langle p_0, A\overline{r}_k\rangle
                \\
                \langle p_1, A\overline{r}_k\rangle
                \\
                \vdots
                \\
                \langle p_{k - 1}, A\overline{r}_k\rangle
            \end{bmatrix}
            \\
            & = 
            a_{k - 1}^{-1}\langle r_k, (r_{k - 1} - r_k)\rangle\xi_k + \sum_{j = 0}^{k - 1}\langle p_j, A\overline{r}_k\rangle \xi_j
            \\
            p_k &:= \overline{r}_k + b_kp_k - 
                \frac{\langle \overline{r}_k, r_{k -1}\rangle}{\langle r_{k - 1}, r_{k - 1}\rangle}p_k
            - \sum_{j = 0}^{k - 1}\frac{\langle p_j, A\overline{r}_k\rangle}{\langle p_k, Ap_k\rangle}p_j
            \\
            r_k &:= \overline{r}_k - \sum_{j = 0}^{k - 1} \langle \hat{r}_j,\overline{r}_k\rangle \hat{r}_j
        \end{align}
        Here, we generate the conjuate vectors $p_k$ correctly by faithfully reproducing the term $(AP_k)^T\overline{r}_k$, and then we upadate the residual $\overline{r}_k$ into $r_k$ by orthogonalizing it against all previous residual vectors. Doing ths require the expensive storage of previous vectors $p_k$. One can use alternative formulas to A-orthogonalize $p_k$ and re-orthogonalize $r_k$. In addition, we have the options for partially orthogonalizing the $r_k, p_k$ vectors for less memory usage. 
    \subsection{Relative Errors of CG Under Floating Points}
        Here we use full reorthogonalization to emulate the effects of exact arithematic. the step required for convergence is less than or equal to the number of unique eigenvalues for the symmetric definite matrix, this is established in part (2.2) of the discussion. However in practice, this is not always the case. Similar experiments are conducted in Greenbaum's works (\textbf{CITATION NEEDED}). In this section, we replicate the same set of experiments using modern Julia to showcase the extra steps required for the CG algorithm to converge. For testing the convergence of the algorithm we use a 16 digits floats to exagerate the floating point roud off error. The spectrum of the matrix we are using are taken to be the same from Greenbaum's works (\textbf{CITATION NEEDED}): 
        \begin{align}
            \lambda_{\min} + \left(
                \frac{j}{N - 1}
            \right)(\lambda_{\max} - \lambda_{\min})\rho^{N - j + 1}\quad \forall\; 1 \le j \le N - 1, \; 0 \le \rho \le 1
        \end{align}
        If the value of $\rho$ is close to zero, then the eigenvalues are clustered around the origin, if it's close to $1$, then the eigenvalues are tend to be more evenlly distributed around the interval $[\lambda_{\min}, \lambda_{\max}]$. 
        \begin{figure}[h]\label{fig:1}
            \centering
            \includegraphics[width=8cm]{cg_convergence_0.9.png}
            \includegraphics[width=8cm]{cg_convergence_1.png}
            \caption{
                The relative energy norm error for different methods. Blue solid line: The exact conjugate gradient convergence. Purple dotdashed: The conjugate gradient that is partially orthgonalized with previous 8 residual and conjugate vectors. orange dashed: the Original conjugate gradient. Green dot: The tighter theretical upper bound derived by chebyshev (4.3.20). Left: $\rho = 0.9$; right: $\rho = 1$, the matrix is $64\times 64$
            }
        \end{figure}
        The Chebyshev bound is no longer a tight bound because the distribution of the eigenvalues are not perfectly uniform. The partially orthogonalized methods diverges from the exact error after more steps of iterations compare to the relative error without any orthogonalizations. These are seemed in \hyperref[fig:1]{(fig 1 left)}. In contrast on the right, then the eigenvalues are uniformlly distributed, the convergence of all 3 types of methods aligns with the exact convergence in \hyperref[fig:1]{(fig 1 right)}. 
        \begin{remark}
            The convergence is disappointing under floating point arithematic and the promised efficiency of the algorithm is not there anymore if the matrix is not neccessarily ill-conditioned. Just from \hyperref[fig:1]{(fig 1)} it seems like outlier eigenvalues provide fast convergence under full orthogonalization, but not for floating point. 
        \end{remark}
    \subsection{Paige's Floating Point Analysis}
        In this section we present the backwards analysis of the algorithm in a more thorough manner and exam its consequences. When floating point arithematic is used, the eigenvalues of the Tridiagonal matrices might inttroduces ghost eigenvectors for the Lanczos Iterations, and using the equivalence of the Lanczos iterations and CG, we can capture a posteri bound on how much the error is exactly during the iterations of CG. 
        \subsubsection{Bounding the The Relative Residuals}
            Recall from proposition \hyperref[prop:Lanczos_Vectors_and_Residuals]{Propposition 4.6} that the residual of the CG can be expressed interms of the Lanczos vectors. However, the Lanczos iterations under floating doesn't produces perfectly orthogonal Lanczos Vectors (The lost of orthogonality is experimented and visualized in the next section), $\tilde{Q}_k$ is not quiet orthogonal, which would also means that $\tilde{Q}_k^HA\tilde{Q}_k\approx T_k$ where $T_k$ is the results from the Lanczos iterations will not equal to $\tilde{Q}^T_KA\tilde{Q}_k$. However the Lanczos iterations will still solve for $y_k$ using the expression $y_k = \beta T^{-1}_k\xi_1$, the algorithm still thought $T_k$ produced by itself is perfectly tridiagonal. As a result the algorithm never quiet find the optimal under the conjugate basis. At first glance, tiny round-off errors in the Lanczos vectors is very problematic. 
            \par
            Surprisingly the recurrence formula for lanczos still holds to some extend, in addition we can leverage the fact that $y_k$ is solved exactly and assume: $y_k = \beta T^{-1}\xi_1$ is at least, exact. Then it left us with fewer type of floating errors to keep track of. Next, we proceed to look for the residual of the CG algorithm by assuming that the lanczos recurrences: 
            \begin{align}
                AQ_k = Q_{k + 1} \begin{bmatrix}
                    T_k
                    \\
                    \beta_k \xi_k^T
                \end{bmatrix} + F_k
            \end{align}
            Reader please reflect on the fact that the $Q_k$ which is not orthogonal, and we are fixing the recurrences with $F_k$, a matrix representing the floating error to correct it so that the equality holds true. $\Vert F_k\Vert$ is small and it will be stated later.  
            \begin{align}
                r_k &= r_0 - AQ_ky_k
                \\
                r_k &= r_0 - 
                    \left(
                        Q_{k + 1} \begin{bmatrix}
                            T_k
                            \\
                            \beta_k \xi_k^T
                        \end{bmatrix}
                        + 
                        F_k
                    \right)
                y_k
                \\
                r_k &= 
                \underbrace{\left(
                    r_0 - Q_{k + 1}\begin{bmatrix}
                        T_k
                        \\
                        \beta_k \xi_k^T
                    \end{bmatrix}y_k
                \right)}_{= -\beta_k\xi_k^Ty_kq_{k + 1}} + F_k \beta T_k^{-1}\xi_1
                \\
                \implies
                \frac{\Vert r_{k + 1}\Vert}
                {
                    \Vert r_0\Vert
                } &\le
                \beta_k \Vert
                    \xi_k^T T_k^{-1}\xi_1 q_{k + 1}
                \Vert + 
                \Vert F_kT_k^{-1}\xi_1 \Vert
                \\
                \frac{\Vert r_{k + 1}\Vert}
                {
                    \Vert r_0\Vert
                } 
                &\le 
                \beta_k |
                    \xi_k^T T_k^{-1}\xi_1
                | + 
                \Vert F_k\Vert  \Vert T_k^{-1}\xi_1\Vert
            \end{align}
            Here, because of the assumption of exactness for the $Q_k$ matrices we are able to reuse \hyperref[prop:Lanczos_Vectors_and_Residuals]{Propposition 4.6} to regaudge it and obtain a similar expression because it didn't make use of the fact that $Q_k$ is orthogonal. This time, we take $F_k$ into account. The residual is now bounded by the sum of scalar $\xi_k^T T_k^{-1}\xi_1$ and the floating point error matrix $F_k$ produced by the Lanczos iterations. 
            \begin{remark}\label{remark:CG_Float_Remark}
                We can bound the first term that made up the upper bound for the residual of CG using previous convergence results of CG under exact arithemtic; recall \hyperref[theorem:CG_Convergence_Rate]{CG convergence rate}. It can be applied here for the first term in (5.3.6): 
                $\beta_k |
                \xi_k^T T_k^{-1}\xi_1|$. 
                \par
                This is true because if we were to perform an CG on the $T_{k + 1}$ produced by the finite precision algorithm with the initial Lanczos vector $q_1$ being $\xi_1^{(n)}$, then its residual $\overline{r}_{k}$ would be exact and it's given as $-\beta_kT_{k}^{-1}\xi_kq_{k + 1}$, but with $q_{k + 1} = \xi_{k + 1}^{(n)}$, the $k + 1$ th standard basis vector in $\mathbb R^{n}$, and $T_k = (T_{k + 1})_{1:k, 1:k}$ . And to our excitment, we already have the bound for $\overline{r}_{k+1}$ proven in section 4.3. 
            \end{remark}

        \subsubsection{Paige's Theorem and Backwards Analysis of CG}
            We now introduce a new theorem poposed by Paige (\textbf{CITATION NEEDED}). It gives a bound to the floating point errors for the Conjugate gradient by bounding the conditions numbr of $T_k$ from Lanczos iterations. It's stated as follow: 
            \begin{theorem}[Paige's Theorem]
                The eigenvalues $\theta_i^{(j)}, i = 1, \cdots, j$ of the tridiagonal matix $T_j$ satisfies: 
                \begin{align}
                    & 
                    \lambda_1 - j^{5/2}\epsilon_2\Vert A\Vert 
                    \le \theta_i^{(j)}
                    \le 
                    \lambda_n + j^{5/2}\epsilon_2\Vert A\Vert
                    \\
                    &
                    \epsilon_2 := \sqrt{2}\max\{6\epsilon_0, \epsilon_1\}
                \end{align}
            \end{theorem}
            Along with this theorem, the following quantities are also defined and cited in Greenbaum's work (\textbf{CITATION NEEDED}) from the work of Paige. 
            \begin{align}
                & \epsilon_0 \equiv 2(n + 4)\epsilon
                \\
                & \epsilon_1 \equiv 2(7 + m \Vert  \;|A|\;\Vert/\Vert A\Vert)\epsilon
                \\
                & \epsilon_0 < \frac{1}{12} \quad k(3\epsilon_0 + \epsilon_1) < 1
                \\
                & \Vert F_k\Vert \le \sqrt{k}(\epsilon_1) \Vert A\Vert
                \\
                & \Vert q^T_jq_j -1\Vert \le 2\epsilon_0
                \\
                & \beta_j \le \Vert A\Vert(1 + (2n + 6)\epsilon + j(3\epsilon_0 + \epsilon_1))
            \end{align}
            The quantity $k$ is the current iterations number of the Lanczos Iterations, $j\le k$. $m$ is the maximum number of non-zero element in the matrix $A$. 
            Using Paige's theroem, we can bound the conditions number for the matrix $T_{k + 1}$ produced by the finite precision Lanczos, which is given by: 
            \begin{align}
                \tilde{\kappa} = \frac{\lambda_n + (k + 1)^{5/2}\epsilon_2\Vert A\Vert}
                {\lambda_1 - (k + 1)^{5/2} \epsilon_2\Vert A\Vert}
            \end{align}
            Using \hyperref[remark:CG_Float_Remark]{(remark 5.3.1)}, we can make the following proposition
            \begin{prop}
                \begin{align}
                    | \beta_k\xi_k^TT_k^{-1}\xi_1 | \le 
                    2 \sqrt{\tilde{\kappa}}\left(
                        \frac{\sqrt{\tilde{\kappa}} - 1}{\sqrt{\tilde{\kappa}} + 1}
                    \right)^k
                \end{align}    
                Where $\tilde\kappa$ is the bound of the condition number of the $T_k$ matrix (from (5.3.15)), the tridiagonal matrix produced by the finite precision Lanczos. 
            \end{prop}
            \begin{proof}
                Using the \hyperref[lemma:Relative_Energy_Norm_and_Relative_2_Norm_Conversions]{lemma in appendix}, we can derive the relations between the 2 norm of the relative residuals and the energy norm of the relative error: 
                \begin{align}
                    \frac{\Vert Ae_k\Vert}{\Vert Ae_0\Vert}
                    & \le 
                    \kappa(T_k)\frac{\Vert e_k\Vert_A}{\Vert e_0\Vert_A}
                    \le 2\sqrt{\kappa(T_k)}
                    \left(
                        \frac{\sqrt{\tilde{\kappa}} - 1}{\sqrt{\tilde{\kappa}} + 1}
                    \right)^k
                    \\
                    \frac{\Vert r_k\Vert}{\Vert r_0\Vert} &= 
                    | \beta_k\xi_k^TT_k^{-1}\xi_1 | \quad \text{by \hyperref[remark:CG_Float_Remark]{(remark 5.3.1)}}
                    \\
                    \implies | \beta_k\xi_k^TT_k^{-1}\xi_1 | 
                    &\le 
                    2 \sqrt{\tilde{\kappa}}
                    \left(
                        \frac{\sqrt{\tilde{\kappa}} - 1}{\sqrt{\tilde{\kappa}} + 1}
                    \right)^k
                \end{align}
                The third inequality is simply from \hyperref[theorem:CG_Convergence_Rate]{CG Convergence Rate} when we assume that the eigenvalues are uniformly distributed convex hull of the spectrum of $A$. The first fraction is actually the relative error of the 2 norm of the residual becaus $Ae_k = r_k$ by definition. Subtituting the quantity $\kappa(T_k)$, the conditions number of the matrix $T_k$, which we figured out using Paige's theorem and denoted it as $\tilde{\kappa}$. 
            \end{proof}
            Finally, if we assume that $T_k^{-1}$ is actually invertible, which requires that the conditions for all the quantities: $\epsilon_0, \epsilon_1$ holds true, and $\lambda_1 - (k + 1)^{5/2} \epsilon_2\Vert A\Vert > 0$. Finally, we make can bound the relative residual of the CG algorithm by considering: 
            \begin{align}
                \frac{\Vert r_{k + 1}\Vert}
                {
                    \Vert r_0\Vert
                } &\le
                \beta_k \Vert
                    \xi_k^T T_k^{-1}\xi_1 q_{k + 1}
                \Vert + 
                \Vert F_kT_k^{-1}\xi_1 \Vert
                \\
                & \le 
                \beta_k|\xi_k^T T_k^{-1}\xi_1|
                \Vert q_{k + 1}\Vert + 
                \Vert F_k\Vert  \Vert T^{-1}_k\xi_1\Vert
                \\
                & \le 
                2 \Vert q_{k + 1}\Vert\sqrt{\tilde{\kappa}}
                    \left(
                        \frac{\sqrt{\tilde{\kappa}} - 1}{\sqrt{\tilde{\kappa}} + 1}
                    \right)^k
                + 
                \sqrt{k}(\epsilon_1) \Vert A\Vert 
                \Vert T^{-1}_k\Vert
            \end{align}
            Now, observe that $|q_j^Tq_j - 1|\le 2\epsilon_0$ from (5.3.13), which implies that $\Vert q_{k + 2}\Vert^2 \le (1 + 3\epsilon_0)$ which is $\Vert q_{k + 1}\Vert \le \sqrt{1 + 2\epsilon_0}$. In persue of mathematical beauty, we look for alternative expression for the quantity $\Vert A\Vert \Vert T^{-1}_k\Vert$ giving us: 
            \begin{align}
                \Vert A\Vert\Vert T_k^{-1}\Vert &= \frac{\lambda_n}{\lambda_1 - k^{5/2}\epsilon_2\Vert A\Vert} \le \tilde{\kappa}
                \\
                \implies 
                \frac{\Vert r_{k + 1}\Vert}
                {
                    \Vert r_0\Vert
                } &\le 
                2  \sqrt{1 + 2\epsilon_0}\sqrt{\tilde{\kappa}}
                    \left(
                        \frac{\sqrt{\tilde{\kappa}} - 1}{\sqrt{\tilde{\kappa}} + 1}
                    \right)^k
                + \sqrt{k}(\epsilon_1) \tilde{\kappa}
            \end{align}
            Which completes the proof for the upper bound on the convergence rate for the Conjugate Gradient Method under floating point arithematic. 
            \begin{remark}
                This proof showed that the Conjugate Gradient method for $T_{k + 1}$ that is non-singular then it will be backwards stable. As long as the value of $\beta_k$ is non zero, and the eigenvalues of $T_{k + 1}$ is not containing zero, the conjugate gradient method will continue to converge in the future iterations. So in laymen's term, it doesn't matter if the roudoff error accumulated, Conjugate Gradient will converge as long as the problem is not too insane, or $A$ being too pathological to deal with. 
                \par
                Finally, I want to point out the fact that Paige's theorem is derived using forward error analysis on the Lanczos Iterations, which is the absolute worst case. For most cases in modern computing platforms, the summation process of vector dot products has much higher floating-accuracy compare to older computing platforms due to the use of parallelizism, or floating point specific summation instructons, which reduces the relative sizes for the sumees, hence reducing the total roudoff error accumulations. 
            \end{remark}

    \subsection{Ghost Eigenvalues and Losing Orthogonality}
        The name Ghost Eigenvalues refers to the phenomena where the Lanczos Algorithm seems to find eigenvalues that are extremely close to each other, when in fact, the extremely close eigenvalues are a single eigenvalue. More specifically, the eigenvalues in $T_k$ seems to cluster closely around the true eigenvalues of the original matrix $A$. We know for a fact that the tridiagonal matrix produced via Lanczos can't have any repeated eigenvalues, this follows from (\hyperref[remark:Minimal_Polynomial_from_Lanczos_Iterations]{remark 3.4.2}). This phenomena is in fact, due to the limited floating point accuracy of the Lancozs orthogonalization process. Here, we conducted the numerical experiments and carefully reproduce the phenomena for a diagonal matrix $A$ with diagonals given by the formula: $\lambda_i = \left(-1 + \frac{2(i - 1)}{(n - 1)}\right)^3$ where $A\in \mathbb R^{n\times n}$. This matrix is particularlly good for reproducing the the pohenomena. For this experiment, we set $n = 64$ and we use Float64. We run the Lanczos iterations and mark the smallest and largest 10 eigenvalues during the iterations and plotted out their trajectories from iteration 10 to 64. The results can be seemed at \hyperref[fig:2]{(fig 2)}. 
        \begin{figure}[H]\label{fig:2}
            \centering
            \includegraphics[width=14cm]{ritz_trajectory_plot.png}
            \caption{The highest and lowest 10 eigenvalues of the matrix $T_k$ during the Lanczos Iterations are being tracked by their relative order. During each iterations, the first, the second, the third, ... etc eigenvalues of $T_k$ are linked together by a line in a different color.}
        \end{figure}
        Recall from the Cauchy Interlace Theorem, the eigenvalues of the Tridiagonal Matrix $T_{k + 1}$ has to be in between each eigenvalues of $T_{k}$ except for the first and the last eigenvalue of $T_{k + 1}$.  This implies that, the $\theta_i^{(k)}$, the $i$ th eigenvalues during the $k$ th iteration will move monotonically upwards or downwards during the Lanczos iterations. The ghoest eigenvalues on the figure appears when some of the interior eigenvalues suddenly switched to another eigenvalue's trajectory that is on the exterior of the spectrum. It appears as the matrix $T_k$ has repeated eigenvalues but in fact they are not and they are just very close. 
        \par
        However, judging the eigenvalues of the matrix $T_k$ alone will distinguish us whether 2 very close eigenvalues correspond to 2 different eigenvalues of $A$ or it's due to the floating point round off error, even if their trajectories seems to sugguest it. We can't tell it because if I keep the matrix $T_k$ generated from an Lanczos iterations with huge amount of round off error and all it $A$, which has eigenvalues that are extremely close to each other and their path shifts over each other on the trajectory like in \hyperref[fig:2]{fig 2}. Now consider performing another Lanczos iteations $A:=T_k$ it but with the initial vector $\xi_1$, then we will exactly reproduce $A$ itself because it's tridiagonal. But in this case, the eigenvalues of $T_k$ is exact after termination of Lanczos iteration. In this case, all eigenvalues are actually presented in the original matrix $A$, which is just $T_k$, itself. 
        \par
        In fact, the ghost eigenvalues here are produced by floating point errors because firstly we know what the actual eigenvalues of $A$ is, we made $A$. To make sense of it better intuitivlye, we observe conduct experiments to show that the lost of orthogonality of $Q_k$ happens together with ghost eigenvalues on spetrum of $T_k$. If $Q_k$ matrix is perfectly orthogonal, then there is no ghost eigenvalues, regardless of how the trajectories of the eigevalues of $T_k$ looks like. In fact, a corresponding plot of $Q_k^HQ_k$ are plotted in \hyperref[fig:3]{(fig 3 left)} for demonstrating lost of orthogonality for the same diagonal matrix $A$ proposed earlier. We plotted the heat map of the matrix $Q^H_kQ_k$ directly as well.
        \begin{figure}[H]\label{fig:3}
            \centering
            \includegraphics[width=8cm]{fig3.png}
            \includegraphics[width=8cm]{fig4.png}
            \caption{left: The heatmap of the plot of log2 of the absolute values of the matrix $Q^H_kQ_k$. right: The plot of $Q_kAQ_k$ from floating point Lanczos iterations}
        \end{figure}
        In addition to the lost orthogonality of the matrix $Q_k$, we also visualized the actual tridiagonal matrix reproduced by $Q_kAQ_k$ which is plotted in \hyperref[fig:3]{(fig 3 right)}. Observe that most of the off tridiagonal entries are non-zero and relatively huge, which doesn't worry us too much because $A$ itself has an extremely bad conditions number. What is important is the blob of non-zero entries on the top left and the bottom right of the plot, which looks similar to how the $Q_k^HQ_k$. And this is a hint that the lost of orthogonality will create extra errors on the off tridiagonal parts of the matrix $T_k$. 
        \par
        To gain a better understanding, let's define the notion of Ritz values and Ritz vectors. For our dicussion, the ritz value $\theta_i^{(k)}$ are the $i$ th eigenvalues of the matrix $T_k$ from the and the Ritz vectors are $Q_ks_{i}^{(k)}$ where $s$ is the $i$ th eigenvector for the matrix $T_k$. Recall from \hyperref[remark:Minimal_Polynomial_from_Lanczos_Iterations]{remark 3.4.2}, the characteristic polynomial of $T_k$ is the monic that minizes the 2 norm error for the vector $p_k(A)q_1$; intuitively, the ritz values and ritz vectors approximates eigenvalues and eigenvectors of matrix $A$. One would expect once the eigenvalue converged for $T_k$, say $\theta_i^{(k)} = \lambda_i + \epsilon$ where $\lambda_i$ is an eigenvalue of $A$, then the vector $Q_ks_i^{(k)}$ approximate eigenvalue of $A$ because: 
        \begin{align}
            AQ_k &= Q_k T_k + q_{k + 1}\beta_k\xi_k^T
            \\
            AQ_ks_i^{(k)} &= \theta Q_k s_i^{(k)} + q_{k + 1}\beta_k \xi_k^Tv
            \\
            AQ_ks_i^{(k)} &= \theta Q_k s_i^{(k)} + \beta_kq_{k + 1}(s_i^{(k)})_k
            \\
            AQ_ks_i^{(k)} &= (\lambda_i + \epsilon) Q_k s_i^{(k)} + \beta_kq_{k + 1}(s_i^{(k)})_k
            \\
            AQ_ks_i^{(k)} &= \lambda_i Q_k s_i^{(k)} + \epsilon Q_k s_i^{(k)} + \beta_kq_{k + 1}(s_i^{(k)})_k
        \end{align}
        When $\theta$ is close to $\lambda_i$, and the characteristic polynomial of $T_k$ interpolates the eigenvalues of $A$ better and better, $\beta_k$ decreases (\textbf{NONTRIVIAL FACT}), the ritz vector $Q_ks_i^{(k)}$ approximates the eigenvector for $A$. 
        \begin{figure}\label{fig:4}
            \centering
            \includegraphics[width=10cm]{lanczos_proj_on_ritz.png}    
        \end{figure}

        
        \begin{remark}
            As a final remark for these numerical experiments, I sugguest an intuitive way of understanding them. Which will be useful when we actually wish to analyze it rigorously. Simply put, the Lanczos Iterations might ``forget'' about the eigenvalues when it converged (Manifested as the usually stable trajectories of eigenvalues on the exerior of the spectrum for the matrix $T_k$ in \hyperref[fig:2]{(fig 2)}), and when it happens, the Lanczos vectors produced by the algorithm has lost its orthogonality corresondingly, which then causes the interior eigenvalues of $T_k$ to shift, creating ghost eigenvalues for the matrix $T_k$. 
        \end{remark}
    
        
    \subsection{The Rizt Vector's View and Another Paige's Theorem}
        In this section, we prove another analysis of the Lanczos Iterations from Dammel's work(\textbf{Citation Needed}) where he introduced proof of another Paige's theorem that shows exactly how to measure the lost of orthogonality of the Lanczos vectors while Lanczos algorithm is running. Here, we wish to prove it in much more details with more thoroughness.
        \par
        The theorem highlights 2 important facts. The first is the the lost of orthogonality of Lanczos Vector and Ghost eigenvalues appears at the same time and they are sysmatic. The second is that the projection of the Lanczos vector onto the converged ritze vectors can be numerically attained, which tells us how much lost of orthogonality is occuring and which direction we need to re-orthogonalize so that the Lanczos vectors retains orthogonality. Here is the statement of the theorem: 
        \begin{theorem}[Another Paige's Theorem]
            
        \end{theorem}
    \subsection{Forward Error Analysis}


\newpage
\section{Appendix}
    \subsection{Useful Lemmas}
        \begin{lemma}[Relative Energy Norm and Relative 2 Norm Conversions]\label{lemma:Relative_Energy_Norm_and_Relative_2_Norm_Conversions}
            Let $A$ be a Positive Symmetric Positive Definite Matrix, then it can be said that: 
            $$
            \frac{\Vert A x\Vert}{\Vert Ay \Vert} \le \kappa(A)\frac{\Vert 
            x\Vert_A}{\Vert y \Vert_A}
            $$
        \end{lemma}
        \begin{proof}
            From the definition of incuded 2-norm of matrices, assuming that $\lambda_1$ is the minimum eigenvalue of the matrix $A$, and $\lambda_n$ the maximum, and the fact that matrix $A$ has factorization $A^{1/2}A^{1/2}$: 
        \end{proof}
        \begin{align}
            \lambda_1 \Vert x \Vert 
            &\le \Vert Ax\Vert 
            \le \lambda_2 \Vert x\Vert
            \\
            \sqrt{\lambda_1} \Vert x\Vert 
            & \le \Vert A^{1/2}x\Vert \le \sqrt{\lambda_n}\Vert x \Vert
            \\
            \implies
            \sqrt{\lambda_1} & \le \frac{\Vert Ax\Vert}{\Vert A^{1/2}x \Vert} 
            \le \sqrt{\lambda_n}
        \end{align}
        Consider another vector $y$: 
        \begin{align}
            \sqrt{\lambda_1} \le \frac{\Vert Ay\Vert}{\Vert A^{1/2}y \Vert} \le \sqrt{\lambda_n}
        \end{align}
        Combining the two we have: 
        \begin{align}
            \sqrt{\lambda_1}\frac{\Vert Ax\Vert}{\Vert A^{1/2}x \Vert} 
            & \le \sqrt{\lambda_n}\sqrt{\lambda_1}
            \\
            \sqrt{\lambda_1}\sqrt{\lambda_n}& \ge \sqrt{\lambda_n} \frac{\Vert Ay\Vert}{\left\Vert
                A^{1/2}y
            \right\Vert}
            \\
            \implies 
            \sqrt{\lambda_1}\frac{\Vert Ax\Vert}{\Vert A^{1/2}x \Vert} & \le 
            \sqrt{\lambda_n} \frac{\Vert Ay\Vert}{\left\Vert
                A^{1/2}y
            \right\Vert}
            \\
            \frac{\Vert Ax\Vert}{\Vert A^{1/2}x\Vert} &\le 
            \sqrt{\kappa(A)} 
            \frac{\Vert Ay\Vert}{\Vert A^{1/2}y\Vert}
            \\
            \frac{\Vert Ax\Vert}{\Vert Ay\Vert} &\le 
            \sqrt{\kappa(A)} 
            \frac{\Vert A^{1/2}x\Vert}{\Vert A^{1/2}y\Vert}
            \\
            \frac{\Vert Ax\Vert}{\Vert Ay\Vert} &\le 
            \sqrt{\kappa(A)} 
            \frac{\Vert x\Vert_A}{\Vert y\Vert_A}
        \end{align}
    \subsection{Theorems, Propositions}
        \begin{prop}[Krylov Subspace Grade Invariant Theorem]\label{prop:Krylov_Subspace_Grade_Invariant_Theorem}
            Once the subspace becomes linear dependent, the subspace becomes invariant. 
        \end{prop}
        \begin{proof}
            \begin{align}
                & K_k = \begin{bmatrix}
                    b & AB & \cdots & A^{k - 1}b
                \end{bmatrix}
                \\
                & K_k \text{ Lin Dep} \implies A^{k-1}b = AK_{k - 1}w
                \\
                & \implies 
                AK_k = K_k
                    \underbrace{\begin{bmatrix}
                        e_2 & \cdots & e_k & c_k
                    \end{bmatrix}}_{:= C_k}
                \\
                & \implies 
                A^2K_k = AK_kC_k = K_kC_k^2
            \end{align}
            $A^2K_k$ will span the same sapce as the range of the matrix $K_k$. 
        \end{proof}
\newpage


            

\end{document}
