\documentclass[]{article}
\usepackage{amsmath}\usepackage{amsfonts}
\usepackage[english]{babel}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\usepackage[margin=1in,footskip=0.25in]{geometry}
\usepackage{mathtools}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}
\usepackage[final]{graphicx}
\usepackage{listings}
\usepackage{courier}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\newcommand{\indep}{\perp \!\!\! \perp}
% \usepackage{wrapfig}
\graphicspath{{.}}
% \usepackage{fancyvrb}

%%
%% Julia definition (c) 2014 Jubobs
%%
\usepackage[T1]{fontenc}
\usepackage{beramono}
\usepackage[usenames,dvipsnames]{xcolor}
\lstdefinelanguage{Julia}%
  {morekeywords={abstract,break,case,catch,const,continue,do,else,elseif,%
      end,export,false,for,function,immutable,import,importall,if,in,%
      macro,module,otherwise,quote,return,switch,true,try,type,typealias,%
      using,while},%
   sensitive=true,%
   alsoother={$},%
   morecomment=[l]\#,%
   morecomment=[n]{\#=}{=\#},%
   morestring=[s]{"}{"},%
   morestring=[m]{'}{'},%
}[keywords,comments,strings]%

\lstset{%
    language         = Julia,
    basicstyle       = \ttfamily,
    keywordstyle     = \bfseries\color{blue},
    stringstyle      = \color{magenta},
    commentstyle     = \color{ForestGreen},
    showstringspaces = false,
}

\linespread{1}
\usepackage[fontsize=10pt]{fontsize}

\begin{document}
\numberwithin{equation}{subsection}
\section{Foundations}
    This sections focuses on important mathematical entities that are important for formulating, analyzing the Conjugate Gradient and the Lanczos Algorithm. Major parts of this sections cited from... 

    \subsection{Projectors}
        There are 2 types of projector, an oblique Projector and Orthgonal Projector. A matrix P is called a projector if: 
        \begin{definition}
            \begin{align}
                P^2 = P
            \end{align}    
        \end{definition}
        
        This property is sometimes referred as idempotent. As a consequence, $\text{ran}(I - P) = \text{null}(P)$ and here is the proof: 
        \begin{proof}
            \begin{align}
                \forall x \in \mathbb{C}^n: P(I - P)x &= \mathbf{0} \implies \text{ran}(I - P)\subseteq \text{null}(P)
                \\
                \forall x \in \text{null}(P): Px &= \mathbf{0} \implies (I - P)x = x \implies x \in \text{ran}(I - P)
                \\
                \implies \text{ran}(I - P) &= \text{null}(P)
                \label{a:1.1.4}
            \end{align}
        \end{proof}
        \noindent
        This consequence states the fact that any vector $x$ can be represented in the form of: $x = Px + (I - P)x$, and every projector will be defined via the range of $I - P$ and $P$. 
        
        \subsubsection{Orthogonal Projector}
            An orthogonal projector is a projector such that: 
            \begin{definition}
                \begin{align}
                    \text{null}(P) \perp \text{ran}(P)
                \end{align}    
            \end{definition}
            
            This property is in fact, very special. A good example of an orthogonal projector would be the Householder Reflector Matrix. Or just any $\hat{u}\hat{u}^H$ where $\hat{u}$ is being an unitary vector. For convenience of proving, assume subspace $M = \text{ran}(P)$. Consider the following lemma: 
            \begin{lemma}
                \begin{align}
                    \text{null}(P^H) = \text{ran}(P)^{\perp}
                    \\
                    \text{null}(P) = \text{ran}(P^H)^{\perp}
                \end{align}    
            \end{lemma}
            \noindent
            Using \hyperref[a:1.1.4]{(1.1.4)} and consider the proof: 
            \begin{proof}
                \begin{align}
                    \langle P^Hx, y\rangle &= \langle x, Py\rangle 
                    \\
                    \forall  x &\in \text{null}(P^H), y\in \mathbb{C}^n
                    \\
                    \implies \langle P^Hx ,y\rangle &= 0 = \langle x, Py\rangle
                    \\
                    \implies \text{null}(P^H) \perp& \text{ran}(P)
                    \\
                    \forall y \in& \text{null}(P), x \in \mathbb{C}^n: 
                    \\
                    \langle x, Py\rangle &= 0 = \langle P^Hx, y\rangle
                    \\
                    \implies \text{ran}(P^H) \perp& \text{null}(P)
                \end{align}
            \end{proof}
            
            \begin{prop}
                A projector is orthogonal iff it's Hermitian. 
            \end{prop}
            \begin{proof}
                $\impliedby$ Assuming the matrix is Hermitian and it's a projector, then we wish to prove that it's an orthogonal projector. Let's recall: 
                \begin{align}
                    \text{null}(P^H) = \text{ran}(P)^{\perp}
                    \\
                    \text{null}(P) = \text{ran}(P^H)^{\perp}
                \end{align}
                Substituting $P^H = P$, we have $\text{null}(P) = \text{ran}(P)^{\perp}$, Which is the definition of Orthogonal Projector. Therefore, $P$ is an orthogonal projector by the definition of the projector. 
                \par
                For the $\implies$ direction, we assume that $P$ is an Orthogonal Projector, then we wish to show that it's also Hermitian. Observe that $P^H$ is also a projector because $(P^H)^2 = (P^2)^H$. Then, using the definition of orthogonal projector: 
                \begin{align}
                    \text{null}(P) &\perp\text{ran}(P) 
                    \\
                    \text{null}(P^H) &\perp \text{ran}(P^H)
                \end{align}
                Notice that using above statement together with Lemma 1 means $\text{null}(P) = \text{ran}(P)^\perp = \text{null}(P^H)$, and then $\text{ran}(P)=\text{null}(P)^\perp = \text{ran}(P^H)$. Therefore, $P^H$ is an projector such that: $\text{ran}(P) = \text{ran}(P^H) \wedge \text{null}(P) = \text{null}(P^H)$. The range and null space of $P^H$ and $P$ is the same therefore $P$ has to be Hermitian. 
            \end{proof}
            \subsubsection{Oblique Projector}
                An oblique projector is not orthogonal, and vice versa. It's a projector that satisfies the following conditions: 
                \begin{definition}
                    \begin{align}
                        Px \in M \quad (I - P)x \perp L \quad \text{where: } M \neq L
                    \end{align}    
                \end{definition}
                An orthgonal projector is the case when the subspace $M = L$. 
                \par
                A famous example of an orthogonal projector is $QQ^H$ where $Q$ is an Unitary Matrix. This is a Hermitian Matrix and it's idempotent, making it an orthogonal projector. 
            \subsubsection{Projector Geometric Intuitions}
                A projector describes a given vector using some elements from another basis. The oblique projector creates a light sources in the form of the subspace $L$ and it shoots parrell light ray orthogonal to $L$, crossing vectors and projecting their shadow onto subspace $M$. 
    \subsection{Projectors and Norm Minimizations}
        An orthogonal projector always reduce the 2 norm of a vector. Given any subspace $M$, we can create a basis of vectors packing into the some matrix, say $A$, then $P_M$ as a projector onto the basis $M$ one example can be: $A(AA^T)^{-1}A^T$. Let's consider the claim: 
        \begin{align}
            \Vert P_Mx\Vert^2 \le \Vert x\Vert^2
        \end{align}
        Proof: 
        \begin{align}
            x &= Px + (I - P)x 
            \\
            \Vert x\Vert^2 &= \Vert Px\Vert^2 + \Vert (I - P)x\Vert^2
            \\
            \Vert x\Vert^2 &\ge \Vert Px\Vert^2
        \end{align}
        Using this property of the Orthogonal Projector, we consider the following minimizations problem: 
        \begin{align}
            \min_{x\in M} \Vert y - x\Vert_2^2 = \Vert y - P_M(y)\Vert_2^2
        \end{align}
        Proof:
        \begin{align}
            \Vert y - x\Vert_2^2 &= 
            \Vert y - P_My + P_My - x\Vert_2^2
            \\
            \Vert y - x\Vert_2^2 &= 
            \Vert y - P_My\Vert_2^2 + \Vert P_My - x\Vert_2^2
            \\
            \implies 
            \Vert y - P_My\Vert_2^2 &\le \Vert y - x\Vert_2^2
        \end{align}
        That concludes the proof. Observe that, $y - P_My\perp M$ and $P_My - x \in M$ because $P_My, x \in M$, which allows us to split the norm of $y - x$ into 2 components. In addition using the fact that the projector is orthogonal. That concludes the proof. 
    \subsection{Subspace Orthogonality Framework}
        Let $\mathcal K, \mathcal L$ be subspaces where candidates soltuions are chosen and residuals are orthogonalized against. Under the idea case the 2 subspaces spans all dimensions, and it's able to approximate all solutions and forcing the residual vector ($b - Ax$) to be zero. This is a description of this framework: 
        \begin{align}
            \tilde{x} \in x_0 + \mathcal{K} \text{ s.t: } b - A\tilde{x} \perp \mathcal{L}
        \end{align}
        it looks for an $x$ in the affine linear subspace $\mathcal{K}$ such that it's perpendicular to the subspace $\mathcal{L}$, or, equivalently, minimizing the projection onto the subspace $\mathcal{L}$. One interpretation of it is an projection of residual onto the basis that is orthogonal to $\mathcal L$. 
        \par
        Sometimes, for convenience and the exposition and exposing hidden connections between ideas, the above conditions can be expressed using matrix. 
        \begin{align}
            \text{Let } V \in \mathbb{C}^{n\times m} \text{ be a aasis for: }\mathcal{K}
            \\
            \text{Let } W \in \mathbb{C}^{n\times m} \text{ be a basis for: } \mathcal{L}
        \end{align}
        We can then make us of (1.3.1) and express it in the form of: 
        \begin{align}
            \tilde{x} &= x^{(0)} + Vy
            \\
            b - A\tilde{x}  &\perp (\text{span}\leftarrow \text{col})(W)
            \\
            W^T(b - A\tilde{x} - AVy) &= \mathbf{0}
            \\
            W^Tr^{(0)} - W^TAVy&= \mathbf{0}
            \\
            W^TAVy = W^Tr^{(0)}
        \end{align}
        \subsubsection{Prototype Algorithm}
            And from here, we can define a simple prototype algorithm using this framworks. 
            \begin{align}
            \begin{aligned}
                & \text{While not converging}: 
                \\&\hspace{1.1em}
                        \begin{aligned}
                        &\text{Increase Span for: } \mathcal{K, L}
                        \\
                        &\text{Choose: } V, W \text{ for } \mathcal{K}, \mathcal{L}
                        \\
                        & r := b - Ax
                        \\
                        & y := (W^TAV)^{-1}W^Tr
                        \\
                        & x := x + Vy
                    \end{aligned}
            \end{aligned}\tag{6}
            \end{align}
            Each time, we increase the span of the subspace $\mathcal K, \mathcal L$, which gives us more space to choose the solution $x$, and more space to reduce the residual vector $r$. This idea is incredibly flexible, and we will see in later part where it reduces to a more concrete algorithm. Finally, when $\mathcal K = \mathcal L$, this is referred to as a Petrov Galerkin's Conditions. 
        

    \subsection{Subspace Minimizations Framework}\label{sec:1.4}
        Other times, iterative method will choose to build up a subspace for each step with a subspace generator, and build up the solution on this expanding subspace, but with the additional objective of minimizing the residual under certain norm. Assuming that the vector $x\in x_0 + \mathcal{K}$, and we want to minimize the residual under a norm induced by positive definite operator $B$. Let it be the case that the columns of matrix $K$ span subspace $\mathcal{K}$ with $\dim(\mathcal K) = k$. 
        \begin{align}
            &\hspace{0.6em} \min_{x\in x_0 + \mathcal{K}} \Vert b - Ax\Vert_B^2 
            \\
            &= \min_{w\in \mathbb{R}^{k}} 
            \Vert b - A(x_0 + Kw)\Vert_B^2 & 
            \\
            &= \min_{w\in \mathbb{R}^{k}} 
            \Vert 
                r_0 - AKw
            \Vert_B^2
        \end{align}
        We take the derivative of it and set the derivative to zero, this translate the problem to a projection problem under the $A$ norm. 
        \begin{align}
            \nabla_w \left[
                \Vert r_0 - AKx\Vert_B^2
            \right] &= \mathbf{0}
            \\
            (AK)^TB(r_0 - AKx) &= \mathbf{0}
            \\
            (AK)^TBr_0 - (AK)^TBAKx &= \mathbf{0}
            \\
            (AK)^TBr_0 &= (AK)^TBAKx
        \end{align}
        The above formulation is tremendously powerful. I used gradient instead of projector for the simplicity of the argument. One can derive the same using projector but the math is bit more hedious. 
    \subsection{Krylov Subspace}
        A Krylov Subspace is a sequence basis paramaterized by $A$, an linear operator, $v$ an initial vector, and $k$, which basis in the sequece of basis that we are looking at. 
        \begin{definition}[Krylov subspace]
            \begin{align}
                \mathcal{K}_k(A|b) = \text{span}( b, Ab, A^2b, \cdots A^{k - 1}b)
            \end{align}
        \end{definition}
        Please immediately observe that from the definition we have: 
        \begin{align}
            \forall v: \mathcal{K}_1(A|v)  \subseteq  \mathcal{K}_2(A|v)  \subseteq \mathcal{K}_3(A|v)  \cdots 
        \end{align}
        Please also observe that, every element inside of krylov subspace generated by matrix $A$, and an initial veoctr $v$ can be represented as a polynomial of matrix $A$ multiplied by the vector $v$ and vice versa.
        \begin{align}
            & \forall x \in \mathcal K_j(A|v) \;\exists\; w: p_k(A|w)v = x
        \end{align}
        We use $p_k(A|w)$ to denotes a matrix polynomial with coefficients $w$, where $w$ is a vector. No proof this is trivial. Take note that, we can change the field of where the scalar $w$ is coming from, but for discussion below, $\mathbb R, \mathbb C$  doesn't matter and won't change the results. 
        \begin{align}
            p_k(A|w)v = \sum_{j = 0}^{k - 1}w_jA^jv
        \end{align}
        \subsubsection{The Grade of a Krylov Subspace}
            The most important porperty of the subspace is the idea of grade denoted as: $\text{grade}(A|v)$, indicating when the Krylov Subspace of $A$ wrt to $v$ stops expanding after a certain size. To show this idea, we consider the following 3 statements about Krylov Subspace which we will proceed to prove.
            \begin{prop}
                \begin{align}
                    \exists 1 \le k \le m + 1: \mathcal K_k(A|v) = \mathcal K_{k + 1}(A|v)
                \end{align}
                There exists an natural number between $1$ and $m+ 1$ such that, the successive krylov subspace span the same space asthe previous one. 
            \end{prop} 
            \begin{prop}
                \begin{align}
                    \exists \min k \text{ s.t: }\mathcal K_k(A|v) = \mathcal K_{k + 1}(A|v) \implies 
                    \mathcal K_k(A|v) \text{ is Lin Ind} \wedge \mathcal K_{k + 1}(A|v) \text{ is Lin Dep}. 
                \end{align}
                There eixsts a minimum such $k$ where the immediate next krylov subspace is linear dependent. 
            \end{prop}
            \begin{prop}
                \begin{align}
                    \mathcal K_k(A|v) \text{ Lin Dep} \implies \mathcal K_{k + 1}(A|v) = \mathcal K_k(A|v)
                \end{align}
                if the $k$ krylov subspace is linear dependent, then it stops expanding and the successive krylov subspace spans the same space. 
            \end{prop}
            \begin{theorem}[Existence of Grade for a Krylov Subspace]
                Let $k$ be the minumum number when the krylov subspace stops expanding, then all successive krylov subspace spand the same space. $\mathcal K_k(A|v) = \mathcal K_{k + j}(A|v) \;\forall j \ge 0$. The number $k$ is regarded as the grade of krylov subspace wrt to v denoted using $\text{grade}(A|v)$. 
            \end{theorem}
            Proposition 2, 3 ensures that there exists a term in the sequence of krylov subspace stops expanding, and when that happens all subsequent Krylov Subspace will span the same subspace, this is by Proposition 4. 
            \par
            Next, let's consider the proof of theorem 1 by proving all proposition 2, 3, and 4. 
            \begin{proof}[Proposition 2]
                For notational simplicity, $\mathcal K_k$ now denotes $\mathcal K_k(A|v)$. Let's start the considerations from the definition of the Krylov Subspace: 
                \begin{align}
                    \forall\; k: \mathcal K_k \subseteq \mathcal K_{k + 1}\implies \text{dim}(\mathcal K_{k})\le \text{dim}(\mathcal K_{k + 1})
                    \\
                    \mathcal K_{k + 1}\setminus \mathcal K_k = \text{span}(A^{k}v) 
                    \\
                    \implies \dim(\mathcal K_{k + 1}) - \dim(\mathcal K_k) \le 1
                \end{align}
                Therefore, the dimension of the successive krylov subspace forms a sequence of positive integer that is monotonically increasing. By the Cayley's Hamilton's theorem (will be stated later), the sequence is bounded by $m$, since there are $m + 1$ terms, it must be the case that at least 2 of the krylov subspace has the same dimension (And the earliest such occurance will exist), implying the the fact that the new added vector from $k$ to $k + 1$ is in the span of the previous subspace. 
            \end{proof}
            \begin{proof}[Proposition 4]
                \begin{align}
                    & \mathcal K_k \text{Lin Dep}
                    \\
                    \implies & \exists w_k \neq \mathbf 0 : p_k(A|w^+_k)v = \mathbf 0
                    \\
                    \implies & Ap_k(A|w_k^+)v = \mathbf 0
                    \\
                    & p_{k + 1}(A| [0 \; (w_k^+)^T]) = \mathbf 0
                    \\
                    & \mathcal K_{k + 1} \text{ is Lin Dep}
                \end{align}
                The recurrence of multplying by $A$ allows the krylov subspace to grow and the new bigger subspace will contain the previous one. Therefore inheriting the linear dependence, we use the idea of matrix polynomial for the proof. 
            \end{proof}
            \begin{proof}[Proposition 3]
                Assuming that prop 2, 4 are true. prop 3 implies the existence of the smallest such $k$. For contradiction, we only have one case to assume, that is $\mathcal K_k$ and $\mathcal K_{k + 1}$ are linear dependence. Then $\mathcal K_k$ is either Linear Dependence, or Independence. 
                \par
                If $\mathcal K_{k - 1}$ is linear dependence, then by (3) $\mathcal K_{k - 1} = \mathcal K_k$, hence $k$ is not the minimum. Else assume $\mathcal K_{k - 1}$ is linear independence, however $\mathcal K_k$ is linear dependence, and $\mathcal K_k \setminus \mathcal K_{k - 1} = \text{span}(A^{k - 1}v)$; therefore, $A^{k -1}v$ is in the span of $\mathcal K_{k -1}$, hence $\mathcal K_{k -1} = \mathcal K_k$, contradicing again that $k$ is the minimum such $k$. 
            \end{proof}
    \subsection{Minimal Polynomial of a Matrix}
        % TODO
    \subsection{Useful Theorems}
        \subsubsection{Cauchy Interlace Theorem}
        \subsubsection{Caley Hamilton's Theorem}
    \subsection{Deriving Conjugate Gradient from First Principles}
        \subsubsection{CG Objective and Framework}
            We introduce the algorithm as an attempt to minimize the energy norm of the error for a linear equation $Ax = b$, here we make the assumptions: 
            \begin{itemize}
                \item [1)] The matrix $A$ is symmetric semi-positive definite.  
                \item [2)] Further assume another matrix $P_k = [p_0 \;p_1\;\cdots p_{k-1}]$ as a matrix whose columns is a basis.
            \end{itemize}
            \begin{align}
                \min_{w \in \mathbb{R}^k}\Vert 
                    A^{-1}b - (x_0 + P_kw)
                \Vert_A^2 \iff P^T_kr_0 = P_k^TAP_kw
            \end{align}
            Refer back to \hyperref[sec:1.4]{(1.4)} for how to deal with the above minimization objective. Using the matrix form for the Petrov Galerkin Conditions where $W, V$ are both $P_k$, we have this orthgonality formulations: 
            \begin{align}
                \text{choose: }x \in x_0 + \text{ran}(P_k) \text{ s.t: } b - Ax \perp \text{ran}(P_k)    
            \end{align}
            Take note that this link between a norm minimzation and an equivalent Orthogonality condition doesn't garantee to happen, for example the FOM and Bi-Lanczos Method are orthogonalizations method that doesn't directly link to a norm minimization objective. 


\end{document}