\documentclass[]{article}
\usepackage{amsmath}\usepackage{amsfonts}
\usepackage[english]{babel}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\usepackage[margin=1in,footskip=0.25in]{geometry}
\usepackage{mathtools}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}
\usepackage[final]{graphicx}
\usepackage{listings}
\usepackage{courier}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\newcommand{\indep}{\perp \!\!\! \perp}
% \usepackage{wrapfig}
\graphicspath{{.}}
% \usepackage{fancyvrb}

%%
%% Julia definition (c) 2014 Jubobs
%%
\usepackage[T1]{fontenc}
\usepackage{beramono}
\usepackage[usenames,dvipsnames]{xcolor}
\lstdefinelanguage{Julia}%
  {morekeywords={abstract,break,case,catch,const,continue,do,else,elseif,%
      end,export,false,for,function,immutable,import,importall,if,in,%
      macro,module,otherwise,quote,return,switch,true,try,type,typealias,%
      using,while},%
   sensitive=true,%
   alsoother={$},%
   morecomment=[l]\#,%
   morecomment=[n]{\#=}{=\#},%
   morestring=[s]{"}{"},%
   morestring=[m]{'}{'},%
}[keywords,comments,strings]%

\lstset{%
    language         = Julia,
    basicstyle       = \ttfamily,
    keywordstyle     = \bfseries\color{blue},
    stringstyle      = \color{magenta},
    commentstyle     = \color{ForestGreen},
    showstringspaces = false,
}
\begin{document}
\numberwithin{equation}{subsection}{section}
\section{Fundations}
    This sections focuses on important mathematical entities that are important for formulating, analyzing the Conjugate Gradient and the Lanczos Algorithm. Major parts of this sections cited from... 

    \subsection{Projectors}
        There are 2 types of projector, an oblique Projector and Orthgonal Projector. A matrix P is called a projector if: 
        \begin{definition}
            \begin{align}
                P^2 = P
            \end{align}    
        \end{definition}
        
        This property is sometimes referred as idempotent. As a consequence, $\text{ran}(I - P) = \text{null}(P)$ and here is the proof: 
        \begin{proof}
            \begin{align}
                \forall x \in \mathbb{C}^n: P(I - P)x &= \mathbf{0} \implies \text{ran}(I - P)\subseteq \text{null}(P)
                \\
                \forall x \in \text{null}(P): Px &= \mathbf{0} \implies (I - P)x = x \implies x \in \text{ran}(I - P)
                \\
                \implies \text{ran}(I - P) &= \text{null}(P)
                \label{a:1.1.4}
            \end{align}
        \end{proof}
        \noindent
        This consequence states the fact that any vector $x$ can be represented in the form of: $x = Px + (I - P)x$, and every projector will be defined via the range of $I - P$ and $P$. 
        
        \subsubsection{Orthogonal Projector}
            An orthogonal projector is a projector such that: 
            \begin{definition}
                \begin{align}
                    \text{null}(P) \perp \text{ran}(P)
                \end{align}    
            \end{definition}
            
            This property is in fact, very special. A good example of an orthogonal projector would be the Householder Reflector Matrix. Or just any $\hat{u}\hat{u}^H$ where $\hat{u}$ is being an unitary vector. For convenience of proving, assume subspace $M = \text{ran}(P)$. Consider the following lemma: 
            \begin{lemma}
                \begin{align}
                    \text{null}(P^H) = \text{ran}(P)^{\perp}
                    \\
                    \text{null}(P) = \text{ran}(P^H)^{\perp}
                \end{align}    
            \end{lemma}
            \noindent
            Using \hyperref[a:1.1.4]{(1.1.4)} and consider the proof: 
            \begin{proof}
                \begin{align}
                    \langle P^Hx, y\rangle &= \langle x, Py\rangle 
                    \\
                    \forall  x &\in \text{null}(P^H), y\in \mathbb{C}^n
                    \\
                    \implies \langle P^Hx ,y\rangle &= 0 = \langle x, Py\rangle
                    \\
                    \implies \text{null}(P^H) \perp& \text{ran}(P)
                    \\
                    \forall y \in& \text{null}(P), x \in \mathbb{C}^n: 
                    \\
                    \langle x, Py\rangle &= 0 = \langle P^Hx, y\rangle
                    \\
                    \implies \text{ran}(P^H) \perp& \text{null}(P)
                \end{align}
            \end{proof}
            
            \begin{prop}
                A projector is orthogonal iff it's Hermitian. 
            \end{prop}
            \begin{proof}
                $\impliedby$ Assuming the matrix is Hermitian and it's a projector, then we wish to prove that it's an orthogonal projector. Let's recall: 
                \begin{align}
                    \text{null}(P^H) = \text{ran}(P)^{\perp}
                    \\
                    \text{null}(P) = \text{ran}(P^H)^{\perp}
                \end{align}
                Substituting $P^H = P$, we have $\text{null}(P) = \text{ran}(P)^{\perp}$, Which is the definition of Orthogonal Projector. Therefore, $P$ is an orthogonal projector by the definition of the projector. 
                \par
                For the $\implies$ direction, we assume that $P$ is an Orthogonal Projector, then we wish to show that it's also Hermitian. Observe that $P^H$ is also a projector because $(P^H)^2 = (P^2)^H$. Then, using the definition of orthogonal projector: 
                \begin{align}
                    \text{null}(P) &\perp\text{ran}(P) 
                    \\
                    \text{null}(P^H) &\perp \text{ran}(P^H)
                \end{align}
                Notice that using above statement together with Lemma 1 means $\text{null}(P) = \text{ran}(P)^\perp = \text{null}(P^H)$, and then $\text{ran}(P)=\text{null}(P)^\perp = \text{ran}(P^H)$. Therefore, $P^H$ is an projector such that: $\text{ran}(P) = \text{ran}(P^H) \wedge \text{null}(P) = \text{null}(P^H)$. The range and null space of $P^H$ and $P$ is the same therefore $P$ has to be Hermitian. 
            \end{proof}
            \subsubsection{Oblique Projector}
                An oblique projector is not orthogonal, and vice versa. It's a projector that satisfies the following conditions: 
                \begin{definition}
                    \begin{align}
                        Px \in M \quad (I - P)x \perp L \quad \text{where: } M \neq L
                    \end{align}    
                \end{definition}
                An orthgonal projector is the case when the subspace $M = L$. 
                \par
                A famous example of an orthogonal projector is $QQ^H$ where $Q$ is an Unitary Matrix. This is a Hermitian Matrix and it's idempotent, making it an orthogonal projector. 
            \subsubsection{Projector Geometric Intuitions}
                A projector describes a given vector using some elements from another basis. The oblique projector creates a light sources in the form of the subspace $L$ and it shoots parrell light ray orthogonal to $L$, crossing vectors and projecting their shadow onto subspace $M$. 
            

    \subsection{Projectors and Norm Minimizations}
        An orthogonal projector always reduce the 2 norm of a vector. Given any subspace $M$, we can create a basis of vectors packing into the some matrix, say $A$, then $P_M$ as a projector onto the basis $M$ one example can be: $A(AA^T)^{-1}A^T$. Let's consider the claim: 
        \begin{align}
            \Vert P_Mx\Vert^2 \le \Vert x\Vert^2
        \end{align}
        Proof: 
        \begin{align}
            x &= Px + (I - P)x 
            \\
            \Vert x\Vert^2 &= \Vert Px\Vert^2 + \Vert (I - P)x\Vert^2
            \\
            \Vert x\Vert^2 &\ge \Vert Px\Vert^2
        \end{align}
        Using this property of the Orthogonal Projector, we consider the following minimizations problem: 
        \begin{align}
            \min_{x\in M} \Vert y - x\Vert_2^2 = \Vert y - P_M(y)\Vert_2^2
        \end{align}
        Proof:
        \begin{align}
            \Vert y - x\Vert_2^2 &= 
            \Vert y - P_My + P_My - x\Vert_2^2
            \\
            \Vert y - x\Vert_2^2 &= 
            \Vert y - P_My\Vert_2^2 + \Vert P_My - x\Vert_2^2
            \\
            \implies 
            \Vert y - P_My\Vert_2^2 &\le \Vert y - x\Vert_2^2
        \end{align}
        That concludes the proof. Observe that, $y - P_My\perp M$ and $P_My - x \in M$ because $P_My, x \in M$, which allows us to split the norm of $y - x$ into 2 components. In addition using the fact that the projector is orthogonal. That concludes the proof. 
    \subsection{Subspace Orthogonality Framework}
        Let $\mathcal K, \mathcal L$ be subspaces where candidates soltuions are chosen and residuals are orthogonalized against. Under the idea case the 2 subspaces spans all dimensions, and it's able to approximate all solutions and forcing the residual vector ($b - Ax$) to be zero. This is a description of this framework: 
        \begin{align}
            \tilde{x} \in x_0 + \mathcal{K} \text{ s.t: } b - A\tilde{x} \perp \mathcal{L}
        \end{align}
        it looks for an $x$ in the affine linear subspace $\mathcal{K}$ such that it's perpendicular to the subspace $\mathcal{L}$, or, equivalently, minimizing the projection onto the subspace $\mathcal{L}$. One interpretation of it is an projection of residual onto the basis that is orthogonal to $\mathcal L$. 
        \par
        Sometimes, for convenience and the exposition and exposing hidden connections between ideas, the above conditions can be expressed using matrix. 
        \begin{align}
            \text{Let } V \in \mathbb{C}^{n\times m} \text{ be a aasis for: }\mathcal{K}
            \\
            \text{Let } W \in \mathbb{C}^{n\times m} \text{ be a basis for: } \mathcal{L}
        \end{align}
        We can then make us of (1.3.1) and express it in the form of: 
        \begin{align}
            \tilde{x} &= x^{(0)} + Vy
            \\
            b - A\tilde{x}  &\perp (\text{span}\leftarrow \text{col})(W)
            \\
            W^T(b - A\tilde{x} - AVy) &= \mathbf{0}
            \\
            W^Tr^{(0)} - W^TAVy&= \mathbf{0}
            \\
            W^TAVy = W^Tr^{(0)}
        \end{align}
        And from here, we can define a simple prototype algorithm using this framworks. 
        \begin{align}
        \begin{aligned}
            & \text{While not converging}: 
            \\&\hspace{1.1em}
                    \begin{aligned}
                    &\text{Increase Span for: } \mathcal{K, L}
                    \\
                    &\text{Choose: } V, W \text{ for } \mathcal{K}, \mathcal{L}
                    \\
                    & r := b - Ax
                    \\
                    & y := (W^TAV)^{-1}W^Tr
                    \\
                    & x := x + Vy
                \end{aligned}
        \end{aligned}\tag{6}
        \end{align}
        Each time, we increase the span of the subspace $\mathcal K, \mathcal L$, which gives us more space to choose the solution $x$, and more space to reduce the residual vector $r$. This idea is incredibly flexible, and we will see in later part where it reduces to a more concrete algorithm. 
    \subsection{Subspace Minimizations Framework}
        Other times, iterative method will choose to build up a subspace for each step with a subspace generator, and build up the solution on this expanding subspace, but with the additional objective of minimizing the residual under certain norm. Assuming that the vector $x\in x_0 + \mathcal{K}$, and we want to minimize the residual under a norm induced by positive definite operator $B$. Let it be the case that the columns of matrix $K$ span subspace $\mathcal{K}$ with $\dim(\mathcal K) = k$. 
        \begin{align}
            &\hspace{0.6em} \min_{x\in x_0 + \mathcal{K}} \Vert b - Ax\Vert_B^2 
            \\
            &= \min_{w\in \mathbb{R}^{k}} 
            \Vert b - A(x_0 + Kw)\Vert_B^2 & 
            \\
            &= \min_{w\in \mathbb{R}^{k}} 
            \Vert 
                r_0 - AKw
            \Vert_B^2
        \end{align}
        We take the derivative of it and set the derivative to zero, this translate the problem to a projection problem under the $A$ norm. 
        \begin{align}
            \nabla_w \left[
                \Vert r_0 - AKx\Vert_B^2
            \right] &= \mathbf{0}
            \\
            (AK)^TB(r_0 - AKx) &= \mathbf{0}
            \\
            (AK)^TBr_0 - (AK)^TBAKx &= \mathbf{0}
            \\
            (AK)^TBr_0 &= (AK)^TBAKx
        \end{align}
        The above formulation is tremendously powerful. I used gradient instead of projector for the simplicity of the argument. One can derive the same using projector but the math is bit more hedious. 
    \subsection{Krylov Subspace}
        A Krylov Subspace is a sequence basis paramaterized by $A$, an linear operator, $v$ an initial vector, and $k$, which basis in the sequece of basis that we are looking at. 
        \begin{definition}[Krylov subspace]
            \begin{align}
                \mathcal{K}_k(A|b) = \text{span}( b, Ab, A^2b, \cdots A^{k - 1}b)
            \end{align}
        \end{definition}
        Please immediately observe that from the definition we have: 
        \begin{align}
            \forall v: \mathcal{K}_1(A|v)  \subseteq  \mathcal{K}_2(A|v)  \subseteq \mathcal{K}_3(A|v)  \cdots 
        \end{align}
        Please also observe that, every element inside of krylov subspace generated by matrix $A$, and an initial veoctr $v$ can be represented as a polynomial of matrix $A$ multiplied by the vector $v$ and vice versa.
        \begin{align}
            & \forall x \in \mathcal K_j(A|v) \;\exists\; w: p_k(A|w)v = x
        \end{align}
        We use $p_k(A|w)$ to denotes a matrix polynomial with coefficients $w$, where $w$ is a vector. No proof this is trivial. Take note that, we can change the field of where the scalar $w$ is coming from, but for discussion below, $\mathbb R, \mathbb C$  doesn't matter and won't change the results. 
        \begin{align}
            p_k(A|w)v = \sum_{j = 0}^{k - 1}w_jA^jv
        \end{align}
    \subsection{Useful Theorems}
        \subsubsection{Cauchy Interlace Theorem}
        \subsubsection{Caley Hamilton's Theorem}
    \subsection{Deriving Conjugate Gradient from First Principles}


\end{document}