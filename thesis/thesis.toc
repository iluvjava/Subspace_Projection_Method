\babel@toc {english}{}
\contentsline {section}{\numberline {1}Notations}{4}{section.1}%
\contentsline {section}{\numberline {2}Introduction}{4}{section.2}%
\contentsline {section}{\numberline {3}Foundations}{5}{section.3}%
\contentsline {subsection}{\numberline {3.1}Projectors}{5}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}Orthogonal Projector}{5}{subsubsection.3.1.1}%
\contentsline {subsubsection}{\numberline {3.1.2}Oblique Projector}{6}{subsubsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.3}Projector Geometric Intuitions}{7}{subsubsection.3.1.3}%
\contentsline {subsubsection}{\numberline {3.1.4}Projector as a 2-Norm Minimizer}{7}{subsubsection.3.1.4}%
\contentsline {subsection}{\numberline {3.2}Subspace Projection Methods}{7}{subsection.3.2}%
\contentsline {subsubsection}{\numberline {3.2.1}Prototype Algorithm}{8}{subsubsection.3.2.1}%
\contentsline {subsubsection}{\numberline {3.2.2}Energy Norm Minimization using Gradient}{8}{subsubsection.3.2.2}%
\contentsline {subsection}{\numberline {3.3}Krylov Subspace}{10}{subsection.3.3}%
\contentsline {subsubsection}{\numberline {3.3.1}The Grade of a Krylov Subspace}{10}{subsubsection.3.3.1}%
\contentsline {subsubsection}{\numberline {3.3.2}The Grade and Matrix Polynomial}{12}{subsubsection.3.3.2}%
\contentsline {subsection}{\numberline {3.4}Useful Theorems and Mathematical Entities}{12}{subsection.3.4}%
\contentsline {subsubsection}{\numberline {3.4.1}Minimal Polynomial of a Matrix}{12}{subsubsection.3.4.1}%
\contentsline {subsubsection}{\numberline {3.4.2}Cauchy Interlace Theorem}{13}{subsubsection.3.4.2}%
\contentsline {subsubsection}{\numberline {3.4.3}Caley Hamilton's Theorem}{13}{subsubsection.3.4.3}%
\contentsline {subsubsection}{\numberline {3.4.4}The Chebyshev Polynomial}{13}{subsubsection.3.4.4}%
\contentsline {subsection}{\numberline {3.5}Deriving Conjugate Gradient from First Principles}{14}{subsection.3.5}%
\contentsline {subsubsection}{\numberline {3.5.1}CG Objective and Framework}{14}{subsubsection.3.5.1}%
\contentsline {subsubsection}{\numberline {3.5.2}Using the Projector}{15}{subsubsection.3.5.2}%
\contentsline {subsubsection}{\numberline {3.5.3}Assisted Conjugate Gradient (Conjugate Directions)}{16}{subsubsection.3.5.3}%
\contentsline {subsubsection}{\numberline {3.5.4}Properties of Assisted Conjugate Gradient}{17}{subsubsection.3.5.4}%
\contentsline {subsubsection}{\numberline {3.5.5}Residual Assisted Conjugate Gradient}{18}{subsubsection.3.5.5}%
\contentsline {subsubsection}{\numberline {3.5.6}RACG and Krylov Subspace}{21}{subsubsection.3.5.6}%
\contentsline {subsection}{\numberline {3.6}Arnoldi Iterations and Lanczos}{21}{subsection.3.6}%
\contentsline {subsubsection}{\numberline {3.6.1}The Arnoldi Iterations}{22}{subsubsection.3.6.1}%
\contentsline {subsubsection}{\numberline {3.6.2}Arnoldi Produces Orthogonal Basis for Krylov Subspace}{23}{subsubsection.3.6.2}%
\contentsline {subsubsection}{\numberline {3.6.3}The Lanczos Iterations}{23}{subsubsection.3.6.3}%
\contentsline {section}{\numberline {4}Analysis of Conjugate Gradient and Lanczos Iterations}{25}{section.4}%
\contentsline {subsection}{\numberline {4.1}Conjugate Gradient and Matrix Polynomial}{25}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Termination Conditions of RACG}{27}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Convergence Rate of RACG under Exact Arithematic}{28}{subsection.4.3}%
\contentsline {subsubsection}{\numberline {4.3.1}Uniformly Distributed Eigenvalues}{28}{subsubsection.4.3.1}%
\contentsline {subsubsection}{\numberline {4.3.2}One Outlier Eigenvalue}{30}{subsubsection.4.3.2}%
\contentsline {subsection}{\numberline {4.4}From Conjugate Gradient to Lanczos}{32}{subsection.4.4}%
\contentsline {subsubsection}{\numberline {4.4.1}The Base Case}{33}{subsubsection.4.4.1}%
\contentsline {subsubsection}{\numberline {4.4.2}The Inductive Case}{34}{subsubsection.4.4.2}%
\contentsline {subsubsection}{\numberline {4.4.3}Fixing the Sign}{35}{subsubsection.4.4.3}%
\contentsline {subsection}{\numberline {4.5}From Lanczos to Conjugate Gradient}{36}{subsection.4.5}%
\contentsline {subsubsection}{\numberline {4.5.1}Matching the Residual and Conjugate Vectors}{36}{subsubsection.4.5.1}%
\contentsline {subsubsection}{\numberline {4.5.2}Matching the $a_k, b_k$ in CG}{38}{subsubsection.4.5.2}%
\contentsline {section}{\numberline {5}Effects of Floating Point Arithematic}{41}{section.5}%
\contentsline {subsection}{\numberline {5.1}Partial Orthogonalization and Full Orthogonalization}{41}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Relative Errors of CG Under Floating Points}{42}{subsection.5.2}%
\contentsline {subsection}{\numberline {5.3}Greenbaum's Backward Analysis and Paige's Floating Point Analysis}{43}{subsection.5.3}%
\contentsline {subsubsection}{\numberline {5.3.1}Bounding the The Relative Residuals}{43}{subsubsection.5.3.1}%
\contentsline {subsubsection}{\numberline {5.3.2}Convergence Rate and Paige's Theorem}{44}{subsubsection.5.3.2}%
\contentsline {subsubsection}{\numberline {5.3.3}Ghost Eigenvalues}{45}{subsubsection.5.3.3}%
\contentsline {subsection}{\numberline {5.4}The Rizt Vector's View and Another Paige's Theorem}{48}{subsection.5.4}%
\contentsline {subsection}{\numberline {5.5}Forward Error Analysis}{48}{subsection.5.5}%
