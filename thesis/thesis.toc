\babel@toc {english}{}
\contentsline {section}{\numberline {1}Notations}{4}{section.1}%
\contentsline {section}{\numberline {2}Introduction}{4}{section.2}%
\contentsline {section}{\numberline {3}Foundations}{6}{section.3}%
\contentsline {subsection}{\numberline {3.1}Projectors}{6}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}Orthogonal Projector}{6}{subsubsection.3.1.1}%
\contentsline {subsubsection}{\numberline {3.1.2}Oblique Projector}{7}{subsubsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.3}Projector Geometric Intuitions}{8}{subsubsection.3.1.3}%
\contentsline {subsubsection}{\numberline {3.1.4}Projector as a 2-Norm Minimizer}{8}{subsubsection.3.1.4}%
\contentsline {subsection}{\numberline {3.2}Subspace Projection Methods}{8}{subsection.3.2}%
\contentsline {subsubsection}{\numberline {3.2.1}Prototype Algorithm}{9}{subsubsection.3.2.1}%
\contentsline {subsubsection}{\numberline {3.2.2}Energy Norm Minimization using Gradient}{9}{subsubsection.3.2.2}%
\contentsline {subsection}{\numberline {3.3}Krylov Subspace}{11}{subsection.3.3}%
\contentsline {subsubsection}{\numberline {3.3.1}The Grade of a Krylov Subspace}{11}{subsubsection.3.3.1}%
\contentsline {subsubsection}{\numberline {3.3.2}The Grade and Matrix Polynomial}{13}{subsubsection.3.3.2}%
\contentsline {subsection}{\numberline {3.4}Useful Theorems and Mathematical Entities}{13}{subsection.3.4}%
\contentsline {subsubsection}{\numberline {3.4.1}Minimal Polynomial of a Matrix}{13}{subsubsection.3.4.1}%
\contentsline {subsubsection}{\numberline {3.4.2}Cauchy Interlace Theorem}{14}{subsubsection.3.4.2}%
\contentsline {subsubsection}{\numberline {3.4.3}Caley Hamilton's Theorem}{14}{subsubsection.3.4.3}%
\contentsline {subsubsection}{\numberline {3.4.4}The Chebyshev Polynomial}{14}{subsubsection.3.4.4}%
\contentsline {subsection}{\numberline {3.5}Deriving Conjugate Gradient from First Principles}{15}{subsection.3.5}%
\contentsline {subsubsection}{\numberline {3.5.1}CG Objective and Framework}{15}{subsubsection.3.5.1}%
\contentsline {subsubsection}{\numberline {3.5.2}Using the Projector}{16}{subsubsection.3.5.2}%
\contentsline {subsubsection}{\numberline {3.5.3}Assisted Conjugate Gradient (Conjugate Directions)}{17}{subsubsection.3.5.3}%
\contentsline {subsubsection}{\numberline {3.5.4}Properties of Assisted Conjugate Gradient}{18}{subsubsection.3.5.4}%
\contentsline {subsubsection}{\numberline {3.5.5}Residual Assisted Conjugate Gradient}{19}{subsubsection.3.5.5}%
\contentsline {subsubsection}{\numberline {3.5.6}RACG and Krylov Subspace}{22}{subsubsection.3.5.6}%
\contentsline {subsection}{\numberline {3.6}Arnoldi Iterations and Lanczos}{22}{subsection.3.6}%
\contentsline {subsubsection}{\numberline {3.6.1}The Arnoldi Iterations}{23}{subsubsection.3.6.1}%
\contentsline {subsubsection}{\numberline {3.6.2}Arnoldi Produces Orthogonal Basis for Krylov Subspace}{24}{subsubsection.3.6.2}%
\contentsline {subsubsection}{\numberline {3.6.3}The Lanczos Iterations}{24}{subsubsection.3.6.3}%
\contentsline {section}{\numberline {4}Analysis of Conjugate Gradient and Lanczos Iterations}{26}{section.4}%
\contentsline {subsection}{\numberline {4.1}Conjugate Gradient and Matrix Polynomial}{26}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Termination Conditions of RACG}{28}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Convergence Rate of RACG under Exact Arithematic}{29}{subsection.4.3}%
\contentsline {subsubsection}{\numberline {4.3.1}Uniformly Distributed Eigenvalues}{29}{subsubsection.4.3.1}%
\contentsline {subsubsection}{\numberline {4.3.2}One Outlier Eigenvalue}{31}{subsubsection.4.3.2}%
\contentsline {subsection}{\numberline {4.4}From Conjugate Gradient to Lanczos}{33}{subsection.4.4}%
\contentsline {subsubsection}{\numberline {4.4.1}The Base Case}{34}{subsubsection.4.4.1}%
\contentsline {subsubsection}{\numberline {4.4.2}The Inductive Case}{35}{subsubsection.4.4.2}%
\contentsline {subsubsection}{\numberline {4.4.3}Fixing the Sign}{36}{subsubsection.4.4.3}%
\contentsline {subsection}{\numberline {4.5}From Lanczos to Conjugate Gradient}{37}{subsection.4.5}%
\contentsline {subsubsection}{\numberline {4.5.1}Matching the Residual and Conjugate Vectors}{37}{subsubsection.4.5.1}%
\contentsline {subsubsection}{\numberline {4.5.2}Matching the $a_k, b_k$ in CG}{39}{subsubsection.4.5.2}%
\contentsline {section}{\numberline {5}Effects of Floating Point Arithematic}{42}{section.5}%
\contentsline {subsection}{\numberline {5.1}Partial Orthogonalization and Full Orthogonalization}{42}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Relative Errors of CG Under Floating Points}{43}{subsection.5.2}%
\contentsline {subsection}{\numberline {5.3}Greenbaum's Backward Analysis and Paige's Floating Point Analysis}{44}{subsection.5.3}%
\contentsline {subsubsection}{\numberline {5.3.1}Bounding the The Relative Residuals}{44}{subsubsection.5.3.1}%
\contentsline {subsubsection}{\numberline {5.3.2}Paige's Theorem and Backwards Stability}{45}{subsubsection.5.3.2}%
\contentsline {subsubsection}{\numberline {5.3.3}Ghost Eigenvalues}{47}{subsubsection.5.3.3}%
\contentsline {subsection}{\numberline {5.4}The Rizt Vector's View and Another Paige's Theorem}{50}{subsection.5.4}%
\contentsline {subsection}{\numberline {5.5}Forward Error Analysis}{50}{subsection.5.5}%
